Year,Volume,Pages,Status,Book Title,Title,Authors,Editors,Publishers,Main Url,Metadata Url,Paper Url,Supplemental Url,Review Url,MetaReview Url,AuthorFeedback Url,Reviews And Public Comment,Abstract
1987,0,,,Neural Information Processing Systems,Bit-Serial Neural Networks,"Murray Alan, Smith Anthony, Butler Zoe",D. Anderson,American Institute of Physics,https://proceedings.neurips.cc/paper_files/paper/1987/hash/02e74f10e0327ad868d138f2b4fdd6f0-Abstract.html,https://proceedings.neurips.cc/paper_files/paper/1987/file/02e74f10e0327ad868d138f2b4fdd6f0-Metadata.json,https://proceedings.neurips.cc/paper_files/paper/1987/file/02e74f10e0327ad868d138f2b4fdd6f0-Paper.pdf,,,,,," A  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a  synapse array through to silicon layout and board design.  The issues surrounding bit  - serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel  development  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and  recall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected  specification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173) nique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms,  using  a  ""paging""  technique  to  time  - multiplex  calculations  through  the  synapse  array. "
1987,0,,,Neural Information Processing Systems,Connectivity Versus Entropy,Abu-Mostafa Yaser,D. Anderson,American Institute of Physics,https://proceedings.neurips.cc/paper_files/paper/1987/hash/03afdbd66e7929b125f8597834fa83a4-Abstract.html,https://proceedings.neurips.cc/paper_files/paper/1987/file/03afdbd66e7929b125f8597834fa83a4-Metadata.json,https://proceedings.neurips.cc/paper_files/paper/1987/file/03afdbd66e7929b125f8597834fa83a4-Paper.pdf,,,,,," How  does  the  connectivity  of a  neural  network  (number  of synapses  per  neuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by  the entropy)?  Switching theory would suggest no relation at all, since all Boolean  functions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g.,  using  two-input  NAND  gates).  However,  for  a  network  that  learns  a  problem  from  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the  problem becomes  a  lower  bound for  the connectivity of the network. "
1987,0,,,Neural Information Processing Systems,The Hopfield Model with Multi-Level Neurons,Fleisher Michael,D. Anderson,American Institute of Physics,https://proceedings.neurips.cc/paper_files/paper/1987/hash/072b030ba126b2f4b2374f342be9ed44-Abstract.html,https://proceedings.neurips.cc/paper_files/paper/1987/file/072b030ba126b2f4b2374f342be9ed44-Metadata.json,https://proceedings.neurips.cc/paper_files/paper/1987/file/072b030ba126b2f4b2374f342be9ed44-Paper.pdf,,,,,," The  Hopfield  neural  network.  model  for  associative  memory  is  generalized.  The  generalization replaces  two  state  neurons by neurons taking a  richer set of values.  Two  classes  of neuron  input output relations are developed guaranteeing convergence to stable states.  The first is a class of ""continuous"" rela- tions and the second is a class of allowed quantization rules for the neurons.  The information capacity for networks from  the second class is fOWld  to be of order N 3 bits for a network with N  neurons. A generalization of the sum of outer products learning rule is developed and investigated as well. Â© American Institute of Physics 1988 279 "
1987,0,,,Neural Information Processing Systems,How Neural Nets Work,"Lapedes Alan, Farber Robert",D. Anderson,American Institute of Physics,https://proceedings.neurips.cc/paper_files/paper/1987/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html,https://proceedings.neurips.cc/paper_files/paper/1987/file/093f65e080a295f8076b1c5722a46aa2-Metadata.json,https://proceedings.neurips.cc/paper_files/paper/1987/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf,,,,,," There is  presently great interest in the abilities of neural networks to mimic 
""qualitative reasoning""  by manipulating neural incodings of symbols.  Less work 
has  been performed on using neural networks to process floating  point numbers 
and it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)
rate  and  therefore  best  suited  for  ""fuzzy""  qualitative reasoning.  Nevertheless, 
the  potential  speed  of massively  parallel  operations  make  neural  net  ""number 
crunching""  an interesting topic  to explore.  In this paper we  discuss some of our 
work in which we  demonstrate that for  certain applications neural networks can 
achieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)
niques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can 
be  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net 
is  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from 
Rn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net 
with just two  ""hidden""  layers.  The network  uses  this functional  approximation 
to perform either interpolation (signal processing applications)  or extrapolation 
(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)
ods to perform. their tasks.  The geometrical viewpoint advocated here seems to 
be a  useful  approach  to analyzing  neural  network  operation  and  relates  neural 
networks  to well  studied topics  in  functional  approximation. "
1987,0,,,Neural Information Processing Systems,Spatial Organization of Neural Networks: A Probabilistic Modeling Approach,"Stafylopatis Andreas, Dikaiakos Marios, Kontoravdis D.",D. Anderson,American Institute of Physics,https://proceedings.neurips.cc/paper_files/paper/1987/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html,https://proceedings.neurips.cc/paper_files/paper/1987/file/14bfa6bb14875e45bba028a21ed38046-Metadata.json,https://proceedings.neurips.cc/paper_files/paper/1987/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf,,,,,," The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed. "

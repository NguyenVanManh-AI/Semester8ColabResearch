{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-G5f-9NGbFDG","pJc2TmYwbCjg","9lSGCcKWbHz6","eHaaE22AOi5a","NC5q4sWieGjE","zzXm4P-6eSka","SKRjTinwOqUX","OkZK4ztUbYdI","GXpkaKqX7nLS","krwkwQa_A1hz"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4807c0a740364d7d826a63d0a8f42d26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18f4ad588db4489c8af7c02d8d4444e1","IPY_MODEL_9118c871d66b45278be535da85ac9014","IPY_MODEL_9a94aeeb8845487ca66fc9f411bf5dc8"],"layout":"IPY_MODEL_08849573294148e59178a9386d4e9890"}},"18f4ad588db4489c8af7c02d8d4444e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef64c9aa0f864fbdb2ccbf043ff289f1","placeholder":"​","style":"IPY_MODEL_e881d4cc25f54a3b9f43b3385c816e00","value":"Downloading builder script: 100%"}},"9118c871d66b45278be535da85ac9014":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a688ad3a6984ca7a152d6d51dc43f2f","max":6473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d57132d1b4b5471b8144f831be641938","value":6473}},"9a94aeeb8845487ca66fc9f411bf5dc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c648eeeda12465483ed0ff43a5acb09","placeholder":"​","style":"IPY_MODEL_84b9ffa0b16a4bc2a198c55bd629fb24","value":" 6.47k/6.47k [00:00&lt;00:00, 500kB/s]"}},"08849573294148e59178a9386d4e9890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef64c9aa0f864fbdb2ccbf043ff289f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e881d4cc25f54a3b9f43b3385c816e00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a688ad3a6984ca7a152d6d51dc43f2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d57132d1b4b5471b8144f831be641938":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c648eeeda12465483ed0ff43a5acb09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84b9ffa0b16a4bc2a198c55bd629fb24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0f219fdcc414b649525f8c8e663e433":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bfacc37289b473389f2128b089048cc","IPY_MODEL_1174df44bc4d4410b8ae778337579905","IPY_MODEL_599818596deb41f08afe8505d81a5655"],"layout":"IPY_MODEL_5c785dd9960d41d3a9d0fb6841a71390"}},"7bfacc37289b473389f2128b089048cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e27f669f85b4c77bfd86f5bab83dea8","placeholder":"​","style":"IPY_MODEL_08f6b64bca9f4605b353974b2c31b23e","value":"Downloading readme: 100%"}},"1174df44bc4d4410b8ae778337579905":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14d87105daf5497f994e64e6ca24c33a","max":25393,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69b220314e48407ca3d25860cdaa7b00","value":25393}},"599818596deb41f08afe8505d81a5655":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da9b1726fa624883926889914214fb6b","placeholder":"​","style":"IPY_MODEL_9fbfbf63e8b34c76ab5192bd0bcf8a11","value":" 25.4k/25.4k [00:00&lt;00:00, 1.61MB/s]"}},"5c785dd9960d41d3a9d0fb6841a71390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e27f669f85b4c77bfd86f5bab83dea8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08f6b64bca9f4605b353974b2c31b23e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14d87105daf5497f994e64e6ca24c33a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b220314e48407ca3d25860cdaa7b00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da9b1726fa624883926889914214fb6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fbfbf63e8b34c76ab5192bd0bcf8a11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56814bbbf0e648c3a38c5fab9db59d22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_015b18178715463a950fc90fc7bd6d77","IPY_MODEL_71dccce3005245e0a4f834541834e3d9","IPY_MODEL_91ab992e8f4e41609e1fd1ab7da3d0e1"],"layout":"IPY_MODEL_48b9533744d54020ab166f6fe2fe0ad5"}},"015b18178715463a950fc90fc7bd6d77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5e9e59c93124ff8a7054965d2a9cfae","placeholder":"​","style":"IPY_MODEL_5fbb7b744eb5484084515a389a37f6ad","value":"Downloading data: 100%"}},"71dccce3005245e0a4f834541834e3d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c37944e53a294d1eb86d9c0f666c0877","max":1156651,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9da78ce16d6f4501875931ff398ec595","value":1156651}},"91ab992e8f4e41609e1fd1ab7da3d0e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b30755e04034d949d98374680e856ac","placeholder":"​","style":"IPY_MODEL_106f4d8807ac40549b1211d6190854ab","value":" 1.16M/1.16M [00:00&lt;00:00, 1.29MB/s]"}},"48b9533744d54020ab166f6fe2fe0ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5e9e59c93124ff8a7054965d2a9cfae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fbb7b744eb5484084515a389a37f6ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c37944e53a294d1eb86d9c0f666c0877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da78ce16d6f4501875931ff398ec595":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b30755e04034d949d98374680e856ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"106f4d8807ac40549b1211d6190854ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a768b85bf8c74991ba080842ff26f379":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10691b1a88fa40f7b9a1ee84626b17da","IPY_MODEL_f1ca95880d234949a7d78142f484e099","IPY_MODEL_57f98fa7c39949d19c91275c3df776e9"],"layout":"IPY_MODEL_ccb71b44355b452aa07972417c66f0cc"}},"10691b1a88fa40f7b9a1ee84626b17da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71586d6ba19b42278ea369ccf6e8fa1b","placeholder":"​","style":"IPY_MODEL_34db1a8622de41f88a595cb19c0ab935","value":"Downloading data: 100%"}},"f1ca95880d234949a7d78142f484e099":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2fb582b765a4b31807729a36a44ea0c","max":2404585,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63df77301027474282a9c56ac76201ed","value":2404585}},"57f98fa7c39949d19c91275c3df776e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6bc1e8cd0c947eb94d1eff4ee9d6a49","placeholder":"​","style":"IPY_MODEL_acc04ab214b44407a181513dcd9ac232","value":" 2.40M/2.40M [00:01&lt;00:00, 1.79MB/s]"}},"ccb71b44355b452aa07972417c66f0cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71586d6ba19b42278ea369ccf6e8fa1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34db1a8622de41f88a595cb19c0ab935":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2fb582b765a4b31807729a36a44ea0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63df77301027474282a9c56ac76201ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6bc1e8cd0c947eb94d1eff4ee9d6a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acc04ab214b44407a181513dcd9ac232":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06036ac58070471fb859c4828ce00221":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3536ce2ecdd9424a8e31f4c267d3e8ef","IPY_MODEL_1d9c2e189d9c41ce8cf8739197367004","IPY_MODEL_74eeeaaa6f1b4b8d82b14755374055d5"],"layout":"IPY_MODEL_eebaa0b3fdfd43d4b11f41bf25d34ec7"}},"3536ce2ecdd9424a8e31f4c267d3e8ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ae75ce49be540e998a7f514f1ea0f5a","placeholder":"​","style":"IPY_MODEL_80938c72b538433996eb7c1679801a80","value":"Downloading data: 100%"}},"1d9c2e189d9c41ce8cf8739197367004":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd1edaf12653462c8f3025efd2d35c20","max":1133587,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf1e4c899c604a82b44c7a656de24702","value":1133587}},"74eeeaaa6f1b4b8d82b14755374055d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d42674de1c3747c7af353e6324b3643b","placeholder":"​","style":"IPY_MODEL_7393a7f74b3c449c8d1e4af9a3fc7c3c","value":" 1.13M/1.13M [00:00&lt;00:00, 1.32MB/s]"}},"eebaa0b3fdfd43d4b11f41bf25d34ec7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ae75ce49be540e998a7f514f1ea0f5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80938c72b538433996eb7c1679801a80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd1edaf12653462c8f3025efd2d35c20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf1e4c899c604a82b44c7a656de24702":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d42674de1c3747c7af353e6324b3643b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7393a7f74b3c449c8d1e4af9a3fc7c3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d391c53512c408bb09f7cbb6e85fb45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ba9128e0f8947a89e26fead7bec59bb","IPY_MODEL_8ce4cf246d79494881a0dbd26ed3cd9d","IPY_MODEL_e27a15d70024402bacce8e5d010cffe5"],"layout":"IPY_MODEL_f1b28040988843e0b53257dfbf3260f3"}},"5ba9128e0f8947a89e26fead7bec59bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f12c2558d5749c0b94f98cc3a65d903","placeholder":"​","style":"IPY_MODEL_0d64202a42d54243b9829002631171e5","value":"Generating train split: "}},"8ce4cf246d79494881a0dbd26ed3cd9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88ad359986b74e0880c8efe4a3100eca","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b56c5a2326a441f28ff29cd63e783b36","value":1}},"e27a15d70024402bacce8e5d010cffe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7599f4c4477d48f7b76aff0db20b7abb","placeholder":"​","style":"IPY_MODEL_c698120e7e7c462db470314a089c0cbf","value":" 1000/0 [00:00&lt;00:00, 1486.75 examples/s]"}},"f1b28040988843e0b53257dfbf3260f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f12c2558d5749c0b94f98cc3a65d903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d64202a42d54243b9829002631171e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88ad359986b74e0880c8efe4a3100eca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b56c5a2326a441f28ff29cd63e783b36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7599f4c4477d48f7b76aff0db20b7abb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c698120e7e7c462db470314a089c0cbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1824e8bf6cfc4516958c1342a18d1e7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91eb2bd0bcce4c0a9cc08b5ffe57db5e","IPY_MODEL_aff1fb01947e4b098cd1a9ea5729a384","IPY_MODEL_1d414fc2c4d34f449bc1a69a16aeca8e"],"layout":"IPY_MODEL_e24e1fcfb1434617b7b0f85e390ff014"}},"91eb2bd0bcce4c0a9cc08b5ffe57db5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a75f32999d39432b9347db5059379484","placeholder":"​","style":"IPY_MODEL_3abaf195410848a18902d41002153ad3","value":"Generating test split: "}},"aff1fb01947e4b098cd1a9ea5729a384":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a4699f3c91b4a73b12d7026708665cc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_737188706cd3427988d9789f0334fbd0","value":1}},"1d414fc2c4d34f449bc1a69a16aeca8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1b271ecf364432ba6589be057c65e36","placeholder":"​","style":"IPY_MODEL_46ff8f6b74234dd3809f1e504f51177e","value":" 500/0 [00:00&lt;00:00, 1866.28 examples/s]"}},"e24e1fcfb1434617b7b0f85e390ff014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75f32999d39432b9347db5059379484":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3abaf195410848a18902d41002153ad3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a4699f3c91b4a73b12d7026708665cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"737188706cd3427988d9789f0334fbd0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1b271ecf364432ba6589be057c65e36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46ff8f6b74234dd3809f1e504f51177e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59dda6778412461db3c49fd31322e304":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb0d39fc251b43adaa4536526d030e0b","IPY_MODEL_1bbec29662cf474899969c07360007e8","IPY_MODEL_94cf8b0abf2047928a07c9fa76e8f50a"],"layout":"IPY_MODEL_a30354060cae4660b08ad2c49f431d5e"}},"bb0d39fc251b43adaa4536526d030e0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_109fb977f50443fcb15c33ebf1b4a1d1","placeholder":"​","style":"IPY_MODEL_c1e810cb39934e32ab0cac8a316741be","value":"Generating validation split: "}},"1bbec29662cf474899969c07360007e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7278a6b1508e4ec299b420722acd1a49","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53aca80b1d90441585847ebb92251916","value":1}},"94cf8b0abf2047928a07c9fa76e8f50a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f512141852ee4adca3561bafd5be85f0","placeholder":"​","style":"IPY_MODEL_0b53dea588a9478188b8b39c3acb8a81","value":" 500/0 [00:00&lt;00:00, 1910.69 examples/s]"}},"a30354060cae4660b08ad2c49f431d5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"109fb977f50443fcb15c33ebf1b4a1d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1e810cb39934e32ab0cac8a316741be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7278a6b1508e4ec299b420722acd1a49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"53aca80b1d90441585847ebb92251916":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f512141852ee4adca3561bafd5be85f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b53dea588a9478188b8b39c3acb8a81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89474a9b91404f0c908284d7e37d0fcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27bd81129a2d477b9179768427ce39d4","IPY_MODEL_05a26df4c8e14cf0b8cfc7f499c7a435","IPY_MODEL_a55f7dcf95f640c6878611d6f0fcf97d"],"layout":"IPY_MODEL_89666fdabace4fc4b4894ea283b63ad0"}},"27bd81129a2d477b9179768427ce39d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea3a93424d354957bddaac1ba5160598","placeholder":"​","style":"IPY_MODEL_e7ae4986296049aba950fd177cd37e93","value":"Downloading builder script: 100%"}},"05a26df4c8e14cf0b8cfc7f499c7a435":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ffae4b43e5a4f95aeaa81379a65eec9","max":7534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb580eacbf544f05813800839893e7ce","value":7534}},"a55f7dcf95f640c6878611d6f0fcf97d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fbed27dabb14747ab0fba9fae40dbe4","placeholder":"​","style":"IPY_MODEL_f0892dd020cb4600a1e639a83d3f89a4","value":" 7.53k/7.53k [00:00&lt;00:00, 644kB/s]"}},"89666fdabace4fc4b4894ea283b63ad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea3a93424d354957bddaac1ba5160598":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7ae4986296049aba950fd177cd37e93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ffae4b43e5a4f95aeaa81379a65eec9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb580eacbf544f05813800839893e7ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fbed27dabb14747ab0fba9fae40dbe4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0892dd020cb4600a1e639a83d3f89a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cd76f982477477aaa13d4f5760075ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc65301a48dc46fab6a250061b95a298","IPY_MODEL_d0907516cd954b629c806530a72e19c0","IPY_MODEL_f678c5bcb86c4522b2165d857011bfc5"],"layout":"IPY_MODEL_fa255a5d7fce44579b49ff9b9a160ae4"}},"bc65301a48dc46fab6a250061b95a298":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d9c5b38b45c45c2b01cb11accfdaa0d","placeholder":"​","style":"IPY_MODEL_eeda85ffb6ec45db8757fa1dd00fad1c","value":"Downloading readme: 100%"}},"d0907516cd954b629c806530a72e19c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f36d8d0e5eaf4b5e9365a8e0e15905c6","max":5929,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cbad95a982840cb878d7e7204d82806","value":5929}},"f678c5bcb86c4522b2165d857011bfc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec103a5eca4548ffad332189d7ab5fc2","placeholder":"​","style":"IPY_MODEL_cdd02276486b4a5f858bba37c4de7213","value":" 5.93k/5.93k [00:00&lt;00:00, 291kB/s]"}},"fa255a5d7fce44579b49ff9b9a160ae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d9c5b38b45c45c2b01cb11accfdaa0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeda85ffb6ec45db8757fa1dd00fad1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f36d8d0e5eaf4b5e9365a8e0e15905c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cbad95a982840cb878d7e7204d82806":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec103a5eca4548ffad332189d7ab5fc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdd02276486b4a5f858bba37c4de7213":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59bdf3aa0e134c8191c7f7d4ed097157":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53b18fe8a99b472992fd5089585316d8","IPY_MODEL_d77fce16792c49a580fa9635736ff942","IPY_MODEL_97d92c690a544a059f8fdbeb1adc2710"],"layout":"IPY_MODEL_582bbe4c98f24b7d8d007346e149841c"}},"53b18fe8a99b472992fd5089585316d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a96dccc35284538ae134cc5314928e9","placeholder":"​","style":"IPY_MODEL_43aa40d3a5864c25b591580da8c0fdc1","value":"Downloading data: 100%"}},"d77fce16792c49a580fa9635736ff942":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fba9795de42b40e98c221d79b438c64c","max":11238079,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da1b1d5643b24a8ba7a418ee563b43d2","value":11238079}},"97d92c690a544a059f8fdbeb1adc2710":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8028c50f50b84daebd15c5dac81a479d","placeholder":"​","style":"IPY_MODEL_f37ba9f8ba1f4ad3852d5c913edd1101","value":" 11.2M/11.2M [00:01&lt;00:00, 10.8MB/s]"}},"582bbe4c98f24b7d8d007346e149841c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a96dccc35284538ae134cc5314928e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43aa40d3a5864c25b591580da8c0fdc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fba9795de42b40e98c221d79b438c64c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da1b1d5643b24a8ba7a418ee563b43d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8028c50f50b84daebd15c5dac81a479d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f37ba9f8ba1f4ad3852d5c913edd1101":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e17ec59b95d64db098933b6de26abbe0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60b2ab982221479c98dd4a53033bb6e8","IPY_MODEL_a8113d34a718499eb8c1828f9f3ca0a5","IPY_MODEL_7f365cea68334055a5f25ac5ab5b0df6"],"layout":"IPY_MODEL_28b00d22135c46c7b4b2c32928779c9b"}},"60b2ab982221479c98dd4a53033bb6e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43af9fd51857436ab1dc7be3237b36db","placeholder":"​","style":"IPY_MODEL_5ac95c6a274940acb04cb71a02d1c707","value":"Downloading data: 100%"}},"a8113d34a718499eb8c1828f9f3ca0a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_889c7432add7495db485621a500953de","max":16055534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_418b2bfda88c49f6b6fd6caa9980b32d","value":16055534}},"7f365cea68334055a5f25ac5ab5b0df6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2cc69bbeb404a59b840c986867fce31","placeholder":"​","style":"IPY_MODEL_7528fdb3f8ec45599be51d10fc9e7c5b","value":" 16.1M/16.1M [00:02&lt;00:00, 13.2MB/s]"}},"28b00d22135c46c7b4b2c32928779c9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43af9fd51857436ab1dc7be3237b36db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ac95c6a274940acb04cb71a02d1c707":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"889c7432add7495db485621a500953de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"418b2bfda88c49f6b6fd6caa9980b32d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2cc69bbeb404a59b840c986867fce31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7528fdb3f8ec45599be51d10fc9e7c5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3bb55efa63b467d9a89f80f38110376":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4014f4a1b594e3abff3ae8d03864249","IPY_MODEL_263c4c29d9714bc0a92887dd0b652ea7","IPY_MODEL_802844ed57d44138b9a8e41478081232"],"layout":"IPY_MODEL_c224b8ae693a4667a48cf7ec56edb8b4"}},"a4014f4a1b594e3abff3ae8d03864249":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfed22d07488418188350743ebcd51f9","placeholder":"​","style":"IPY_MODEL_d06e268485a741bf96b035dc2cd4dff0","value":"Generating train split: "}},"263c4c29d9714bc0a92887dd0b652ea7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba1e29668b7b4bc290ef5927a3bb8ed8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54c59b5bf24440529d0014d7e99e51ec","value":1}},"802844ed57d44138b9a8e41478081232":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13fb524ad203471e904a0ac4e2fc230b","placeholder":"​","style":"IPY_MODEL_06d5df021da3473fb1c511a1b71d60d6","value":" 144/0 [00:00&lt;00:00, 1011.41 examples/s]"}},"c224b8ae693a4667a48cf7ec56edb8b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfed22d07488418188350743ebcd51f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d06e268485a741bf96b035dc2cd4dff0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba1e29668b7b4bc290ef5927a3bb8ed8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"54c59b5bf24440529d0014d7e99e51ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13fb524ad203471e904a0ac4e2fc230b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06d5df021da3473fb1c511a1b71d60d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73bbe05e4cf4451fb20fcab8a0f1acf6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_610ddc0c14a94278a08bc850e06b823a","IPY_MODEL_cde4496e6a4448eaa45259d3fb24cdcf","IPY_MODEL_ecfe0dfc6d8343ff8eec4b4c143fabec"],"layout":"IPY_MODEL_8374d198c9764e78889754038b103eb3"}},"610ddc0c14a94278a08bc850e06b823a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c79598acf56f462aa23e38ad43525e3d","placeholder":"​","style":"IPY_MODEL_7101d492feb9497d848c3c8bef5efa22","value":"Generating test split: "}},"cde4496e6a4448eaa45259d3fb24cdcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0af9d6c67c36498e952203ad117a4e5e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50529e3a043a49d58506f8f5e5105777","value":1}},"ecfe0dfc6d8343ff8eec4b4c143fabec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec52f38da90b43aa8dfc498d7c03ca50","placeholder":"​","style":"IPY_MODEL_7ba2893dc919416b95dbf876a9c0310d","value":" 100/0 [00:00&lt;00:00, 1077.82 examples/s]"}},"8374d198c9764e78889754038b103eb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c79598acf56f462aa23e38ad43525e3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7101d492feb9497d848c3c8bef5efa22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0af9d6c67c36498e952203ad117a4e5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"50529e3a043a49d58506f8f5e5105777":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec52f38da90b43aa8dfc498d7c03ca50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ba2893dc919416b95dbf876a9c0310d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eef63402359f487aa7c67c03497cff8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afd0ec6aa6ea485a81382e798db51818","IPY_MODEL_ef366be79f3d4a7995ebe5245889663e","IPY_MODEL_141c22687d714cdcbe1a28b442d9a4dc"],"layout":"IPY_MODEL_968a6e4cf19f46cb81f5229c99a6c90b"}},"afd0ec6aa6ea485a81382e798db51818":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f80fa94e9578452a8b4189b68703930f","placeholder":"​","style":"IPY_MODEL_bb477b1017b241cab74b73b599cffb9f","value":"Downloading builder script: 100%"}},"ef366be79f3d4a7995ebe5245889663e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22e95415150e41da8e2e6b875f425de3","max":6556,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe09f32dd8ba407a8f51a25fbab159d2","value":6556}},"141c22687d714cdcbe1a28b442d9a4dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8f00e5d81b047b5852f3b9cc548ab84","placeholder":"​","style":"IPY_MODEL_f28b850b4ec24d7dbd8beaa1ff990682","value":" 6.56k/6.56k [00:00&lt;00:00, 242kB/s]"}},"968a6e4cf19f46cb81f5229c99a6c90b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f80fa94e9578452a8b4189b68703930f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb477b1017b241cab74b73b599cffb9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22e95415150e41da8e2e6b875f425de3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe09f32dd8ba407a8f51a25fbab159d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8f00e5d81b047b5852f3b9cc548ab84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f28b850b4ec24d7dbd8beaa1ff990682":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed83cab240be44e294e88a119b5c57ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0f3df92253b4e7488e74b24ae7125d9","IPY_MODEL_06ece1b06cef420bb911c9e13f975c98","IPY_MODEL_bf95bbc30a674ad9b662a0c110b28c0f"],"layout":"IPY_MODEL_e044a70208624955b4b809a6144b5fb6"}},"b0f3df92253b4e7488e74b24ae7125d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdc24e0eccc7401a87a10d4f00c44652","placeholder":"​","style":"IPY_MODEL_6a94809527ec426db0eedff8f0dd67be","value":"Downloading readme: 100%"}},"06ece1b06cef420bb911c9e13f975c98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9493e91559b34f798e5d700c7699f4e2","max":22818,"min":0,"orientation":"horizontal","style":"IPY_MODEL_897f4f20bcde4f669ee9fef52f0ebebe","value":22818}},"bf95bbc30a674ad9b662a0c110b28c0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1e301c920b44e2ebaefcbb271a7c7bc","placeholder":"​","style":"IPY_MODEL_254412c46ca64bcbb46224de4223a2c4","value":" 22.8k/22.8k [00:00&lt;00:00, 1.76MB/s]"}},"e044a70208624955b4b809a6144b5fb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdc24e0eccc7401a87a10d4f00c44652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a94809527ec426db0eedff8f0dd67be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9493e91559b34f798e5d700c7699f4e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"897f4f20bcde4f669ee9fef52f0ebebe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1e301c920b44e2ebaefcbb271a7c7bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"254412c46ca64bcbb46224de4223a2c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58392037e2c744c1a427ca606d23947c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f58e4a9132664067ab51da423104e2eb","IPY_MODEL_48cf83dcdbaf4d94a872fd919a4dd42c","IPY_MODEL_1f9d8f9fcbb14afc9cd92003d2a0715f"],"layout":"IPY_MODEL_d8e63ca503fb4f0ca9858d85f9f67654"}},"f58e4a9132664067ab51da423104e2eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e00aed41ea34a76b74708141bb8dc5e","placeholder":"​","style":"IPY_MODEL_bdb86c09c69447219cfdc7b2ec97b2a1","value":"Downloading data: 100%"}},"48cf83dcdbaf4d94a872fd919a4dd42c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41a3d288068948dd929d5786297262fa","max":337326,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57d3764dd2354f1db8eeb6d6273864ae","value":337326}},"1f9d8f9fcbb14afc9cd92003d2a0715f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb3f8865e71940879a998537c858fe7c","placeholder":"​","style":"IPY_MODEL_35bb487ea8ef475f866f0cad6e733177","value":" 337k/337k [00:00&lt;00:00, 609kB/s]"}},"d8e63ca503fb4f0ca9858d85f9f67654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e00aed41ea34a76b74708141bb8dc5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb86c09c69447219cfdc7b2ec97b2a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41a3d288068948dd929d5786297262fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57d3764dd2354f1db8eeb6d6273864ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb3f8865e71940879a998537c858fe7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35bb487ea8ef475f866f0cad6e733177":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"08217b6344ce4a99a4ab4e795b6a0162":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44120d9e04004cf0a93b3d7fb3c711b6","IPY_MODEL_33cad541472144e99b874c6c751792f3","IPY_MODEL_b8be985d66d248c6b1ab9c1a20bcb962"],"layout":"IPY_MODEL_ce5ab5bc756e4869bceab1655ad8f6f2"}},"44120d9e04004cf0a93b3d7fb3c711b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14e58c7a9e4e491aa4d94ebb7644fe6c","placeholder":"​","style":"IPY_MODEL_440f1d8dc023484eb383520f924cb4c4","value":"Downloading data: 100%"}},"33cad541472144e99b874c6c751792f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e08e0413bd954d148946861203e35efc","max":1048540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_caa0e31b0e1341f28140df134e28f821","value":1048540}},"b8be985d66d248c6b1ab9c1a20bcb962":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce5b5f49fce04c3888c0c337e4a6052f","placeholder":"​","style":"IPY_MODEL_4812bb99ce5c45498088136928ba7c25","value":" 1.05M/1.05M [00:00&lt;00:00, 1.35MB/s]"}},"ce5ab5bc756e4869bceab1655ad8f6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14e58c7a9e4e491aa4d94ebb7644fe6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"440f1d8dc023484eb383520f924cb4c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e08e0413bd954d148946861203e35efc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caa0e31b0e1341f28140df134e28f821":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce5b5f49fce04c3888c0c337e4a6052f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4812bb99ce5c45498088136928ba7c25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a4c3fcef2434ea29f2980ce6148bf7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_034059e662534469ba9c60b6fa83e735","IPY_MODEL_a615dd46ea054f13b61f4d220da7a88c","IPY_MODEL_4311ff7c08fd41128ecd13dd454b34f0"],"layout":"IPY_MODEL_c6346f52f12a4bef8da14f65d535b168"}},"034059e662534469ba9c60b6fa83e735":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c998bff59cfc4daa9762b47ea25fa8ff","placeholder":"​","style":"IPY_MODEL_d039052175664a85a164df1390578830","value":"Downloading data: 100%"}},"a615dd46ea054f13b61f4d220da7a88c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8727d7f7c7fe4a619bdd1d2ee2486af6","max":175108,"min":0,"orientation":"horizontal","style":"IPY_MODEL_617cc7196d2f467fa996a80b669e53b2","value":175108}},"4311ff7c08fd41128ecd13dd454b34f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_147a77cb7a96439a8d607992c5b6adad","placeholder":"​","style":"IPY_MODEL_b38e3a4d875f4894a66bcacd8b6d93d9","value":" 175k/175k [00:00&lt;00:00, 207kB/s]"}},"c6346f52f12a4bef8da14f65d535b168":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c998bff59cfc4daa9762b47ea25fa8ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d039052175664a85a164df1390578830":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8727d7f7c7fe4a619bdd1d2ee2486af6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617cc7196d2f467fa996a80b669e53b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"147a77cb7a96439a8d607992c5b6adad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b38e3a4d875f4894a66bcacd8b6d93d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76dce3d478a3489f91c12abc873d2db4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1b3b2f98be34807b8a31942ee6e0917","IPY_MODEL_d0fb9a4cc1a94e3d9a26fb315de8aa61","IPY_MODEL_f22fec2e982e4c979be4537208671dea"],"layout":"IPY_MODEL_af493325bccc4c5a99fe70b9f41bf499"}},"f1b3b2f98be34807b8a31942ee6e0917":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bc1fcd3452c46b3bde01875e210e18d","placeholder":"​","style":"IPY_MODEL_316061152b03413d9a4e43435de573d0","value":"Generating train split: "}},"d0fb9a4cc1a94e3d9a26fb315de8aa61":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c19d2947d3d42f2b67ed72921890ab3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4550c30b5467461ab8a449dc3a8d10f8","value":1}},"f22fec2e982e4c979be4537208671dea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35df4bda9fd149ef9ad2b5fee10285ed","placeholder":"​","style":"IPY_MODEL_4b4268076448459fa5c6433b8495fadd","value":" 350/0 [00:00&lt;00:00, 1510.47 examples/s]"}},"af493325bccc4c5a99fe70b9f41bf499":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc1fcd3452c46b3bde01875e210e18d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316061152b03413d9a4e43435de573d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c19d2947d3d42f2b67ed72921890ab3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4550c30b5467461ab8a449dc3a8d10f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35df4bda9fd149ef9ad2b5fee10285ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b4268076448459fa5c6433b8495fadd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40dd6043e1bf43d981ddda95436a7855":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2e3ce5a2a394de6b4c499df0831c67f","IPY_MODEL_7a473a291e75457596dc53ebfaf125ff","IPY_MODEL_ad60b6e5aa284523aac81d7d59be48ca"],"layout":"IPY_MODEL_428a34588ce64d999a61873d482a3ffe"}},"b2e3ce5a2a394de6b4c499df0831c67f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_288ed44e1fb44c7c98c68d884790c533","placeholder":"​","style":"IPY_MODEL_b1079f260c014837857c788fe4c0268f","value":"Generating test split: "}},"7a473a291e75457596dc53ebfaf125ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d20e0762bf34aeba125d9c2d0cf2e8d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1606b6fd265549a696e4d83318441f6f","value":1}},"ad60b6e5aa284523aac81d7d59be48ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99f0bfc71b3e43bb860deafb0f1cf97e","placeholder":"​","style":"IPY_MODEL_4a8ee0d2c332434887e9df5b6abcffa9","value":" 100/0 [00:00&lt;00:00, 954.75 examples/s]"}},"428a34588ce64d999a61873d482a3ffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"288ed44e1fb44c7c98c68d884790c533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1079f260c014837857c788fe4c0268f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d20e0762bf34aeba125d9c2d0cf2e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1606b6fd265549a696e4d83318441f6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99f0bfc71b3e43bb860deafb0f1cf97e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a8ee0d2c332434887e9df5b6abcffa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c9e11bff2b043e0bd35c33e03ce5027":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8288c4232a9e436b9d44de4bb5f5c5af","IPY_MODEL_66c98d11501748869e89ebffa7be3507","IPY_MODEL_4d9d06beab9c4344b95ac75770ddb925"],"layout":"IPY_MODEL_71835a3622a34b52b80e6f382c849379"}},"8288c4232a9e436b9d44de4bb5f5c5af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9711d6eb6ff44ac0b202b7ff551f47d9","placeholder":"​","style":"IPY_MODEL_5e41e910f6f147b2921a87c9e6445738","value":"Generating validation split: "}},"66c98d11501748869e89ebffa7be3507":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c369b24123b844e0a3d06ddb83465154","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24ae707eeba145adaa94363731a99af6","value":1}},"4d9d06beab9c4344b95ac75770ddb925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04c79e97f93c43ed840730b0cfd62673","placeholder":"​","style":"IPY_MODEL_3f46caa2446645e2aa15160ab8e3fb27","value":" 50/0 [00:00&lt;00:00, 655.88 examples/s]"}},"71835a3622a34b52b80e6f382c849379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9711d6eb6ff44ac0b202b7ff551f47d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e41e910f6f147b2921a87c9e6445738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c369b24123b844e0a3d06ddb83465154":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"24ae707eeba145adaa94363731a99af6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04c79e97f93c43ed840730b0cfd62673":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f46caa2446645e2aa15160ab8e3fb27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0df9d3df214346f787b290a2310948d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_58d8e5d6b2a84d34ba0f58ace8274856","IPY_MODEL_dc7c37f4d54540e1903f27ef18a35634","IPY_MODEL_6c66bea2b4a2496ebb9b32827f9d182b"],"layout":"IPY_MODEL_c094af9bca8e4c118d8246e0f4356a64"}},"58d8e5d6b2a84d34ba0f58ace8274856":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37c80adea0964de88faefe492a3d23f9","placeholder":"​","style":"IPY_MODEL_9a71af8dd1714817addc8dc2649fa054","value":"tokenizer_config.json: 100%"}},"dc7c37f4d54540e1903f27ef18a35634":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a2deb70746b4b4ebf4596facef53c76","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3d952e4a8524e0c854d0275875e2b39","value":48}},"6c66bea2b4a2496ebb9b32827f9d182b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_105ea0ec11d24578880f76705dad0d82","placeholder":"​","style":"IPY_MODEL_39f7820db1c34b8891886123c6160bff","value":" 48.0/48.0 [00:00&lt;00:00, 3.84kB/s]"}},"c094af9bca8e4c118d8246e0f4356a64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37c80adea0964de88faefe492a3d23f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a71af8dd1714817addc8dc2649fa054":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a2deb70746b4b4ebf4596facef53c76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d952e4a8524e0c854d0275875e2b39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"105ea0ec11d24578880f76705dad0d82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f7820db1c34b8891886123c6160bff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cc1bfc9b1c648ffab9a2687d4a751e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17c5adc983c64c60a60dc6cffbd9659e","IPY_MODEL_96a648e6594c404b91df95e9b937fd2e","IPY_MODEL_9b17588eaca84e95a3f4462835e95dab"],"layout":"IPY_MODEL_4ea990e887ae44c19e0c10f9ac53a3fb"}},"17c5adc983c64c60a60dc6cffbd9659e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c5f8dbf1d2748babdac9f95c29ff1ce","placeholder":"​","style":"IPY_MODEL_581ed6aae14e400084795bc4f2b503d0","value":"config.json: 100%"}},"96a648e6594c404b91df95e9b937fd2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b0b0e7c71634baf8ba8b9bee18279cf","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aeee6896b65a4f178067c1115544f4f1","value":570}},"9b17588eaca84e95a3f4462835e95dab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d12ae979e994ace8405e4b9334da70c","placeholder":"​","style":"IPY_MODEL_6dbd3f2b9bb44b1db994224a28b9485c","value":" 570/570 [00:00&lt;00:00, 26.6kB/s]"}},"4ea990e887ae44c19e0c10f9ac53a3fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c5f8dbf1d2748babdac9f95c29ff1ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"581ed6aae14e400084795bc4f2b503d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b0b0e7c71634baf8ba8b9bee18279cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aeee6896b65a4f178067c1115544f4f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d12ae979e994ace8405e4b9334da70c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dbd3f2b9bb44b1db994224a28b9485c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41a694f139b1423c820dc6405f267cf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cab16b0cc3924216bb3947f0652136a0","IPY_MODEL_e2045f0eeb294778a2a81c881a53b311","IPY_MODEL_d2c3d45857f54685ba348fae60c1b777"],"layout":"IPY_MODEL_f4c157927bbe45289d439abe9a7240f4"}},"cab16b0cc3924216bb3947f0652136a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d977fbff9dc4c01994201fa25b8d468","placeholder":"​","style":"IPY_MODEL_b60ee8a711274cc0a723cdba239e05b8","value":"vocab.txt: 100%"}},"e2045f0eeb294778a2a81c881a53b311":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb676dcfdb774d798e1185a2efa96333","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f3d2fcacb9946578e35e366081daeda","value":231508}},"d2c3d45857f54685ba348fae60c1b777":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4233a54f56904bf096ea0112ef98e320","placeholder":"​","style":"IPY_MODEL_fb38363f579941529f5de9f6f45528e5","value":" 232k/232k [00:00&lt;00:00, 525kB/s]"}},"f4c157927bbe45289d439abe9a7240f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d977fbff9dc4c01994201fa25b8d468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b60ee8a711274cc0a723cdba239e05b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb676dcfdb774d798e1185a2efa96333":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f3d2fcacb9946578e35e366081daeda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4233a54f56904bf096ea0112ef98e320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb38363f579941529f5de9f6f45528e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c11ae8d992e045ec9d5c4928cefa2b02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_494761c4d8b64f80bd9c69436b029ace","IPY_MODEL_981976df85cf422395205200ab6a41c8","IPY_MODEL_8c4bef2ae68742dc9a448743701e0695"],"layout":"IPY_MODEL_c3b3422be59541cc9d05558aace32379"}},"494761c4d8b64f80bd9c69436b029ace":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_852b9ebaf275492d82ac31ee98384afe","placeholder":"​","style":"IPY_MODEL_f4568a803ff94a039ee7f3d71ed47737","value":"tokenizer.json: 100%"}},"981976df85cf422395205200ab6a41c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_55e2c16cd1cd47c1880a984c43f54ccb","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93f11afb14314306817bdf54489488b2","value":466062}},"8c4bef2ae68742dc9a448743701e0695":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a011c2245be84c06b0e95a0820490310","placeholder":"​","style":"IPY_MODEL_877f6de33f9c43f2b32844ca44ad7e2c","value":" 466k/466k [00:00&lt;00:00, 714kB/s]"}},"c3b3422be59541cc9d05558aace32379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"852b9ebaf275492d82ac31ee98384afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4568a803ff94a039ee7f3d71ed47737":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55e2c16cd1cd47c1880a984c43f54ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f11afb14314306817bdf54489488b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a011c2245be84c06b0e95a0820490310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"877f6de33f9c43f2b32844ca44ad7e2c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## **Import Lib**\n","\n"],"metadata":{"id":"-G5f-9NGbFDG"}},{"cell_type":"code","source":["%%capture\n","!pip install datasets"],"metadata":{"id":"4fJxRfWYOHe4","executionInfo":{"status":"ok","timestamp":1714621519815,"user_tz":-420,"elapsed":14137,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import os\n","import ast\n","import json\n","import pandas as pd\n","from datasets import load_dataset\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AutoTokenizer, BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"GPT0iXG9OGnK","executionInfo":{"status":"ok","timestamp":1714621527567,"user_tz":-420,"elapsed":7758,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcM0w3BxHHqk","executionInfo":{"status":"ok","timestamp":1714621561157,"user_tz":-420,"elapsed":33602,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"002f8239-0e19-4e76-d7e9-4784bc0f7f3e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/2_PBL7/\")"],"metadata":{"id":"sE4cC_AXbEVK","executionInfo":{"status":"ok","timestamp":1714621561157,"user_tz":-420,"elapsed":5,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## **Init**"],"metadata":{"id":"pJc2TmYwbCjg"}},{"cell_type":"code","source":["dataset_names = ['inspec', 'se-2010', 'se-2017', 'kp20k']"],"metadata":{"id":"dBnIydssbBRi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Load Data**"],"metadata":{"id":"9lSGCcKWbHz6"}},{"cell_type":"markdown","source":["- input: abstract\n","- label: danh sách các keyword"],"metadata":{"id":"FeGoNsVebmvc"}},{"cell_type":"markdown","source":["Nguồn data:\n","- Inspec: https://huggingface.co/datasets/midas/inspec?row=0\n","- SE-2010: https://huggingface.co/datasets/taln-ls2n/semeval-2010-pre?row=0\n","- SE-2017: https://huggingface.co/datasets/midas/semeval2017"],"metadata":{"id":"EShIZ-JZz3at"}},{"cell_type":"markdown","source":["Hoặc sử dụng kp20k: gồm 530809 rows dữ liệu"],"metadata":{"id":"12HqQ_EI_ZAT"}},{"cell_type":"markdown","source":["### KP20K"],"metadata":{"id":"9Nsy6SLTNGdr"}},{"cell_type":"code","source":["# !unzip \"./Phraserformer/data.zip\" -d \"./Phraserformer\""],"metadata":{"id":"VDuOGhLzNNEw","executionInfo":{"status":"ok","timestamp":1714621676855,"user_tz":-420,"elapsed":4,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Đường dẫn tới thư mục chứa các file JSON\n","folder_path = \"./Phraserformer/data/dataset/kp20k\"\n","\n","# Khởi tạo một danh sách để lưu trữ dữ liệu từ các file JSON\n","data_list = []\n","\n","# Lặp qua tất cả các file trong thư mục\n","for filename in os.listdir(folder_path):\n","    # Kiểm tra nếu file có đuôi là \".json\"\n","    if filename.endswith(\".json\"):\n","        # Đường dẫn đến file JSON\n","        file_path = os.path.join(folder_path, filename)\n","        # Mở file và load nội dung\n","        with open(file_path, \"r\") as file:\n","            # Thêm dữ liệu từ file vào danh sách\n","            data_list.extend([json.loads(line) for line in file])"],"metadata":{"id":"PFE1iSOkbqM4","executionInfo":{"status":"ok","timestamp":1714621702035,"user_tz":-420,"elapsed":25183,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(len(data_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfvWm1z_Qwuq","executionInfo":{"status":"ok","timestamp":1714621702035,"user_tz":-420,"elapsed":10,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"3db34da2-0383-4f6a-e58a-3f185f5b523e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["570809\n"]}]},{"cell_type":"code","source":["print(data_list[0:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBbxn0nCR3f2","executionInfo":{"status":"ok","timestamp":1714621702035,"user_tz":-420,"elapsed":8,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"0a3f7298-4d69-4caa-cb2a-40cb14dc9683"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'abstract': 'This paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application. Here, distance may refer either to space ( e.g. in a remote synchronous collaboration) or time ( e.g. during playback of recorded actions). Our approach consists in immersing the application in a virtual inhabited 3D space and mimicking user actions by animating avatars. We illustrate this approach with two applications, the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions. We suggest this could be a low cost enhancement for telepresence.', 'keyword': 'telepresence;animation;avatars;application sharing;collaborative virtual environments', 'title': 'virtually enhancing the perception of user actions'}, {'abstract': 'This paper presents an improved architecture of the multistage multibit sigma-delta modulators (EAMs) for wide-band applications. Our approach is based on two resonator topologies, high-Q cascade-of-resonator-with-feedforward (HQCRFF) and low-Q cascade-of-integrator-with-feedforward (LQCEFF). Because of in-band zeros introduced by internal loop filters, the proposed architecture enhances the suppression of the in-band quantization noise at a low OSR. The HQCRFF-based modulator with single-bit quantizer has two modes of operation, modulation and oscillation. When the HQCRFF-based modulator is operating in oscillation mode, the feedback path from the quantizer output to the input summing node is disabled and hence the modulator output is free of the quantization noise terms. Although operating in oscillation mode is not allowed for single-stage SigmaDeltaM, the oscillation of HQCRFF-based modulator can improve dynamic range (DR) of the multistage (MASH) SigmaDeltaM. The key to improving DR is to use HQCRFF-based modulator in the first stage and have the first stage oscillated. When the first stage oscillates, the coarse quantization noise vanishes and hence circuit nonidealities, such as finite op-amp gain and capacitor mismatching, do not cause leakage quantization noise problem. According to theoretical and numerical analysis, the proposed MASH architecture can inherently have wide DR without using additional calibration techniques.', 'keyword': 'sigma delta modulators;analog-to-digital converters (adcs);multistage (mash);multibit quantizer;dynamic range improvement', 'title': 'Dynamic range improvement of multistage multibit Sigma Delta modulator for low oversampling ratios'}, {'abstract': 'In this paper, we discuss the motivation and the fundamentals of an ontology representation of business reporting data and metadata structures as defined in the eXtensible business reporting language (XBRL) standard. The core motivation for an ontology representation is the enhanced potential for integrated analytic applications that build on quantitative reporting data combined with structured and unstructured data from additional sources. Applications of this kind will enable significant enhancements in regulatory compliance management, as they enable business analytics combined with inference engines for statistical, but also for logical inferences. In order to define a suitable ontology representation of business reporting language structures, an analysis of the logical principles of the reporting metadata taxonomies and further classification systems is presented. Based on this analysis, a representation of the generally accepted accounting principles taxonomies in XBRL by an ontology provided in the web ontology language (OWL) is proposed. An additional advantage of this representation is its compliance with the recent ontology definition metamodel (ODM) standard issued by OMG.', 'keyword': 'enterprise information integration and interoperability;languages for conceptual modelling;ontological approaches to content and knowledge management;ontology-based software engineering for enterprise solutions;domain engineering', 'title': 'An ontology modelling perspective on business reporting'}, {'abstract': 'An overview of the self-organizing map algorithm, on which the papers in this issue are based, is presented in this article.', 'keyword': 'self-organizing map;learning vector quantization', 'title': 'The self-organizing map'}, {'abstract': 'The amygdala comprises part of an extended network of neural circuits that are critically involved in the processing of socially salient stimuli. Such stimuli may be explicitly social, such as facial expressions, or they may be only tangentially social, such as abstract shapes moving with apparent intention relative to one another. The coordinated interplay between neural activity in the amygdala and other brain regions, especially the medial prefrontal cortex, the occipitofrontal cortex, the fusiform gyrus, and the superior temporal sulcus, allows us to develop social responses and to engage in social behaviors appropriate to our species. The harmonious functioning of this integrated social cognitive network may be disrupted by congenital or acquired lesions, by genetic anomalies, and by exceptional early experiences. Each form of disruption is associated with a slightly different outcome, dependent on the timing of the experience, the location of the lesion, or the nature of the genetic anomaly. Studies in both humans and primates concur; the dysregulation of basic emotions, especially the processing of fear and anger, is an almost invariable consequence of such disruption. These, in turn, have direct or indirect consequences for social behavior.', 'keyword': 'social brain;amygdala;behavior;facial expression', 'title': 'The Amygdala and Development of the Social Brain'}, {'abstract': 'A movement based location update (MBLU) scheme is an LU scheme, under which a user equipment (UE) performs an LU when the number of cells crossed by the UE reaches a movement threshold. The MBLU scheme suffers from ping-pong LU effect. The ping-pong LU effect arises when the UE that moves repetitively between two adjacent cells performs LUs in the same way as in the case of straight movement. To tackle the ping-pong LU effect encountered by the original MBLU (OMBLU) scheme, an improved MBLU (IMBLU) scheme was proposed in the literature. Under the IMBLU scheme, the UE performs an LU when the number of different cells, rather than the number of cells, visited by the UE reaches the movement threshold. In this paper we develop an embedded Markov chain model to calculate the signaling cost of the IMBLU scheme. We derive analytical formulas for the signaling cost, whose accuracy is tested through simulation. It is observed from a numerical study based on these formulas that 1) the signaling cost is a downward convex function of the movement threshold, implying the existence of an optimal movement threshold that can minimize the signaling cost, and that 2) the reduction in the signaling cost achieved by the IMBLU scheme relative to the OMBLU scheme is more prominent in the case of stronger repetitiveness in the UE movement. The model developed and the formulas derived in this paper can guide the implementation of the IMBLU scheme in wireless communication networks.', 'keyword': 'movement-based location update;ping-pong lu effect;modeling;embedded markov chain', 'title': 'Modeling and Cost Analysis of an Improved Movement-Based Location Update Scheme in Wireless Communication Networks'}, {'abstract': 'In order to realize a high resolution and high throughput printing method for thin film transistor application, a modified offset roll printing was studied. This roll printing chiefly consists of a blanket with low surface energy and a printing plate (clich) with high surface energy. In this study, a finite element analysis was done to predict the blanket deformation and to find the optimal angle of clichs sidewall. Various etching methods were investigated to obtain a high resolution clich and the surface energy of the blanket and clich was analyzed for ink transfer. A high resolution clich with the sidewall angle of 90 and the intaglio depth of 13?m was fabricated by the deep reactive ion etching method. Based on the surface energy analysis, we extracted the most favorable condition to transfer inks from a blanket to a clich, and thus thin films were deposited on a Si-clich to increase the surface energy. Through controlling roll speed and pressure, two inks, etch-resist and silver paste, were printed on a rigid substrate, and the fine patterns of 10?m width and 6?m line spacing were achieved. By using this printing process, the top gate amorphous indiumgalliumzinc-oxide TFTs with channel width/length of 12/6?m were successfully fabricated by printing etch-resists.', 'keyword': 'printing plate (clich);surface energy;offset roll printing;tft', 'title': 'A modified offset roll printing for thin film transistor applications'}, {'abstract': 'In this paper, we obtain several tight bounds of the defensive k k -alliance number in the complement graph from other parameters of the graph. In particular, we investigate the relationship between the alliance numbers of the complement graph and the minimum and maximum degree, the domination number and the isoperimetric number of the graph. Moreover, we prove the NP-completeness of the decision problem underlying the defensive k k -alliance number.', 'keyword': '05c69;15c05', 'title': 'On the complement graph and defensive k k -alliances'}, {'abstract': 'Efficient segmentation of hyperspectral images through the use of cellular automata. The rule set for the CA is automatically obtained using an evolutionary algorithm. Synthetic images of much lower dimensionality are used to evolve the automata. The CA works with spectral information but do not project it onto a lower dimension. The CA-based classification outperforms reference techniques.', 'keyword': 'hyperspectral imaging;evolution;cellular automata;segmentation', 'title': 'Hyperspectral image segmentation through evolved cellular automata'}, {'abstract': 'Bluetooth (BT) is a leading technology for the deployment of wireless Personal Area Networks and Body Area Networks. Versions 2.0 and 2.1 of the standard, which are massively implemented in commercial devices, improve the throughput of the BT technology by enabling the so-called Enhanced Data Rates (EDR). EDRs are achieved by utilizing new modulation techniques (?/4-DQPSK and 8-DPSK), apart from the typical Gaussian Frequency Shift Keying modulation supported by previous versions of BT. This manuscript presents and validates a model to characterize the impact of white noise on the performance of these modulations. The validation is systematically accomplished in a testbed with actual BT interfaces and a calibrated white noise generator.', 'keyword': 'bluetooth;bit error rate;modulation;white noise', 'title': 'Analytical and empirical evaluation of the impact of Gaussian noise on the modulations employed by Bluetooth Enhanced Data Rates'}]\n"]}]},{"cell_type":"code","source":["# Creating DataFrame from list of dictionaries\n","kp20k_df = pd.DataFrame(data_list)"],"metadata":{"id":"dW8M50Ijs4l2","executionInfo":{"status":"ok","timestamp":1714621702036,"user_tz":-420,"elapsed":8,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["kp20k_df['keyword'] = kp20k_df['keyword'].apply(lambda x: x.split(\";\"))"],"metadata":{"id":"CNVuRfWm9EqP","executionInfo":{"status":"ok","timestamp":1714621703879,"user_tz":-420,"elapsed":1849,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Displaying the DataFrame\n","kp20k_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Tcvu6EFotMzx","executionInfo":{"status":"ok","timestamp":1714621703879,"user_tz":-420,"elapsed":8,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"8ca10c92-bbaf-4c39-bc69-19960620094f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  This paper proposes using virtual reality to e...   \n","1  This paper presents an improved architecture o...   \n","2  In this paper, we discuss the motivation and t...   \n","3  An overview of the self-organizing map algorit...   \n","4  The amygdala comprises part of an extended net...   \n","\n","                                             keyword  \\\n","0  [telepresence, animation, avatars, application...   \n","1  [sigma delta modulators, analog-to-digital con...   \n","2  [enterprise information integration and intero...   \n","3  [self-organizing map, learning vector quantiza...   \n","4  [social brain, amygdala, behavior, facial expr...   \n","\n","                                               title  \n","0  virtually enhancing the perception of user act...  \n","1  Dynamic range improvement of multistage multib...  \n","2  An ontology modelling perspective on business ...  \n","3                            The self-organizing map  \n","4   The Amygdala and Development of the Social Brain  "],"text/html":["\n","  <div id=\"df-08b090ce-5ae3-4671-87c4-c8a66a4f4111\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","      <th>title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This paper proposes using virtual reality to e...</td>\n","      <td>[telepresence, animation, avatars, application...</td>\n","      <td>virtually enhancing the perception of user act...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>This paper presents an improved architecture o...</td>\n","      <td>[sigma delta modulators, analog-to-digital con...</td>\n","      <td>Dynamic range improvement of multistage multib...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In this paper, we discuss the motivation and t...</td>\n","      <td>[enterprise information integration and intero...</td>\n","      <td>An ontology modelling perspective on business ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>An overview of the self-organizing map algorit...</td>\n","      <td>[self-organizing map, learning vector quantiza...</td>\n","      <td>The self-organizing map</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The amygdala comprises part of an extended net...</td>\n","      <td>[social brain, amygdala, behavior, facial expr...</td>\n","      <td>The Amygdala and Development of the Social Brain</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08b090ce-5ae3-4671-87c4-c8a66a4f4111')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-08b090ce-5ae3-4671-87c4-c8a66a4f4111 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-08b090ce-5ae3-4671-87c4-c8a66a4f4111');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c453dcca-a503-4031-8386-7ac19c495ca7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c453dcca-a503-4031-8386-7ac19c495ca7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c453dcca-a503-4031-8386-7ac19c495ca7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"kp20k_df"}},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### Inspec"],"metadata":{"id":"eHaaE22AOi5a"}},{"cell_type":"code","execution_count":13,"metadata":{"id":"FXtxUg1TaonT","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["4807c0a740364d7d826a63d0a8f42d26","18f4ad588db4489c8af7c02d8d4444e1","9118c871d66b45278be535da85ac9014","9a94aeeb8845487ca66fc9f411bf5dc8","08849573294148e59178a9386d4e9890","ef64c9aa0f864fbdb2ccbf043ff289f1","e881d4cc25f54a3b9f43b3385c816e00","6a688ad3a6984ca7a152d6d51dc43f2f","d57132d1b4b5471b8144f831be641938","5c648eeeda12465483ed0ff43a5acb09","84b9ffa0b16a4bc2a198c55bd629fb24","b0f219fdcc414b649525f8c8e663e433","7bfacc37289b473389f2128b089048cc","1174df44bc4d4410b8ae778337579905","599818596deb41f08afe8505d81a5655","5c785dd9960d41d3a9d0fb6841a71390","5e27f669f85b4c77bfd86f5bab83dea8","08f6b64bca9f4605b353974b2c31b23e","14d87105daf5497f994e64e6ca24c33a","69b220314e48407ca3d25860cdaa7b00","da9b1726fa624883926889914214fb6b","9fbfbf63e8b34c76ab5192bd0bcf8a11","56814bbbf0e648c3a38c5fab9db59d22","015b18178715463a950fc90fc7bd6d77","71dccce3005245e0a4f834541834e3d9","91ab992e8f4e41609e1fd1ab7da3d0e1","48b9533744d54020ab166f6fe2fe0ad5","c5e9e59c93124ff8a7054965d2a9cfae","5fbb7b744eb5484084515a389a37f6ad","c37944e53a294d1eb86d9c0f666c0877","9da78ce16d6f4501875931ff398ec595","9b30755e04034d949d98374680e856ac","106f4d8807ac40549b1211d6190854ab","a768b85bf8c74991ba080842ff26f379","10691b1a88fa40f7b9a1ee84626b17da","f1ca95880d234949a7d78142f484e099","57f98fa7c39949d19c91275c3df776e9","ccb71b44355b452aa07972417c66f0cc","71586d6ba19b42278ea369ccf6e8fa1b","34db1a8622de41f88a595cb19c0ab935","b2fb582b765a4b31807729a36a44ea0c","63df77301027474282a9c56ac76201ed","b6bc1e8cd0c947eb94d1eff4ee9d6a49","acc04ab214b44407a181513dcd9ac232","06036ac58070471fb859c4828ce00221","3536ce2ecdd9424a8e31f4c267d3e8ef","1d9c2e189d9c41ce8cf8739197367004","74eeeaaa6f1b4b8d82b14755374055d5","eebaa0b3fdfd43d4b11f41bf25d34ec7","6ae75ce49be540e998a7f514f1ea0f5a","80938c72b538433996eb7c1679801a80","fd1edaf12653462c8f3025efd2d35c20","cf1e4c899c604a82b44c7a656de24702","d42674de1c3747c7af353e6324b3643b","7393a7f74b3c449c8d1e4af9a3fc7c3c","1d391c53512c408bb09f7cbb6e85fb45","5ba9128e0f8947a89e26fead7bec59bb","8ce4cf246d79494881a0dbd26ed3cd9d","e27a15d70024402bacce8e5d010cffe5","f1b28040988843e0b53257dfbf3260f3","8f12c2558d5749c0b94f98cc3a65d903","0d64202a42d54243b9829002631171e5","88ad359986b74e0880c8efe4a3100eca","b56c5a2326a441f28ff29cd63e783b36","7599f4c4477d48f7b76aff0db20b7abb","c698120e7e7c462db470314a089c0cbf","1824e8bf6cfc4516958c1342a18d1e7c","91eb2bd0bcce4c0a9cc08b5ffe57db5e","aff1fb01947e4b098cd1a9ea5729a384","1d414fc2c4d34f449bc1a69a16aeca8e","e24e1fcfb1434617b7b0f85e390ff014","a75f32999d39432b9347db5059379484","3abaf195410848a18902d41002153ad3","3a4699f3c91b4a73b12d7026708665cc","737188706cd3427988d9789f0334fbd0","a1b271ecf364432ba6589be057c65e36","46ff8f6b74234dd3809f1e504f51177e","59dda6778412461db3c49fd31322e304","bb0d39fc251b43adaa4536526d030e0b","1bbec29662cf474899969c07360007e8","94cf8b0abf2047928a07c9fa76e8f50a","a30354060cae4660b08ad2c49f431d5e","109fb977f50443fcb15c33ebf1b4a1d1","c1e810cb39934e32ab0cac8a316741be","7278a6b1508e4ec299b420722acd1a49","53aca80b1d90441585847ebb92251916","f512141852ee4adca3561bafd5be85f0","0b53dea588a9478188b8b39c3acb8a81"],"height":0},"executionInfo":{"status":"ok","timestamp":1714621715420,"user_tz":-420,"elapsed":11547,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"2fd84931-aa90-484e-b800-a8df00dfbf16"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for midas/inspec contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/midas/inspec\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.47k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4807c0a740364d7d826a63d0a8f42d26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/25.4k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f219fdcc414b649525f8c8e663e433"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Repo card metadata block was not found. Setting CardData to empty.\n","WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56814bbbf0e648c3a38c5fab9db59d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a768b85bf8c74991ba080842ff26f379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06036ac58070471fb859c4828ce00221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d391c53512c408bb09f7cbb6e85fb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1824e8bf6cfc4516958c1342a18d1e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59dda6778412461db3c49fd31322e304"}},"metadata":{}}],"source":["inspec_ds = load_dataset(\"midas/inspec\")"]},{"cell_type":"code","source":["print(inspec_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhlllG1GOh0J","executionInfo":{"status":"ok","timestamp":1714621715421,"user_tz":-420,"elapsed":5,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"95578068-e7af-48c9-d246-3c5e6179a4d9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 1000\n","    })\n","    test: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 500\n","    })\n","    validation: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 500\n","    })\n","})\n"]}]},{"cell_type":"code","source":["train_inspec_df = pd.DataFrame(inspec_ds['train'])\n","validation_inspec_df = pd.DataFrame(inspec_ds['validation'])\n","test_inspec_df = pd.DataFrame(inspec_ds['test'])"],"metadata":{"id":"F5PssJEyVROf","executionInfo":{"status":"ok","timestamp":1714621715893,"user_tz":-420,"elapsed":6,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["all_inspec_df = pd.concat([train_inspec_df, validation_inspec_df, test_inspec_df], ignore_index=True)"],"metadata":{"id":"RPMs0D-sVAK6","executionInfo":{"status":"ok","timestamp":1714621715893,"user_tz":-420,"elapsed":5,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(all_inspec_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IyLdxSywV7V3","executionInfo":{"status":"ok","timestamp":1714621715893,"user_tz":-420,"elapsed":5,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"373d36a7-e561-4d4e-fd54-6dbefc3aee89"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["        id                                           document  \\\n","0     1001  [A, conflict, between, language, and, atomisti...   \n","1     1002  [Selective, representing, and, world-making, W...   \n","2     1000  [Does, classicism, explain, universality, ?, A...   \n","3      100  [Separate, accounts, go, mainstream, -LSB-, in...   \n","4     1012  [Evolving, receptive-field, controllers, for, ...   \n","...    ...                                                ...   \n","1995   402  [Fast, frequency, acquisition, phase-frequency...   \n","1996   392  [Time-varying, properties, of, renal, autoregu...   \n","1997   384  [Brightness-independent, start-up, routine, fo...   \n","1998    37  [Design, PID, controllers, for, desired, time-...   \n","1999   411  [CAD/CAE, software, aids, converter, design, -...   \n","\n","                                           doc_bio_tags  \n","0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","1     [B, I, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","2     [O, B, O, B, O, O, O, O, O, B, I, I, I, O, O, ...  \n","3     [O, O, O, O, O, B, O, O, O, O, O, O, O, O, O, ...  \n","4     [O, O, O, O, B, I, O, O, O, B, I, O, O, O, O, ...  \n","...                                                 ...  \n","1995  [B, I, I, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","1996  [O, O, O, B, I, I, O, O, O, O, O, O, O, O, O, ...  \n","1997  [O, O, O, O, B, I, B, I, I, O, O, O, O, O, O, ...  \n","1998  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","1999  [O, O, O, O, O, O, O, O, O, O, O, O, B, I, I, ...  \n","\n","[2000 rows x 3 columns]\n"]}]},{"cell_type":"code","source":["def keyword_list(x):\n","  keywords = set()\n","  i = 0\n","  while i<len(x['doc_bio_tags']):\n","    if x['doc_bio_tags'][i]=='B':\n","      keyword = [x['document'][i]]\n","      while (i+1<len(x['doc_bio_tags']) and x['doc_bio_tags'][i+1]=='I'):\n","        keyword.append(x['document'][i+1])\n","        i+=1\n","      keyword = ' '.join(keyword)\n","      keywords.add(keyword)\n","    i+=1\n","  return list(keywords)\n","keyword_list(all_inspec_df.loc[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_SjZHzTYq5Y","executionInfo":{"status":"ok","timestamp":1714621715893,"user_tz":-420,"elapsed":4,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"55c00cfa-c360-4ab2-9c6f-a0d4d50d17b5"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Content Atomism',\n"," 'philosophy of mind',\n"," 'IBS',\n"," 'cognitive states',\n"," 'LOT',\n"," 'beliefs',\n"," 'desires',\n"," 'Language of Thought']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Tạo dataframe mới\n","inspec_df = pd.DataFrame()\n","\n","# Cột abstract là ghép các phần tử trong document thành chuỗi\n","inspec_df['abstract'] = all_inspec_df['document'].apply(' '.join)\n","\n","# Cột keyword lấy các từ có doc_bio_tags = 'B'\n","inspec_df['keyword'] = all_inspec_df.apply(lambda x: keyword_list(x), axis=1)\n","\n"],"metadata":{"id":"_6ajPXf_WG-i","executionInfo":{"status":"ok","timestamp":1714621717517,"user_tz":-420,"elapsed":1627,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(inspec_df.iloc[0]['abstract'])\n","print(inspec_df.iloc[0]['keyword'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wLZueeJtkeI","executionInfo":{"status":"ok","timestamp":1714621717517,"user_tz":-420,"elapsed":8,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"12ceffb9-ae2c-4daa-da4f-509ef742d33a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\n","['Content Atomism', 'philosophy of mind', 'IBS', 'cognitive states', 'LOT', 'beliefs', 'desires', 'Language of Thought']\n"]}]},{"cell_type":"code","source":["inspec_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"hfdLpa0BrUqA","executionInfo":{"status":"ok","timestamp":1714621717518,"user_tz":-420,"elapsed":7,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"e85ab509-278d-4299-97bd-385bbcecf2f2"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  A conflict between language and atomistic info...   \n","1  Selective representing and world-making We dis...   \n","2  Does classicism explain universality ? Argumen...   \n","3  Separate accounts go mainstream -LSB- investme...   \n","4  Evolving receptive-field controllers for mobil...   \n","\n","                                             keyword  \n","0  [Content Atomism, philosophy of mind, IBS, cog...  \n","1  [realism, Selective representing, selective re...  \n","2  [human cognition, connectionist models, univer...  \n","3           [independent money managers, investment]  \n","4  [nonlinear interactions, evolutionary methods,...  "],"text/html":["\n","  <div id=\"df-5f366a91-becd-4da7-ad96-7d3b60ed6545\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A conflict between language and atomistic info...</td>\n","      <td>[Content Atomism, philosophy of mind, IBS, cog...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Selective representing and world-making We dis...</td>\n","      <td>[realism, Selective representing, selective re...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does classicism explain universality ? Argumen...</td>\n","      <td>[human cognition, connectionist models, univer...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Separate accounts go mainstream -LSB- investme...</td>\n","      <td>[independent money managers, investment]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Evolving receptive-field controllers for mobil...</td>\n","      <td>[nonlinear interactions, evolutionary methods,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f366a91-becd-4da7-ad96-7d3b60ed6545')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5f366a91-becd-4da7-ad96-7d3b60ed6545 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5f366a91-becd-4da7-ad96-7d3b60ed6545');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-67c43f73-038a-4340-bf98-f1c1f3146c71\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67c43f73-038a-4340-bf98-f1c1f3146c71')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-67c43f73-038a-4340-bf98-f1c1f3146c71 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"inspec_df","summary":"{\n  \"name\": \"inspec_df\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          \"Presentation media , information complexity , and learning outcomes Multimedia computing provides a variety of information presentation modality combinations . Educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and , therefore , comprehension . The article reports the findings from a 3 -LRB- text-only , overhead slides , and multimedia presentation -RRB- * 2 -LRB- high and low information complexity -RRB- factorial experiment . Subjects read a text script , viewed an acetate overhead slide presentation , or viewed a multimedia presentation depicting the greenhouse effect -LRB- low complexity -RRB- or photocopier operation -LRB- high complexity -RRB- . Multimedia was superior to text-only and overhead slides for comprehension . Information complexity diminished comprehension and perceived presentation quality . Multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation . These findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity . Moreover , multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention ; the quality and effectiveness of information processing attained -LRB- i.e. , use of verbal and visual working memory -RRB- is also significant\",\n          \"The crossing number of P -LRB- N , 3 -RRB- It is proved that the crossing number of the generalized Petersen graph P -LRB- 3k + h , 3 -RRB- is k + h if h in -LCB- 0 , 2 -RCB- and k + 3 if h = 1 , for each k > or = 3 , with the single exception of P -LRB- 9,3 -RRB- , whose crossing number is 2\",\n          \"Application of normal possibility decision rule to silence The paper presents the way of combining two decision problems concerning a single -LRB- or a common -RRB- dimension , so that an effective fuzzy decision rule can be obtained . Normality of the possibility distribution is assumed , leading to possibility of fusing the respective functions related to the two decision problems and their characteristics -LRB- decisions , states of nature , utility functions , etc. -RRB- . The approach proposed can be applied in cases when the statement of the problem requires making of more refined distinctions rather than considering simply a bi-criterion or bi-utility two-decision problem\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### SE-2010"],"metadata":{"id":"NC5q4sWieGjE"}},{"cell_type":"code","source":["se2010_ds = load_dataset(\"taln-ls2n/semeval-2010-pre\")"],"metadata":{"id":"ojicR38DeGDt","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["89474a9b91404f0c908284d7e37d0fcc","27bd81129a2d477b9179768427ce39d4","05a26df4c8e14cf0b8cfc7f499c7a435","a55f7dcf95f640c6878611d6f0fcf97d","89666fdabace4fc4b4894ea283b63ad0","ea3a93424d354957bddaac1ba5160598","e7ae4986296049aba950fd177cd37e93","4ffae4b43e5a4f95aeaa81379a65eec9","fb580eacbf544f05813800839893e7ce","2fbed27dabb14747ab0fba9fae40dbe4","f0892dd020cb4600a1e639a83d3f89a4","2cd76f982477477aaa13d4f5760075ef","bc65301a48dc46fab6a250061b95a298","d0907516cd954b629c806530a72e19c0","f678c5bcb86c4522b2165d857011bfc5","fa255a5d7fce44579b49ff9b9a160ae4","6d9c5b38b45c45c2b01cb11accfdaa0d","eeda85ffb6ec45db8757fa1dd00fad1c","f36d8d0e5eaf4b5e9365a8e0e15905c6","1cbad95a982840cb878d7e7204d82806","ec103a5eca4548ffad332189d7ab5fc2","cdd02276486b4a5f858bba37c4de7213","59bdf3aa0e134c8191c7f7d4ed097157","53b18fe8a99b472992fd5089585316d8","d77fce16792c49a580fa9635736ff942","97d92c690a544a059f8fdbeb1adc2710","582bbe4c98f24b7d8d007346e149841c","9a96dccc35284538ae134cc5314928e9","43aa40d3a5864c25b591580da8c0fdc1","fba9795de42b40e98c221d79b438c64c","da1b1d5643b24a8ba7a418ee563b43d2","8028c50f50b84daebd15c5dac81a479d","f37ba9f8ba1f4ad3852d5c913edd1101","e17ec59b95d64db098933b6de26abbe0","60b2ab982221479c98dd4a53033bb6e8","a8113d34a718499eb8c1828f9f3ca0a5","7f365cea68334055a5f25ac5ab5b0df6","28b00d22135c46c7b4b2c32928779c9b","43af9fd51857436ab1dc7be3237b36db","5ac95c6a274940acb04cb71a02d1c707","889c7432add7495db485621a500953de","418b2bfda88c49f6b6fd6caa9980b32d","b2cc69bbeb404a59b840c986867fce31","7528fdb3f8ec45599be51d10fc9e7c5b","c3bb55efa63b467d9a89f80f38110376","a4014f4a1b594e3abff3ae8d03864249","263c4c29d9714bc0a92887dd0b652ea7","802844ed57d44138b9a8e41478081232","c224b8ae693a4667a48cf7ec56edb8b4","bfed22d07488418188350743ebcd51f9","d06e268485a741bf96b035dc2cd4dff0","ba1e29668b7b4bc290ef5927a3bb8ed8","54c59b5bf24440529d0014d7e99e51ec","13fb524ad203471e904a0ac4e2fc230b","06d5df021da3473fb1c511a1b71d60d6","73bbe05e4cf4451fb20fcab8a0f1acf6","610ddc0c14a94278a08bc850e06b823a","cde4496e6a4448eaa45259d3fb24cdcf","ecfe0dfc6d8343ff8eec4b4c143fabec","8374d198c9764e78889754038b103eb3","c79598acf56f462aa23e38ad43525e3d","7101d492feb9497d848c3c8bef5efa22","0af9d6c67c36498e952203ad117a4e5e","50529e3a043a49d58506f8f5e5105777","ec52f38da90b43aa8dfc498d7c03ca50","7ba2893dc919416b95dbf876a9c0310d"],"height":0},"executionInfo":{"status":"ok","timestamp":1714621729036,"user_tz":-420,"elapsed":11523,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"f9cad0be-d5e1-4206-80d6-fe5ac53a79dd"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for taln-ls2n/semeval-2010-pre contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/taln-ls2n/semeval-2010-pre\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/7.53k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89474a9b91404f0c908284d7e37d0fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/5.93k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd76f982477477aaa13d4f5760075ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/11.2M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59bdf3aa0e134c8191c7f7d4ed097157"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/16.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17ec59b95d64db098933b6de26abbe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3bb55efa63b467d9a89f80f38110376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73bbe05e4cf4451fb20fcab8a0f1acf6"}},"metadata":{}}]},{"cell_type":"code","source":["print(se2010_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq0LC4d5uJv8","executionInfo":{"status":"ok","timestamp":1714621729037,"user_tz":-420,"elapsed":15,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"7bcb85c5-07b2-4f63-ac6a-5c3259f916bc"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'title', 'abstract', 'keyphrases', 'prmu', 'lvl-1', 'lvl-2', 'lvl-3', 'lvl-4'],\n","        num_rows: 144\n","    })\n","    test: Dataset({\n","        features: ['id', 'title', 'abstract', 'keyphrases', 'prmu', 'lvl-1', 'lvl-2', 'lvl-3', 'lvl-4'],\n","        num_rows: 100\n","    })\n","})\n"]}]},{"cell_type":"code","source":["train_se2010_df = pd.DataFrame(se2010_ds['train'])\n","test_se2010_df = pd.DataFrame(se2010_ds['test'])"],"metadata":{"id":"FsENbY8OuTIH","executionInfo":{"status":"ok","timestamp":1714621729037,"user_tz":-420,"elapsed":13,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["all_se2010_df = pd.concat([train_se2010_df, test_se2010_df], ignore_index=True)"],"metadata":{"id":"kacJRI8puiGS","executionInfo":{"status":"ok","timestamp":1714621729037,"user_tz":-420,"elapsed":12,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["all_se2010_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"C4Bo8hYLusTh","executionInfo":{"status":"ok","timestamp":1714621730279,"user_tz":-420,"elapsed":1253,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"f8792d6e-fa40-44f1-a0cb-5517f9edd971"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     id                                              title  \\\n","0  J-39  The Sequential Auction Problem on eBay: An Emp...   \n","1  I-54     Approximate and Online Multi-Issue Negotiation   \n","2  I-68  On Opportunistic Techniques for Solving Decent...   \n","3  I-55  Searching for Joint Gains in Automated Negotia...   \n","4  J-38                  Multi-Attribute Coalitional Games   \n","\n","                                            abstract  \\\n","0  Bidders on eBay have no dominant bidding strat...   \n","1  This paper analyzes bilateral multi-issue nego...   \n","2  Decentralized Markov Decision Processes (DEC-M...   \n","3  It is well established by conflict theorists a...   \n","4  We study coalitional games where the value of ...   \n","\n","                                          keyphrases  \\\n","0  [sequenti auction problem, empir analysi, bid ...   \n","1  [approxim, negoti, time constraint, equilibriu...   \n","2  [decentr markov decis process, decentr markov ...   \n","3  [autom negoti, negoti, creat valu, claim valu,...   \n","4  [multi-attribut coalit game, coalit game, coop...   \n","\n","                                                prmu  \\\n","0  [P, P, P, P, P, P, P, P, U, M, M, U, U, M, U, ...   \n","1         [P, P, P, P, P, P, M, U, M, U, U, R, U, R]   \n","2   [P, P, P, P, P, P, M, M, U, U, U, M, U, M, U, M]   \n","3         [P, P, P, P, P, U, U, U, U, U, M, U, M, M]   \n","4  [P, P, P, P, P, P, P, M, U, U, U, M, U, U, M, ...   \n","\n","                                               lvl-1  \\\n","0  The Sequential Auction Problem on eBay: An Emp...   \n","1  Approximate and Online Multi-Issue Negotiation...   \n","2  On Opportunistic Techniques for Solving Decent...   \n","3  Searching for Joint Gains in Automated Negotia...   \n","4  Multi-Attribute Coalitional Games∗ Samuel Ieon...   \n","\n","                                               lvl-2  \\\n","0  The Sequential Auction Problem on eBay: An Emp...   \n","1  Approximate and Online Multi-Issue Negotiation...   \n","2  On Opportunistic Techniques for Solving Decent...   \n","3  Searching for Joint Gains in Automated Negotia...   \n","4  Multi-Attribute Coalitional Games * t\\nABSTRAC...   \n","\n","                                               lvl-3  \\\n","0  The Sequential Auction Problem on eBay: An Emp...   \n","1  Approximate and Online Multi-Issue Negotiation...   \n","2  On Opportunistic Techniques for Solving Decent...   \n","3  Searching for Joint Gains in Automated Negotia...   \n","4  Multi-Attribute Coalitional Games * t\\nABSTRAC...   \n","\n","                                               lvl-4  \n","0  The Sequential Auction Problem on eBay: An Emp...  \n","1  Approximate and Online Multi-Issue Negotiation...  \n","2  On Opportunistic Techniques for Solving Decent...  \n","3  Searching for Joint Gains in Automated Negotia...  \n","4  Multi-Attribute Coalitional Games * t\\nABSTRAC...  "],"text/html":["\n","  <div id=\"df-4b21b83d-c330-453b-8ea6-53a464d6ca1e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>keyphrases</th>\n","      <th>prmu</th>\n","      <th>lvl-1</th>\n","      <th>lvl-2</th>\n","      <th>lvl-3</th>\n","      <th>lvl-4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>J-39</td>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","      <td>Bidders on eBay have no dominant bidding strat...</td>\n","      <td>[sequenti auction problem, empir analysi, bid ...</td>\n","      <td>[P, P, P, P, P, P, P, P, U, M, M, U, U, M, U, ...</td>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I-54</td>\n","      <td>Approximate and Online Multi-Issue Negotiation</td>\n","      <td>This paper analyzes bilateral multi-issue nego...</td>\n","      <td>[approxim, negoti, time constraint, equilibriu...</td>\n","      <td>[P, P, P, P, P, P, M, U, M, U, U, R, U, R]</td>\n","      <td>Approximate and Online Multi-Issue Negotiation...</td>\n","      <td>Approximate and Online Multi-Issue Negotiation...</td>\n","      <td>Approximate and Online Multi-Issue Negotiation...</td>\n","      <td>Approximate and Online Multi-Issue Negotiation...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I-68</td>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","      <td>Decentralized Markov Decision Processes (DEC-M...</td>\n","      <td>[decentr markov decis process, decentr markov ...</td>\n","      <td>[P, P, P, P, P, P, M, M, U, U, U, M, U, M, U, M]</td>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I-55</td>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","      <td>It is well established by conflict theorists a...</td>\n","      <td>[autom negoti, negoti, creat valu, claim valu,...</td>\n","      <td>[P, P, P, P, P, U, U, U, U, U, M, U, M, M]</td>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>J-38</td>\n","      <td>Multi-Attribute Coalitional Games</td>\n","      <td>We study coalitional games where the value of ...</td>\n","      <td>[multi-attribut coalit game, coalit game, coop...</td>\n","      <td>[P, P, P, P, P, P, P, M, U, U, U, M, U, U, M, ...</td>\n","      <td>Multi-Attribute Coalitional Games∗ Samuel Ieon...</td>\n","      <td>Multi-Attribute Coalitional Games * t\\nABSTRAC...</td>\n","      <td>Multi-Attribute Coalitional Games * t\\nABSTRAC...</td>\n","      <td>Multi-Attribute Coalitional Games * t\\nABSTRAC...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b21b83d-c330-453b-8ea6-53a464d6ca1e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4b21b83d-c330-453b-8ea6-53a464d6ca1e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4b21b83d-c330-453b-8ea6-53a464d6ca1e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-eea7352a-0304-4b3f-8e81-1f51175e951b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eea7352a-0304-4b3f-8e81-1f51175e951b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-eea7352a-0304-4b3f-8e81-1f51175e951b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"all_se2010_df","summary":"{\n  \"name\": \"all_se2010_df\",\n  \"rows\": 244,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"C-81\",\n          \"I-43\",\n          \"J-13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Harvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks. In this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment. The algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energy-neutrality constraint, and (c) adapting to the dynamics of the energy source at run-time. We present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data. We also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source. Our methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\",\n          \"In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\",\n          \"The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NP-hard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs). Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyphrases\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prmu\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lvl-1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems Jason Hsu, Sadaf Zahedi, Aman Kansal, Mani Srivastava Electrical Engineering Department University of California Los Angeles {jasonh,kansal,szahedi,mbs} @ ee.ucla.edu Vijay Raghunathan NEC Labs America Princeton, NJ vijay@nec-labs.com ABSTRACT Harvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks.\\nIn this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment.\\nThe algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energyneutrality constraint, and (c) adapting to the dynamics of the energy source at run-time.\\nWe present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data.\\nWe also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source.\\nOur methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\\nCategories and Subject Descriptors C.2.4 [Computer Systems Organization]: Computer Communication Networks-Distributed Systems General Terms Algorithms, Design 1.\\nINTRODUCTION Energy supply has always been a crucial issue in designing battery-powered wireless sensor networks because the lifetime and utility of the systems are limited by how long the batteries are able to sustain the operation.\\nThe fidelity of the data produced by a sensor network begins to degrade once sensor nodes start to run out of battery power.\\nTherefore, harvesting energy from the environment has been proposed to supplement or completely replace battery supplies to enhance system lifetime and reduce the maintenance cost of replacing batteries periodically.\\nHowever, metrics for evaluating energy harvesting systems are different from those used for battery powered systems.\\nEnvironmental energy is distinct from battery energy in two ways.\\nFirst it is an inexhaustible supply which, if appropriately used, can allow the system to last forever, unlike the battery which is a limited resource.\\nSecond, there is an uncertainty associated with its availability and measurement, compared to the energy stored in the battery which can be known deterministically.\\nThus, power management methods based on battery status are not always applicable to energy harvesting systems.\\nIn addition, most power management schemes designed for battery-powered systems only account for the dynamics of the energy consumers (e.g., CPU, radio) but not the dynamics of the energy supply.\\nConsequently, battery powered systems usually operate at the lowest performance level that meets the minimum data fidelity requirement in order to maximize the system life.\\nEnergy harvesting systems, on the other hand, can provide enhanced performance depending on the available energy.\\nIn this paper, we will study how to adapt the performance of the available energy profile.\\nThere exist many techniques to accomplish performance scaling at the node level, such as radio transmit power adjustment [1], dynamic voltage scaling [2], and the use of low power modes [3].\\nHowever, these techniques require hardware support and may not always be available on resource constrained sensor nodes.\\nAlternatively, a common performance scaling technique is duty cycling.\\nLow power devices typically provide at least one low power mode in which the node is shut down and the power consumption is negligible.\\nIn addition, the rate of duty cycling is directly related to system performance metrics such as network latency and sampling frequency.\\nWe will use duty cycle adjustment as the primitive performance scaling technique in our algorithms.\\n2.\\nRELATED WORK Energy harvesting has been explored for several different types of systems, such as wearable computers [4], [5], [6], sensor networks [7], etc..\\nSeveral technologies to extract energy from the environment have been demonstrated including solar, motion-based, biochemical, vibration-based [8], [9], [10], [11], and others are being developed [12], [13].\\nWhile several energy harvesting sensor node platforms have been prototyped [14], [15], [16], there is a need for systematic power management techniques that provide performance guarantees during system operation.\\nThe first work to take environmental energy into account for data routing was [17], followed by [18].\\nWhile these works did demonstrate that environment aware decisions improve performance compared to battery aware decisions, their objective was not to achieve energy neutral operation.\\nOur proposed techniques attempt to maximize system performance while maintaining energy-neutral operation.\\n3.\\nSYSTEM MODEL The energy usage considerations in a harvesting system vary significantly from those in a battery powered system, as mentioned earlier.\\nWe propose the model shown in Figure 1 for designing energy management methods in a harvesting system.\\nThe functions of the various blocks shown in the figure are discussed below.\\nThe precise methods used in our system to achieve these functions will be discussed in subsequent sections.\\nHarvested Energy Tracking: This block represents the mechanisms used to measure the energy received from the harvesting device, such as the solar panel.\\nSuch information is useful for determining the energy availability profile and adapting system performance based on it.\\nCollecting this information requires that the node hardware be equipped with the facility to measure the power generated from the environment, and the Heliomote platform [14] we used for evaluating the algorithms has this capability.\\nEnergy Generation Model: For wireless sensor nodes with limited storage and processing capabilities to be able to use the harvested energy data, models that represent the essential components of this information without using extensive storage are required.\\nThe purpose of this block is to provide a model for the energy available to the system in a form that may be used for making power management decisions.\\nThe data measured by the energy tracking block is used here to predict future energy availability.\\nA good prediction model should have a low prediction error and provide predicted energy values for durations long enough to make meaningful performance scaling decisions.\\nFurther, for energy sources that exhibit both long-term and short-term patterns (e.g., diurnal and climate variations vs. weather patterns for solar energy), the model must be able to capture both characteristics.\\nSuch a model can also use information from external sources such as local weather forecast service to improve its accuracy.\\nEnergy Consumption Model: It is also important to have detailed information about the energy usage characteristics of the system, at various performance levels.\\nFor general applicability of our design, we will assume that only one sleep mode is available.\\nWe assume that the power consumption in the sleep and active modes is known.\\nIt may be noted that for low power systems with more advanced capabilities such as dynamic voltage scaling (DVS), multiple low power modes, and the capability to shut down system components selectively, the power consumption in each of the states and the resultant effect on application performance should be known to make power management decisions.\\nEnergy Storage Model: This block represents the model for the energy storage technology.\\nSince all the generated energy may not be used instantaneously, the harvesting system will usually have some energy storage technology.\\nStorage technologies (e.g., batteries and ultra-capacitors) are non-ideal, in that there is some energy loss while storing and retrieving energy from them.\\nThese characteristics must be known to efficiently manage energy usage and storage.\\nThis block also includes the system capability to measure the residual stored energy.\\nMost low power systems use batteries to store energy and provide residual battery status.\\nThis is commonly based on measuring the battery voltage which is then mapped to the residual battery energy using the known charge to voltage relationship for the battery technology in use.\\nMore sophisticated methods which track the flow of energy into and out of the battery are also available.\\nHarvesting-aware Power Management: The inputs provided by the previously mentioned blocks are used here to determine the suitable power management strategy for the system.\\nPower management could be carried to meet different objectives in different applications.\\nFor instance, in some systems, the harvested energy may marginally supplement the battery supply and the objective may be to maximize the system lifetime.\\nA more interesting case is when the harvested energy is used as the primary source of energy for the system with the objective of achieving indefinitely long system lifetime.\\nIn such cases, the power management objective is to achieve energy neutral operation.\\nIn other words, the system should only use as much energy as harvested from the environment and attempt to maximize performance within this available energy budget.\\n4.\\nTHEORETICALLY OPTIMAL POWER MANAGEMENT We develop the following theory to understand the energy neutral mode of operation.\\nLet us define Ps(t) as the energy harvested from the environment at time t, and the energy being consumed by the load at that time is Pc(t).\\nFurther, we model the non-ideal storage buffer by its round-trip efficiency \\u03b7 (strictly less than 1) and a constant leakage power Pleak.\\nUsing this notation, applying the rule of energy conservation leads to the following inequality: 0 0 00 [ ( ) ( )] [ ( ) ( )] 0 T T T s c c s leakP t P t dt P t P t dt P dtB \\u03b7 + + \\u2212 \\u2212 \\u2212 \\u2265+ \\u2212\\u222b \\u222b \\u222b (1) where B0 is the initial battery level and the function [X]+ = X if X > 0 and zero otherwise.\\nDEFINITION 1 (\\u03c1,\\u03c31,\\u03c32) function: A non-negative, continuous and bounded function P (t) is said to be a (\\u03c1,\\u03c31,\\u03c32) function if and only if for any value of finite real number T , the following are satisfied: 2 1( ) T T P t dt T\\u03c1 \\u03c3 \\u03c1 \\u03c3\\u2212 \\u2264 \\u2264 +\\u222b (2) This function can be used to model both energy sources and loads.\\nIf the harvested energy profile Ps(t) is a (\\u03c11,\\u03c31,\\u03c32) function, then the average rate of available energy over long durations becomes \\u03c11, and the burstiness is bounded by \\u03c31 and \\u03c32 .\\nSimilarly, Pc(t) can be modeled as a (\\u03c12,\\u03c33) function, when \\u03c12 and \\u03c33 are used to place an upper bound on power consumption (the inequality on the right side) while there are no minimum power consumption constraints.\\nThe condition for energy neutrality, equation (1), leads to the following theorem, based on the energy production, consumption, and energy buffer models discussed above.\\nTHEOREM 1 (ENERGY NEUTRAL OPERATION): Consider a harvesting system in which the energy production profile is characterized by a (\\u03c11, \\u03c31, \\u03c32) function, the load is characterized by a (\\u03c12, \\u03c33) function and the energy buffer is characterized by parameters \\u03b7 for storage efficiency, and Pleak for leakage power.\\nThe following conditions are sufficient for the system to achieve energy neutrality: \\u03c12 \\u2264 \\u03b7\\u03c11 \\u2212 Pleak (3) B0 \\u2265 \\u03b7\\u03c32 + \\u03c33 (4) B \\u2265 B0 (5) where B0 is the initial energy stored in the buffer and provides a lower bound on the capacity of the energy buffer B.\\nThe proof is presented in our prior work [19].\\nTo adjust the duty cycle D using our performance scaling algorithm, we assume the following relation between duty cycle and the perceived utility of the system to the user: Suppose the utility of the application to the user is represented by U(D) when the system operates at a duty cycle D. Then, min 1 min max 2 max ( ) 0, ( ) , ( ) , U D if D D U D k D if D D D U D k if D D \\u03b2 = < = + \\u2264 \\u2264 = > This is a fairly general and simple model and the specific values of Dmin and Dmax may be determined as per application requirements.\\nAs an example, consider a sensor node designed to detect intrusion across a periphery.\\nIn this case, a linear increase in duty cycle translates into a linear increase in the detection probability.\\nThe fastest and the slowest speeds of the intruders may be known, leading to a minimum and Harvested Energy Tracking Energy Consumption Model Energy Storage Model Harvestingaware Power Mangement Energy Generation Model LOAD Figure 1.\\nSystem model for an energy harvesting system.\\n181 maximum sensing delay tolerable, which results in the relevant Dmax and Dmin for the sensor node.\\nWhile there may be cases where the relationship between utility and duty cycle may be non-linear, in this paper, we restrict our focus on applications that follow this linear model.\\nIn view of the above models for the system components and the required performance, the objective of our power management strategy is adjust the duty cycle D(i) dynamically so as to maximize the total utility U(D) over a period of time, while ensuring energy neutral operation for the sensor node.\\nBefore discussing the performance scaling methods for harvesting aware duty cycle adaptation, let us first consider the optimal power management strategy that is possible for a given energy generation profile.\\nFor the calculation of the optimal strategy, we assume complete knowledge of the energy availability profile at the node, including the availability in the future.\\nThe calculation of the optimal is a useful tool for evaluating the performance of our proposed algorithm.\\nThis is particularly useful for our algorithm since no prior algorithms are available to serve as a baseline for comparison.\\nSuppose the time axis is partitioned into discrete slots of duration \\u0394T, and the duty cycle adaptation calculation is carried out over a window of Nw such time slots.\\nWe define the following energy profile variables, with the index i ranging over {1,..., Nw}: Ps(i) is the power output from the harvested source in time slot i, averaged over the slot duration, Pc is the power consumption of the load in active mode, and D(i) is the duty cycle used in slot i, whose value is to be determined.\\nB(i) is the residual battery energy at the beginning of slot i. Following this convention, the battery energy left after the last slot in the window is represented by B(Nw+1).\\nThe values of these variables will depend on the choice of D(i).\\nThe energy used directly from the harvested source and the energy stored and used from the battery must be accounted for differently.\\nFigure 2 shows two possible cases for Ps(i) in a time slot.\\nPs(i) may either be less than or higher than Pc , as shown on the left and right respectively.\\nWhen Ps(i) is lower than Pc, some of the energy used by the load comes from the battery, while when Ps(i) is higher than Pc, all the energy used is supplied directly from the harvested source.\\nThe crosshatched area shows the energy that is available for storage into the battery while the hashed area shows the energy drawn from the battery.\\nWe can write the energy used from the battery in any slot i as: ( ) ( ) ( ) ( )[ ] ( ) ( ){ } ( ) ( )[ ]1 1c cs s sB i B i TD i P P i TP i D i TD i P i P\\u03b7 \\u03b7 + + \\u2212 + = \\u0394 \\u2212 \\u2212 \\u0394 \\u2212 \\u2212 \\u2212 (6) In equation (6), the first term on the right hand side measures the energy drawn from the battery when Ps(i) < Pc, the next term measures the energy stored into the battery when the node is in sleep mode, and the last term measures the energy stored into the battery in active mode if Ps(i) > Pc.\\nFor energy neutral operation, we require the battery at the end of the window of Nw slots to be greater than or equal to the starting battery.\\nClearly, battery level will go down when the harvested energy is not available and the system is operated from stored energy.\\nHowever, the window Nw is judiciously chosen such that over that duration, we expect the environmental energy availability to complete a periodic cycle.\\nFor instance, in the case of solar energy harvesting, Nw could be chosen to be a twenty-four hour duration, corresponding to the diurnal cycle in the harvested energy.\\nThis is an approximation since an ideal choice of the window size would be infinite, but a finite size must be used for analytical tractability.\\nFurther, the battery level cannot be negative at any time, and this is ensured by having a large enough initial battery level B0 such that node operation is sustained even in the case of total blackout during a window period.\\nStating the above constraints quantitatively, we can express the calculation of the optimal duty cycles as an optimization problem below: 1 max ( ) wN i D i = \\u2211 (7) ( ) ( ) ( ) ( ) ( ) ( ){ } ( ) ( )1 1c s s s cB i B i TD i P P i TP i D i TD i P i P\\u03b7 \\u03b7 + + \\u23a1 \\u23a4 \\u23a1 \\u23a4\\u2212 + = \\u0394 \\u2212 \\u2212 \\u0394 \\u2212 \\u2212 \\u2212\\u23a3 \\u23a6 \\u23a3 \\u23a6 (8) 0(1)B B= (9) 0( 1)wB N B+ \\u2265 (10) min w( ) i {1,...,N }D i D\\u2265 \\u2200 \\u2208 (11) max w( ) i {1,...,N }D i D\\u2264 \\u2200 \\u2208 (12) The solution to the optimization problem yields the duty cycles that must be used in every slot and the evolution of residual battery over the course of Nw slots.\\nNote that while the constraints above contain the non-linear function [x]+ , the quantities occurring within that function are all known constants.\\nThe variable quantities occur only in linear terms and hence the above optimization problem can be solved using standard linear programming techniques, available in popular optimization toolboxes.\\n5.\\nHARVESTING-AWARE POWER MANAGEMENT We now present a practical algorithm for power management that may be used for adapting the performance based on harvested energy information.\\nThis algorithm attempts to achieve energy neutral operation without using knowledge of the future energy availability and maximizes the achievable performance within that constraint.\\nThe harvesting-aware power management strategy consists of three parts.\\nThe first part is an instantiation of the energy generation model which tracks past energy input profiles and uses them to predict future energy availability.\\nThe second part computes the optimal duty cycles based on the predicted energy, and this step uses our computationally tractable method to solve the optimization problem.\\nThe third part consists of a method to dynamically adapt the duty cycle in response to the observed energy generation profile in real time.\\nThis step is required since the observed energy generation may deviate significantly from the predicted energy availability and energy neutral operation must be ensured with the actual energy received rather than the predicted values.\\n5.1.\\nEnergy Prediction Model We use a prediction model based on Exponentially Weighted Moving-Average (EWMA).\\nThe method is designed to exploit the diurnal cycle in solar energy but at the same time adapt to the seasonal variations.\\nA historical summary of the energy generation profile is maintained for this purpose.\\nWhile the storage data size is limited to a vector length of Nw values in order to minimize the memory overheads of the power management algorithm, the window size is effectively infinite as each value in the history window depends on all the observed data up to that instant.\\nThe window size is chosen to be 24 hours and each time slot is taken to be 30 minutes as the variation in generated power by the solar panel using this setting is less than 10% between each adjacent slots.\\nThis yields Nw = 48.\\nSmaller slot durations may be used at the expense of a higher Nw.\\nThe historical summary maintained is derived as follows.\\nOn a typical day, we expect the energy generation to be similar to the energy generation at the same time on the previous days.\\nThe value of energy generated in a particular slot is maintained as a weighted average of the energy received in the same time-slot during all observed days.\\nThe weights are exponential, resulting in decaying contribution from older Figure 2.\\nTwo possible cases for energy calculations Slot i Slot k Pc Pc P(i) P(i) Active Sleep 182 data.\\nMore specifically, the historical average maintained for each slot is given by: 1 (1 )k k kx x x\\u03b1 \\u03b1\\u2212= + \\u2212 where \\u03b1 is the value of the weighting factor, kx is the observed value of energy generated in the slot, and 1kx \\u2212 is the previously stored historical average.\\nIn this model, the importance of each day relative to the previous one remains constant because the same weighting factor was used for all days.\\nThe average value derived for a slot is treated as an estimate of predicted energy value for the slot corresponding to the subsequent day.\\nThis method helps the historical average values adapt to the seasonal variations in energy received on different days.\\nOne of the parameters to be chosen in the above prediction method is the parameter \\u03b1, which is a measure of rate of shift in energy pattern over time.\\nSince this parameter is affected by the characteristics of the energy and sensor node location, the system should have a training period during which this parameter will be determined.\\nTo determine a good value of \\u03b1, we collected energy data over 72 days and compared the average error of the prediction method for various values of \\u03b1.\\nThe error based on the different values of \\u03b1 is shown in Figure 3.\\nThis curve suggests an optimum value of \\u03b1 = 0.15 for minimum prediction error and this value will be used in the remainder of this paper.\\n5.2.\\nLow-complexity Solution The energy values predicted for the next window of Nw slots are used to calculated the desired duty cycles for the next window, assuming the predicted values match the observed values in the future.\\nSince our objective is to develop a practical algorithm for embedded computing systems, we present a simplified method to solve the linear programming problem presented in Section 4.\\nTo this end, we define the sets S and D as follows: { } { } | ( ) 0 | ( ) 0 s c c s S i P i P D i P P i = \\u2212 \\u2265 = \\u2212 > The two sets differ by the condition that whether the node operation can be sustained entirely from environmental energy.\\nIn the case that energy produced from the environment is not sufficient, battery will be discharged to supplement the remaining energy.\\nNext we sum up both sides of (6) over the entire Nw window and rewrite it with the new notation.\\n1 1 1 1 ( )[ ( )] ( ) ( ) ( ) ( )[ ( ) ] Nw Nw Nw i i c s s s s c i i D i i i S B B TD i P P i TP i TP i D i TD i P i P\\u03b7 \\u03b7 \\u03b7+ = \\u2208 = = \\u2208 \\u2212 = \\u0394 \\u2212 \\u2212 \\u0394 + \\u0394 \\u2212 \\u0394 \\u2212\\u2211 \\u2211 \\u2211 \\u2211 \\u2211 The term on the left hand side is actually the battery energy used over the entire window of Nw slots, which can be set to 0 for energy neutral operation.\\nAfter some algebraic manipulation, this yields: 1 1 ( ) ( ) ( ) 1 ( ) Nw c s s c i i D i S P P i D i P i P D i \\u03b7 \\u03b7= \\u2208 \\u2208 \\u239b \\u239e\\u239b \\u239e = + \\u2212 +\\u239c \\u239f\\u239c \\u239f \\u239d \\u23a0\\u239d \\u23a0 \\u2211 \\u2211 \\u2211 (13) The term on the left hand side is the total energy received in Nw slots.\\nThe first term on the right hand side can be interpreted as the total energy consumed during the D slots and the second term is the total energy consumed during the S slots.\\nWe can now replace three constraints (8), (9), and (10) in the original problem with (13), restating the optimization problem as follows: 1 max ( ) wN i D i = \\u2211 1 1 ( ) ( ) ( ) 1 ( ) Nw c s s c i i D i S P P i D i P i P D i \\u03b7 \\u03b7= \\u2208 \\u2208 \\u239b \\u239e\\u239b \\u239e = + \\u2212 +\\u239c \\u239f\\u239c \\u239f \\u239d \\u23a0\\u239d \\u23a0 \\u2211 \\u2211 \\u2211 min max D(i) D {1,...,Nw) D(i) D {1,...,Nw) i i \\u2265 \\u2200 \\u2208 \\u2264 \\u2200 \\u2208 This form facilitates a low complexity solution that doesn``t require a general linear programming solver.\\nSince our objective is to maximize the total system utility, it is preferable to set the duty cycle to Dmin for time slots where the utility per unit energy is the least.\\nOn the other hand, we would also like the time slots with the highest Ps to operate at Dmax because of better efficiency of using energy directly from the energy source.\\nCombining these two characteristics, we define the utility co-efficient for each slot i as follows: 1 ( ) 1 1 ( ) 1 c c s P for i S W i P P i for i D \\u03b7 \\u03b7 \\u2208\\u23a7 \\u23aa = \\u239b \\u239e\\u239b \\u239e\\u23a8 + \\u2212 \\u2208\\u239c \\u239f\\u239c \\u239f\\u23aa \\u239d \\u23a0\\u239d \\u23a0\\u23a9 where W(i) is a representation of how efficient the energy usage in a particular time slot i is.\\nA larger W(i) indicates more system utility per unit energy in slot i and vice versa.\\nThe algorithm starts by assuming D(i) =Dmin for i = {1...NW} because of the minimum duty cycle requirement, and computes the remaining system energy R by: 1 1 ( ) ( ) ( ) 1 ( ) (14) Nw c s s c i i D i S P R P i D i P i P D i \\u03b7 \\u03b7= \\u2208 \\u2208 \\u239b \\u239e\\u239b \\u239e = \\u2212 + \\u2212 \\u2212\\u239c \\u239f\\u239c \\u239f \\u239d \\u23a0\\u239d \\u23a0 \\u2211 \\u2211 \\u2211 A negative R concludes that the optimization problem is infeasible, meaning the system cannot achieve energy neutrality even at the minimum duty cycle.\\nIn this case, the system designer is responsible for increasing the environment energy availability (e.g., by using larger solar panels).\\nIf R is positive, it means the system has excess energy that is not being used, and this may be allocated to increase the duty cycle beyond Dmin for some slots.\\nSince our objective is to maximize the total system utility, the most efficient way to allocate the excess energy is to assign duty cycle Dmax to the slots with the highest W(i).\\nSo, the coefficients W(i) are arranged in decreasing order and duty cycle Dmax is assigned to the slots beginning with the largest coefficients until the excess energy available, R (computed by (14) in every iteration), is insufficient to assign Dmax to another slot.\\nThe remaining energy, RLast, is used to increase the duty cycle to some value between Dmin and Dmax in the slot with the next lower coefficient.\\nDenoting this slot with index j, the duty cycle is given by: D(j)= min / ( ( ) ) / ( ) Last c Last s c s R P if j D DR if j S P j P P j\\u03b7 \\u2208\\u23a7 \\u23ab \\u23aa \\u23aa +\\u23a8 \\u23ac \\u2208\\u23aa \\u23aa\\u2212 \\u2212\\u23a9 \\u23ad The above solution to the optimization problem requires only simple arithmetic calculations and one sorting step which can be easily implemented on an embedded platform, as opposed to implementing a general linear program solver.\\n5.3.\\nSlot-by-slot continual duty cycle adaptiation.\\nThe observed energy values may vary greatly from the predicted ones, such as due to the effect of clouds or other sudden changes.\\nIt is thus important to adapt the duty cycles calculated using the predicted values, to the actual energy measurements in real time to ensure energy neutrality.\\nDenote the initial duty cycle assignments for each time slot i computed using the predicted energy values as D(i) = {1, ...,Nw}.\\nFirst we compute the difference between predicted power level Ps(i) and actual power level observed, Ps''(i) in every slot i. Then, the excess energy in slot i, denoted by X, can be obtained as follows: ( ) '( ) '( ) 1 ( ) '( ) ( )[ ( ) '( )](1 ) '( ) s s s c s s s s s c P i P i if P i P X P i P i D i P i P i if P i P \\u03b7 \\u2212 >\\u23a7 \\u23aa = \\u23a8 \\u2212 \\u2212 \\u2212 \\u2212 \\u2264\\u23aa \\u23a9 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 2.6 2.7 2.8 2.9 3 alpha AvgError(mA) Figure 3.\\nChoice of prediction parameter.\\n183 The upper term accounts for the energy difference when actual received energy is more than the power drawn by the load.\\nOn the other hand, if the energy received is less than Pc, we will need to account for the extra energy used from the battery by the load, which is a function of duty cycle used in time slot i and battery efficiency factor \\u03b7.\\nWhen more energy is received than predicted, X is positive and that excess energy is available for use in the subsequent solutes, while if X is negative, that energy must be compensated from subsequent slots.\\nCASE I: X<0.\\nIn this case, we want to reduce the duty cycles used in the future slots in order to make up for this shortfall of energy.\\nSince our objective function is to maximize the total system utility, we have to reduce the duty cycles for time slots with the smallest normalized utility coefficient, W(i).\\nThis is accomplished by first sorting the coefficient W(j) ,where j>i. in decreasing order, and then iteratively reducing Dj to Dmin until the total reduction in energy consumption is the same as X. CASE II: X>0.\\nHere, we want to increase the duty cycles used in the future to utilize this excess energy we received in recent time slot.\\nIn contrast to Case I, the duty cycles of future time slots with highest utility coefficient W(i) should be increased first in order to maximize the total system utility.\\nSuppose the duty cycle is changed by d in slot j. Define a quantity R(j,d) as follows: \\u23aa \\u23a9 \\u23aa \\u23a8 \\u23a7 <=\\u239f \\u239f \\u23a0 \\u239e \\u239c \\u239c \\u239d \\u239b \\u239f\\u239f \\u23a0 \\u239e \\u239c\\u239c \\u239d \\u239b \\u2212+ >\\u22c5 = lji l ljl PPifP P d PPifP djR 1 1 d ),( \\u03b7\\u03b7 The precise procedure to adapt the duty cycle to account for the above factors is presented in Algorithm 1.\\nThis calculation is performed at the end of every slot to set the duty cycle for the next slot.\\nWe claim that our duty cycling algorithm is energy neutral because an surplus of energy at the previous time slot will always translate to additional energy opportunity for future time slots, and vice versa.\\nThe claim may be violated in cases of severe energy shortages especially towards the end of window.\\nFor example, a large deficit in energy supply can``t be restored if there is no future energy input until the end of the window.\\nIn such case, this offset will be carried over to the next window so that long term energy neutrality is still maintained.\\n6.\\nEVALUATION Our adaptive duty cycling algorithm was evaluated using an actual solar energy profile measured using a sensor node called Heliomote, capable of harvesting solar energy [14].\\nThis platform not only tracks the generated energy but also the energy flow into and out of the battery to provide an accurate estimate of the stored energy.\\nThe energy harvesting platform was deployed in a residential area in Los Angeles from the beginning of June through the middle of August for a total of 72 days.\\nThe sensor node used is a Mica2 mote running at a fixed 40% duty cycle with an initially full battery.\\nBattery voltage and net current from the solar panels are sampled at a period of 10 seconds.\\nThe energy generation profile for that duration, measured by tracking the output current from the solar cell is shown in Figure 4, both on continuous and diurnal scales.\\nWe can observe that although the energy profile varies from day to day, it still exhibits a general pattern over several days.\\n0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70 Day mA 0 5 10 15 20 0 10 20 30 40 50 60 70 Hour mA 6.1.\\nPrediction Model We first evaluate the performance of the prediction model, which is judged by the amount of absolute error it made between the predicted and actual energy profile.\\nFigure 5 shows the average error of each time slot in mA over the entire 72 days.\\nGenerally, the amount of error is larger during the day time because that``s when the factor of weather can cause deviations in received energy, while the prediction made for night time is mostly correct.\\n6.2.\\nAdaptive Duty cycling algorithm Prior methods to optimize performance while achieving energy neutral operation using harvested energy are scarce.\\nInstead, we compare the performance of our algorithm against two extremes: the theoretical optimal calculated assuming complete knowledge about future energy availability and a simple approach which attempts to achieve energy neutrality using a fixed duty cycle without accounting for battery inefficiency.\\nThe optimal duty cycles are calculated for each slot using the future knowledge of actual received energy for that slot.\\nFor the simple approach, the duty cycle is kept constant within each day and is Figure 4 Solar Energy Profile (Left: Continuous, Right: Diurnal) Input: D: Initial duty cycle, X: Excess energy due to error in the prediction, P: Predicted energy profile, i: index of current time slot Output: D: Updated duty cycles in one or more subsequent slots AdaptDutyCycle() Iteration: At each time slot do: if X > 0 Wsorted = W{1, ...,Nw} sorted in decending order.\\nQ := indices of Wsorted for k = 1 to |Q| if Q(k) \\u2264 i or D(Q(k)) \\u2265 Dmax //slot is already passed continue if R(Q(k), Dmax \\u2212 D(Q(k))) < X D(Q(k)) = Dmax X = X \\u2212 R(j, Dmax \\u2212 D(Q(k))) else //X insufficient to increase duty cycle to Dmax if P (Q(k)) > Pl D(Q(k)) = D(Q(k)) + X/Pl else D(Q(k)) = D(Q(k)) + ( ( ))(1 1 ))c s X P P Q k\\u03b7 \\u03b7+ \\u2212 if X < 0 Wsorted = W{1, ...,Nw} sorted in ascending order.\\nQ := indices of Wsorted for k = 1 to |Q| if Q(k) \\u2264 I or D(Q(k)) \\u2264 Dmin continue if R(Q(k), Dmax \\u2212 D(Q(k))) > X D(Q(k)) = Dmin X = X \\u2212 R(j, Dmin \\u2212 D(Q(k))) else if P (Q(k)) > Pc D(Q(k)) = D(Q(k)) + X/Pc else D(Q(k)) = D(Q(k)) + ( ( ))(1 1 ))c s X P P Q k\\u03b7 \\u03b7+ \\u2212 ALGORITHM 1 Pseudocode for the duty-cycle adaptation algorithm Figure 5.\\nAverage Predictor Error in mA 0 5 10 15 20 25 0 2 4 6 8 10 12 Time(H) abserror(mA) 184 computed by taking the ratio of the predicted energy availability and the maximum usage, and this guarantees that the senor node will never deplete its battery running at this duty cycle.\\n{1.\\n. )\\n( )s w c i Nw D P i N P\\u03b7 \\u2208 = \\u22c5 \\u22c5\\u2211 We then compare the performance of our algorithm to the two extremes with varying battery efficiency.\\nFigure 6 shows the results, using Dmax = 0.8 and Dmin = 0.3.\\nThe battery efficiency was varied from 0.5 to 1 on the x-axis and solar energy utilizations achieved by the three algorithms are shown on the y-axis.\\nIt shows the fraction of net received energy that is used to perform useful work rather than lost due to storage inefficiency.\\nAs can be seen from the figure, battery efficiency factor has great impact on the performance of the three different approaches.\\nThe three approaches all converges to 100% utilization if we have a perfect battery (\\u03b7=1), that is, energy is not lost by storing it into the batteries.\\nWhen battery inefficiency is taken into account, both the adaptive and optimal approach have much better solar energy utilization rate than the simple one.\\nAdditionally, the result also shows that our adaptive duty cycle algorithm performs extremely close to the optimal.\\n0.4 0.5 0.6 0.7 0.8 0.9 1 0.4 0.5 0.6 0.7 0.8 0.9 1 Eta-batery roundtrip efficiency SolarEnergyUtilization(%) Optimal Adaptive Simple We also compare the performance of our algorithm with different values of Dmin and Dmax for \\u03b7=0.7, which is typical of NiMH batteries.\\nThese results are shown in Table 1 as the percentage of energy saved by the optimal and adaptive approaches, and this is the energy which would normally be wasted in the simple approach.\\nThe figures and table indicate that our real time algorithm is able to achieve a performance very close to the optimal feasible.\\nIn addition, these results show that environmental energy harvesting with appropriate power management can achieve much better utilization of the environmental energy.\\nDmax Dmin 0.8 0.05 0.8 0.1 0.8 0.3 0.5 0.2 0.9 0.2 1.0 0.2 Adaptive 51.0% 48.2% 42.3% 29.4% 54.7% 58.7% Optimal 52.3% 49.6% 43.7% 36.7% 56.6% 60.8% 7.\\nCONCLUSIONS We discussed various issues in power management for systems powered using environmentally harvested energy.\\nSpecifically, we designed a method for optimizing performance subject to the constraint of energy neutral operation.\\nWe also derived a theoretically optimal bound on the performance and showed that our proposed algorithm operated very close to the optimal.\\nThe proposals were evaluated using real data collected using an energy harvesting sensor node deployed in an outdoor environment.\\nOur method has significant advantages over currently used methods which are based on a conservative estimate of duty cycle and can only provide sub-optimal performance.\\nHowever, this work is only the first step towards optimal solutions for energy neutral operation.\\nIt is designed for a specific power scaling method based on adapting the duty cycle.\\nSeveral other power scaling methods, such as DVS, submodule power switching and the use of multiple low power modes are also available.\\nIt is thus of interest to extend our methods to exploit these advanced capabilities.\\n8.\\nACKNOWLEDGEMENTS This research was funded in part through support provided by DARPA under the PAC/C program, the National Science Foundation (NSF) under award #0306408, and the UCLA Center for Embedded Networked Sensing (CENS).\\nAny opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA, NSF, or CENS.\\nREFERENCES [1] R Ramanathan, and R Hain, Toplogy Control of Multihop Wireless Networks Using Transmit Power Adjustment in Proc.\\nInfocom.\\nVol 2.\\n26-30 pp. 404-413.\\nMarch 2000 [2] T.A. Pering, T.D. Burd, and R. W. Brodersen, The simulation and evaluation of dynamic voltage scaling algorithms, in Proc.\\nACM ISLPED, pp. 76-81, 1998 [3] L. Benini and G. De Micheli, Dynamic Power Management: Design Techniques and CAD Tools.\\nKluwer Academic Publishers, Norwell, MA, 1997.\\n[4] John Kymisis, Clyde Kendall, Joseph Paradiso, and Neil Gershenfeld.\\nParasitic power harvesting in shoes.\\nIn ISWC, pages 132-139.\\nIEEE Computer Society press, October 1998.\\n[5] Nathan S. Shenck and Joseph A. Paradiso.\\nEnergy scavenging with shoemounted piezoelectrics.\\nIEEE Micro, 21(3):30\\u00f142, May-June 2001.\\n[6] T Starner.\\nHuman-powered wearable computing.\\nIBM Systems Journal, 35(3-4), 1996.\\n[7] Mohammed Rahimi, Hardik Shah, Gaurav S. Sukhatme, John Heidemann, and D. Estrin.\\nStudying the feasibility of energy harvesting in a mobile sensor network.\\nIn ICRA, 2003.\\n[8] ChrisMelhuish.\\nThe ecobot project.\\nwww.ias.uwe.ac.uk/energy autonomy/EcoBot web page.html.\\n[9] Jan M.Rabaey, M. Josie Ammer, Julio L. da Silva Jr., Danny Patel, and Shad Roundy.\\nPicoradio supports ad-hoc ultra-low power wireless networking.\\nIEEE Computer, pages 42-48, July 2000.\\n[10] Joseph A. Paradiso and Mark Feldmeier.\\nA compact, wireless, selfpowered pushbutton controller.\\nIn ACM Ubicomp, pages 299-304, Atlanta, GA, USA, September 2001.\\nSpringer-Verlag Berlin Heidelberg.\\n[11] SE Wright, DS Scott, JB Haddow, andMA Rosen.\\nThe upper limit to solar energy conversion.\\nvolume 1, pages 384 - 392, July 2000.\\n[12] Darpa energy harvesting projects.\\nhttp://www.darpa.mil/dso/trans/energy/projects.html.\\n[13] Werner Weber.\\nAmbient intelligence: industrial research on a visionary concept.\\nIn Proceedings of the 2003 international symposium on Low power electronics and design, pages 247-251.\\nACM Press, 2003.\\n[14] V Raghunathan, A Kansal, J Hsu, J Friedman, and MB Srivastava, ``Design Considerations for Solar Energy Harvesting Wireless Embedded Systems,'' (IPSN/SPOTS), April 2005.\\n[15] Xiaofan Jiang, Joseph Polastre, David Culler, Perpetual Environmentally Powered Sensor Networks, (IPSN/SPOTS), April 25-27, 2005.\\n[16] Chulsung Park, Pai H. Chou, and Masanobu Shinozuka, ``DuraNode: Wireless Networked Sensor for Structural Health Monitoring,'' to appear in Proceedings of the 4th IEEE International Conference on Sensors, Irvine, CA, Oct. 31 - Nov. 1, 2005.\\n[17] Aman Kansal and Mani B. Srivastava.\\nAn environmental energy harvesting framework for sensor networks.\\nIn International symposium on Low power electronicsand design, pages 481-486.\\nACM Press, 2003.\\n[18] Thiemo Voigt, Hartmut Ritter, and Jochen Schiller.\\nUtilizing solar power in wireless sensor networks.\\nIn LCN, 2003.\\n[19] A. Kansal, J. Hsu, S. Zahedi, and M. B. Srivastava.\\nPower management in energy harvesting sensor networks.\\nTechnical Report TR-UCLA-NESL200603-02, Networked and Embedded Systems Laboratory, UCLA, March 2006.\\nFigure 6.\\nDuty Cycles achieved with respect to \\u03b7 TABLE 1.\\nEnergy Saved by adaptive and optimal approach.\\n185\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems Zinovi Rabinovich Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel nomad@cs.huji.ac.il Jeffrey S. Rosenschein Engineering and Computer Science Hebrew University of Jerusalem Jerusalem, Israel jeff@cs.huji.ac.il Gal A. Kaminka The MAVERICK Group Department of Computer Science Bar Ilan University, Israel galk@cs.biu.ac.il ABSTRACT In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.\\nUnlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.\\nWe show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.\\nEMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.\\nWe exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).\\nWe show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\\nCategories and Subject Descriptors I.2.8 [Problem Solving, Control Methods, and Search]: Control Theory; I.2.9 [Robotics]; I.2.11 [Distributed Artificial Intelligence]: Intelligent Agents General Terms Algorithms, Theory 1.\\nINTRODUCTION Planning and control constitutes a central research area in multiagent systems and artificial intelligence.\\nIn recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.\\nIn this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.\\nWhile theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].\\nWe take an alternative view of planning in stochastic environments.\\nWe do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.\\nThe idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.\\nWe call this general planning framework Dynamics Based Control (DBC).\\nIn DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.\\nAs actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].\\nHere, optimality is measured in terms of probability of deviation magnitudes.\\nIn this paper, we present the structure of Dynamics Based Control.\\nWe show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.\\nEMT is an efficient instantiation of DBC.\\nTo evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.\\nExperimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent``s position).\\nThe paper is organized as follows.\\nIn Section 2 we motivate DBC using area-sweeping problems, and discuss related work.\\nSection 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.\\nThis is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.\\nThat section also discusses the limitations of EMT-based control relative to the general DBC framework.\\nExperimental settings and results are then presented in Section 5.\\nSection 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work.\\n790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2.\\nMOTIVATION AND RELATED WORK Many real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.\\nFor example, security guards perform persistent sweeps of an area to detect any sign of intrusion.\\nCunning thieves will attempt to track these sweeps, and time their operation to key points of the guards'' motion.\\nIt is thus advisable to make the guards'' motion dynamics appear irregular and random.\\nRecent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.\\nThe goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.\\nOur focus differs from this work in that DBC does not optimize expected rewards-indeed we do not consider rewards at all-but instead maintains desired dynamics (including, but not limited to, randomization).\\nThe Game of Tag is another example of the applicability of the approach.\\nIt was introduced in the work by Pineau et al. [11].\\nThere are two agents that can move about an area, which is divided into a grid.\\nThe grid may have blocked cells (holes) into which no agent can move.\\nOne agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a successful tag).\\nThe quarry seeks to avoid the hunter agent, and is always aware of the hunter``s position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.\\nThe hunter knows the quarry``s probabilistic law of motion, but does not know its current location.\\nTag is an instance of a family of area-sweeping (pursuit-evasion) problems.\\nIn [11], the hunter modeled the problem using a POMDP.\\nA reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.\\nDue to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.\\nIn this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.\\nIn fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.\\nDynamics Based Control provides a natural approach to solving these problems.\\n3.\\nDYNAMICS BASED CONTROL The specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level.\\n\\u2022 Environment Design Level is concerned with the formal specification and modeling of the environment.\\nFor example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant.\\n\\u2022 User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.\\nThe User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.\\nIn the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping.\\n\\u2022 Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.\\nAs we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.\\nTo this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.\\nSpecific action selection then depends on system formalization.\\nOne possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].\\nThe other alternative would be to rely on the estimation procedure provided by the User Level-to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.\\nNotice that this manipulation is not direct, but via the environment.\\nThus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).\\nDBC levels can also have a back-flow of information (see Figure 1).\\nFor instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.\\nData would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.\\nUserEnv.\\nDesign Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.\\nFor example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.\\nIn this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.\\nIn fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9].\\n3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.\\nNotice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): \\u2022 Environment Design level is to specify a tuple < S, A, T, O, \\u03a9, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the environment; - T is the environment``s probabilistic transition function: T : S \\u00d7A \\u2192 \\u03a0(S).\\nThat is, T(s |a, s) is the probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.\\nThis is what the sensor input would look like for an outside observer; - \\u03a9 is the observation probability function: \\u03a9 : S \\u00d7 A \\u00d7 S \\u2192 \\u03a0(O).\\nThat is, \\u03a9(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a. \\u2022 User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {\\u03c4 : S \\u00d7 A \\u2192 \\u03a0(S)}.\\nThus specification of target dynamics can be expressed by q \\u2208 F, and the learning or tracking algorithm can be represented as a function L : O\\u00d7(A\\u00d7O)\\u2217 \\u2192 F; that is, it maps sequences of observations and actions performed so far into an estimate \\u03c4 \\u2208 F of system dynamics.\\nThere are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two distributions p and q defined by D(p(\\u00b7), q(\\u00b7)) = 1 2 x |p(x) \\u2212 q(x)| - Fidelity measure of distance F(p(\\u00b7), q(\\u00b7)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(\\u00b7) q(\\u00b7)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.\\nFor instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p.\\nThe User Level also defines the threshold of dynamics deviation probability \\u03b8.\\n\\u2022 Agent Level is then faced with a problem of selecting a control signal function a\\u2217 to satisfy a minimization problem as follows: a\\u2217 = arg min a Pr(d(\\u03c4a, q) > \\u03b8) where d(\\u03c4a, q) is a random variable describing deviation of the dynamics estimate \\u03c4a, created by L under control signal a, from the ideal dynamics q. Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level.\\n3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.\\nPOMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.\\nThis can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.\\nDBC concentrates on the underlying principle of state sequencing, the system dynamics.\\nDBC``s target dynamics specification can use the environment``s state space as a means to describe, discern, and preserve changes that occur within the system.\\nAs a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.\\nFor example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.\\nPOMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goalaccumulating higher reward can be reached only by faster motion.\\nAlternatively, the state space could directly include the notion of speed.\\nFor POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.\\nNow, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.\\nOn the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.\\nIn this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm.\\n4.\\nEMT-BASED CONTROL AS A DBC Recently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.\\nAlthough it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].\\nEMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality.\\n\\u2022 User Level of EMT-based control defines a limited-case target system dynamics independent of action: qEMT : S \\u2192 \\u03a0(S).\\nIt then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator-the Extended Markov Tracking (EMT) algorithm.\\nThe algorithm keeps a system dynamics estimate \\u03c4t EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt\\u22121 to pt, and updates it conservatively using Kullback-Leibler divergence.\\nSince \\u03c4t EMT and pt\\u22121,t are respectively the conditional and marginal probabilities over the system``s state space, explanation simply means that pt(s ) = s \\u03c4t EMT (s |s)pt\\u22121(s), and the dynamics estimate update is performed by solving a 792 The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment Level Approach MDP Markovian DBC Environment < S, A, T, O, \\u03a9 >,where S - set of states A - set of actions Design T : S \\u00d7 A \\u2192 \\u03a0(S) - transition O - observation set \\u03a9 : S \\u00d7 A \\u00d7 S \\u2192 \\u03a0(O) User r : S \\u00d7 A \\u00d7 S \\u2192 R q : S \\u00d7 A \\u2192 \\u03a0(S) F(\\u03c0\\u2217 ) \\u2192 r L(o1, ..., ot) \\u2192 \\u03c4 r - reward function q - ideal dynamics F - reward remodeling L - dynamics estimator \\u03b8 - threshold Agent \\u03c0\\u2217 = arg max \\u03c0 E[ \\u03b3t rt] \\u03c0\\u2217 = arg min \\u03c0 Prob(d(\\u03c4 q) > \\u03b8) minimization problem: \\u03c4t EMT = H[pt, pt\\u22121, \\u03c4t\\u22121 EMT ] = arg min \\u03c4 DKL(\\u03c4 \\u00d7 pt\\u22121 \\u03c4t\\u22121 EMT \\u00d7 pt\\u22121) s.t. pt(s ) = s (\\u03c4 \\u00d7 pt\\u22121)(s , s) pt\\u22121(s) = s (\\u03c4 \\u00d7 pt\\u22121)(s , s) \\u2022 Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMT``s reaction.\\nThe prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environment``s transition function limited to action a, and pt\\u22121 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by a\\u2217 = arg min a\\u2208A DKL(H[Ta \\u00d7 pt, pt, \\u03c4t EMT ] qEMT \\u00d7 pt\\u22121) Note that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.\\nYet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2).\\n4.1 Multi-Target EMT At times, there may exist several behavioral preferences.\\nFor example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.\\nOn the other hand, no corner of the museum is to be left unchecked, which demands constant motion.\\nSuccessful museum security would demand that the guards adhere to, and balance, both of these behaviors.\\nFor EMT-based control, this would mean facing several tactical targets {qk}K k=1, and the question becomes how to merge and balance them.\\nA balancing mechanism can be applied to resolve this issue.\\nNote that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted performance with respect to a given target.\\nIf these preference vectors are normalized, they can be combined into a single unified preference.\\nThis requires replacement of standard EMT-based action selection by the algorithm below [15]: \\u2022 Given: - a set of target dynamics {qk}K k=1, - vector of weights w(k) \\u2022 Select action as follows - For each action a \\u2208 A predict the future state distribution \\u00afpa t+1 = Ta \\u2217 pt; - For each action, compute Da = H(\\u00afpa t+1, pt, PDt) - For each a \\u2208 A and qk tactical target, denote V (a, k) = DKL (Da qk) pt .\\nLet Vk(a) = 1 Zk V (a, k), where Zk = a\\u2208A V (a, k) is a normalization factor.\\n- Select a\\u2217 = arg min a k k=1 w(k)Vk(a) The weights vector w = (w1, ..., wK ) allows the additional tuning of importance among target dynamics without the need to redesign the targets themselves.\\nThis balancing method is also seamlessly integrated into the EMT-based control flow of operation.\\n4.2 EMT-based Control Limitations EMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.\\nIt limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.\\nThis kind of combination, however, is common for on-line algorithms.\\nAlthough further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.\\nThere are two further, EMT-specific, limitations to EMT-based control that are evident at this point.\\nBoth already have partial solutions and are subjects of ongoing research.\\nThe first limitation is the problem of negative preference.\\nIn the POMDP framework for example, this is captured simply, through The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793 the appearance of values with different signs within the reward structure.\\nFor EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.\\nAvoidance is thus unnatural in native EMT-based control.\\nThe second limitation comes from the fact that standard environment modeling can create pure sensory actions-actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.\\nSince the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.\\nNotice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference.\\n5.\\nEMT PLAYING TAG The Game of Tag was first introduced in [11].\\nIt is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.\\nAn example domain is shown in Figure 2.\\n0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q) The Game of Tag extremely limits the agent``s perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.\\nIn the classical version of the game, co-location leads to a special observation, and the `Tag'' action can be performed.\\nWe slightly modify this setting: the moment that both agents occupy the same cell, the game ends.\\nAs a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.\\nThese form a formal space of actions within a Markovian environment.\\nThe state space of the formal Markovian environment is described by the cross-product of the agent and quarry``s positions.\\nFor Figure 2, it would be S = {s0, ..., s23} \\u00d7 {s0, ..., s23}.\\nThe effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.\\nWith probability q0 1 it stays put, and with probability 1 \\u2212 q0 it moves to an adjacent cell further away from the 1 In our experiments this was taken to be q0 = 0.2.\\nagent.\\nSo for the instance shown in Figure 2 and q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Although the evasive behavior of the quarry is known to the agent, the quarry``s position is not.\\nThe only sensory information available to the agent is its own location.\\nWe use EMT and directly specify the target dynamics.\\nFor the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.\\nThis results in the following three target dynamics: Tcatch(At+1 = si|Qt = sj, At = sa) \\u221d 1 si = sj 0 otherwise Tmobile(At+1 = si|Qt = so, At = sj) \\u221d 0 si = sj 1 otherwise Tstalk(At+1 = si|Qt = so, At = sj) \\u221d 1 dist(si,so) Note that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At+1 = s9 as required by the Tcatch target dynamics.\\nWe ran several experiments to evaluate EMT performance in the Tag Game.\\nThree configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.\\nIn each setting, a set of 1000 runs was performed with a time limit of 100 steps.\\nIn every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarry``s initial position was uniformly distributed over the entire domain cell space.\\nWe also used two variations of the environment observability function.\\nIn the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.\\nIn the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunter``s location.\\nThe second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.\\nThe results of these experiments are shown in Table 2.\\nBalancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.\\nAlthough direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis-`a-vis the POMDP approach.\\nIn spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.\\nFor the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.\\nThat is, over 4 \\u00d7 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].\\nThe significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.\\nWe also tested the behavior cell frequency entropy, empirical measures from trial data.\\nAs Figure 4 and Figure 5 show, empir794 The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) A Q Q A 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 A Q Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor Table 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunter``s position Model Domain Capture% E(Steps) Time/Step I Dead-ends 100 14.8 72(mSec) Arena 80.2 42.4 500(mSec) Circle 91.4 34.6 187(mSec) II Dead-ends 100 13.2 91(mSec) Arena 96.8 28.67 396(mSec) Circle 94.4 31.63 204(mSec) ical entropy grows with the length of interaction.\\nFor runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.\\nAs the agent actively seeks the quarry, the entropy never reaches its maximum.\\nOne characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.\\nNear the maximum limit of trial length (100 steps), entropy suddenly dropped.\\nFurther analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.\\nSince the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.\\nIt is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.\\nThis behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them.\\n6.\\nDISCUSSION The design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.\\nPOMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.\\nEMT discards any reward scheme, and instead measures and influences system dynamics directly.\\n2 Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.\\nThus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agent``s behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.\\nExperimental data shows that these targets need not be directly achievable via the agent``s actions.\\nHowever, the ratio between EMT performance and achievability of target dynamics remains to be explored.\\nThe tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.\\nPOMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.\\nDBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.\\nThe importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.\\nThe structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.\\nFor instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.\\nThe complementary properties of POMDPs and EMT can be further exploited.\\nThere is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.\\nDBC can be an effective partner in such a hybrid solution.\\nFor instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation.\\n7.\\nCONCLUSIONS AND FUTURE WORK In this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.\\nDBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.\\nOptimality of DBC plans of action are measured The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead\\u2212ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 4: Observation Model I: Omniposition quarry.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\n0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Dead\\u2212ends 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Steps Entropy Circle Figure 5: Observation Model II: quarry not observed at hunter``s position.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nwith respect to the deviation of actual system dynamics from the target dynamics.\\nWe show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.\\nIn fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.\\nSince EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.\\nAs enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.\\nThis prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).\\nHowever, DBC in general has no such limitations, and readily enables the formulation of evasion games.\\nIn future work, we intend to proceed with the development of dynamics-based controllers for these problems.\\n8.\\nACKNOWLEDGMENT The work of the first two authors was partially supported by Israel Science Foundation grant #898/05, and the third author was partially supported by a grant from Israel``s Ministry of Science and Technology.\\n9.\\nREFERENCES [1] R. C. Arkin.\\nBehavior-Based Robotics.\\nMIT Press, 1998.\\n[2] J. A. Bilmes.\\nA gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models.\\nTechnical Report TR-97-021, Department of Electrical Engeineering and Computer Science, University of California at Berkeley, 1998.\\n[3] T. M. Cover and J. A. Thomas.\\nElements of information theory.\\nWiley, 1991.\\n[4] M. E. desJardins, E. H. Durfee, C. L. Ortiz, and M. J. Wolverton.\\nA survey of research in distributed, continual planning.\\nAI Magazine, 4:13-22, 1999.\\n[5] V. R. Konda and J. N. Tsitsiklis.\\nActor-Critic algorithms.\\nSIAM Journal on Control and Optimization, 42(4):1143-1166, 2003.\\n[6] W. S. Lim.\\nA rendezvous-evasion game on discrete locations with joint randomization.\\nAdvances in Applied Probability, 29(4):1004-1017, December 1997.\\n[7] M. L. Littman, T. L. Dean, and L. P. Kaelbling.\\nOn the complexity of solving Markov decision problems.\\nIn Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence (UAI-95), pages 394-402, 1995.\\n[8] O. Madani, S. Hanks, and A. Condon.\\nOn the undecidability of probabilistic planning and related stochastic optimization problems.\\nArtificial Intelligence Journal, 147(1-2):5-34, July 2003.\\n[9] R. M. Neal and G. E. Hinton.\\nA view of the EM algorithm 796 The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) that justifies incremental, sparse, and other variants.\\nIn M. I. Jordan, editor, Learning in Graphical Models, pages 355-368.\\nKluwer Academic Publishers, 1998.\\n[10] P. Paruchuri, M. Tambe, F. Ordonez, and S. Kraus.\\nSecurity in multiagent systems by policy randomization.\\nIn Proceeding of AAMAS 2006, 2006.\\n[11] J. Pineau, G. Gordon, and S. Thrun.\\nPoint-based value iteration: An anytime algorithm for pomdps.\\nIn International Joint Conference on Artificial Intelligence (IJCAI), pages 1025-1032, August 2003.\\n[12] M. L. Puterman.\\nMarkov Decision Processes.\\nWiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section.\\nWiley-Interscience Publication, New York, 1994.\\n[13] Z. Rabinovich and J. S. Rosenschein.\\nExtended Markov Tracking with an application to control.\\nIn The Workshop on Agent Tracking: Modeling Other Agents from Observations, at the Third International Joint Conference on Autonomous Agents and Multiagent Systems, pages 95-100, New-York, July 2004.\\n[14] Z. Rabinovich and J. S. Rosenschein.\\nMultiagent coordination by Extended Markov Tracking.\\nIn The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 431-438, Utrecht, The Netherlands, July 2005.\\n[15] Z. Rabinovich and J. S. Rosenschein.\\nOn the response of EMT-based control to interacting targets and models.\\nIn The Fifth International Joint Conference on Autonomous Agents and Multiagent Systems, pages 465-470, Hakodate, Japan, May 2006.\\n[16] R. F. Stengel.\\nOptimal Control and Estimation.\\nDover Publications, 1994.\\n[17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, and P. Varakantham.\\nConflicts in teamwork: Hybrids to the The Sixth Intl..\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 797\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions [Extended Abstract] Georg Gottlob Computing Laboratory Oxford University OX1 3QD Oxford, UK georg.gottlob@comlab.ox.ac.uk Gianluigi Greco Dipartimento di Matematica University of Calabria I-87030 Rende, Italy ggreco@mat.unical.it ABSTRACT The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices.\\nWhile this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs).\\nFormally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph.\\nNote that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness.\\nIn fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem.\\nIn this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3.\\nMotivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions.\\nWe show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here.\\nIndeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width.\\nEven more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\\nCategories and Subject Descriptors J.4 [Computer Applications]: Social and Behavioral Sciences-Economics; F.2 [Theory of Computation]: Analysis of Algorithms and Problem Complexity 1.\\nINTRODUCTION Combinatorial auctions.\\nCombinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items.\\nThis is desirable when a bidder``s valuation of a bundle of items is not equal to the sum of her valuations of the individual items.\\nThis framework is currently used to regulate agents'' interactions in several application domains (cf., e.g., [21]) such as, electricity markets [13], bandwidth auctions [14], and transportation exchanges [18].\\nFormally, a combinatorial auction is a pair I, B , where I = {I1, ..., Im} is the set of items the auctioneer has to sell, and B = {B1, ..., Bn} is the set of bids from the buyers interested in the items in I. Each bid Bi has the form item(Bi), pay(Bi) , where pay(Bi) is a rational number denoting the price a buyer offers for the items in item(Bi) \\u2286 I.\\nAn outcome for I, B is a subset b of B such that item(Bi)\\u2229item(Bj) = \\u2205, for each pair Bi and Bj of bids in b with i = j.\\nThe winner determination problem.\\nA crucial problem for combinatorial auctions is to determine the outcome b\\u2217 that maximizes the sum of the accepted bid prices (i.e., Bi\\u2208b\\u2217 pay(Bi)) over all the possible outcomes.\\nThis problem, called winner determination problem (e.g., [11]), is known to be intractable, actually NP-hard [17], and even not approximable in polynomial time unless NP = ZPP [19].\\nHence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions (e.g., [20, 5, 2, 8, 23]) and to identify classes of instances where solving the winner determination problem is feasible in polynomial time (e.g., [15, 22, 12, 21]).\\nIn fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions.\\nItem graphs.\\nCurrently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any 152 Figure 1: Example MaxWSP problem: (a) Hypergraph H I0,B0 , and a packing h for it; (b) Primal graph for H I0,B0 ; and, (c,d) Two item graphs for H I0,B0 .\\nbid, the items occurring in it induce a connected subgraph.\\nIndeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure [3]-formally bounded treewidth [16].\\nTo have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction I, B can be represented by means of a hypergraph H I,B such that its set of nodes N(H I,B ) coincides with set of items I, and where its edges E(H I,B ) are precisely the bids of the buyers {item(Bi) | Bi \\u2208 B}.\\nA special item graph for I, B is the primal graph of H I,B , denoted by G(H I,B ), which contains an edge between any pair of nodes in some hyperedge of H I,B .\\nThen, any item graph for H I,B can be viewed as a simplification of G(H I,B ) obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge.\\nExample 1.\\nThe hypergraph H I0,B0 reported in Figure 1.\\n(a) is an encoding for a combinatorial auction I0, B0 , where I0 = {I1, ..., I5}, and item(Bi) = hi, for each 1 \\u2264 i \\u2264 3.\\nThe primal graph for H I0,B0 is reported in Figure 1.\\n(b), while two example item graphs are reported in Figure 1.\\n(c) and (d), where edges required for maintaining the connectivity for h1 are depicted in bold.\\n\\u00a1 Open Problem: Computing structured item graphs efficiently.\\nThe above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined.\\nHowever, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain (constant) treewidth exists, and if so, how to compute such a structured item graph efficiently.\\nPolynomial time algorithms to find the best simplification of the primal graph were so far only known for the cases where the item graph to be constructed is a line [10], a cycle [4], or a tree [3], but it was an important open problem (cf. [3]) whether it is tractable to check if for a combinatorial auction, an item graph of treewidth bounded by a fixed natural number k exists and can be constructed in polynomial time, if so.\\nWeighted Set Packing.\\nLet us note that the hypergraph representation H I,B of a combinatorial auction I, B is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear (e.g., [17]).\\nFormally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h \\u2208 h with h = h , it holds that h \\u2229 h = \\u2205.\\nLetting w be a weighting function for H, i.e., a polynomially-time computable function from E(H) to rational numbers, the weight of a packing h is the rational number w(h) = h\\u2208h w(h), where w({}) = 0.\\nThen, the maximum-weighted set packing problem for H w.r.t. w, denoted by MaxWSP(H, w), is the problem of finding a packing for H having the maximum weight over all the packings for H. To see that MaxWSP is just a different formulation for the winner determination problem, given a combinatorial auction I, B , it is sufficient to define the weighting function w I,B (item(Bi)) = pay(Bi).\\nThen, the set of the solutions for the weighted set packing problem for H I,B w.r.t. w I,B coincides with the set of the solutions for the winner determination problem on I, B .\\nExample 2.\\nConsider again the hypergraph H I0,B0 reported in Figure 1.\\n(a).\\nAn example packing for H I0,B0 is h = {h1}, which intuitively corresponds to an outcome for I0, B0 , where the auctioneer accepted the bid B1.\\nBy assuming that bids B1, B2, and B3 are such that pay(B1) = pay(B2) = pay(B3), the packing h is not a solution for the problem MaxWSP(H I0,B0 , w I0,B0 ).\\nIndeed, the packing h\\u2217 = {h2, h3} is such that w I0,B0 (h\\u2217 ) > w I0,B0 (h).\\n\\u00a1 Contributions The primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable.\\nTowards this aim, we first study structured item graphs and solve the open problem in [3].\\nThe result is very bad news: It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3.\\nMore formally, letting C(ig, k) denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph (associated with a combinatorial auction problem) belongs to C(ig, 3) is NP-complete.\\nIn the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem.\\nOur investigations, this time, led to very good news which are summarized below: For a hypergraph H, its dual \\u00afH = (V, E) is such that nodes in V are in one-to-one correspondence with hyperedges in H, and for each node x \\u2208 N(H), {h | x \\u2208 h \\u2227 h \\u2208 153 E(H)} is in E.\\nWe show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width[7] bounded by k (short: class C(hw, k) of hypergraphs).\\nNote that a key issue of the tractability is to consider the hypertree width of the dual hypergraph \\u00afH instead of the auction hypergraph H.\\nIn fact, we can show that MaxWSP remains NP-hard even when H is acyclic (i.e., when it has hypertree width 1), even when each node is contained in 3 hyperedges at most.\\nFor some relevant special classes of hypergraphs in C(hw, k), we design a higly-parallelizeable algorithm for MaxWSP.\\nSpecifically, if the weighting functions can be computed in logarithmic space and weights are polynomial (e.g., when all the hyperegdes have unitary weights and one is interested in finding the packing with the maximum number of edges), we show that MaxWSP can be solved by a LOGCFL algorithm.\\nRecall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL \\u2286 NC2 \\u2286 P (see, e.g., [9]).\\nSurprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs.\\nTo the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs.\\nIn fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach.\\nIntuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition.\\nThe proof of the above results give us some interesting insight into the notion of structured item graph.\\nIndeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions.\\nA game-characterization for the notion of strict hypertree width is also proposed, which specializes the Robber and Marshals game in [6] (proposed to characterize the hypertree width), and which makes it clear the further requirements on hypertree decompositions.\\nThe rest of the paper is organized as follows.\\nSection 2 discusses the intractability of structured item graphs.\\nSection 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable.\\nThe comparison between the classes C(ig, k) and C(hw, k) is discussed in Section 4.\\nFinally, in Section 5 we draw our conclusions by also outlining directions for further research.\\n2.\\nCOMPLEXITY OF STRUCTURED ITEM GRAPHS Let H be a hypergraph.\\nA graph G = (V, E) is an item graph for H if V = N(H) and, for each h \\u2208 E(H), the subgraph of G induced over the nodes in h is connected.\\nAn important class of item graphs is that of structured item graphs, i.e., of those item graphs having bounded treewidth as formalized below.\\nA tree decomposition [16] of a graph G = (V, E) is a pair T, \\u03c7 , where T = (N, F) is a tree, and \\u03c7 is a labelling function assigning to each vertex p \\u2208 N a set of vertices \\u03c7(p) \\u2286 V , such that the following conditions are satisfied: (1) for each vertex b of G, there exists p \\u2208 N such that b \\u2208 \\u03c7(p); (2) for each edge {b, d} \\u2208 E, there exists p \\u2208 N such that {b, d} \\u2286 \\u03c7(p); (3) for each vertex b of G, the set {p \\u2208 N | b \\u2208 \\u03c7(p)} induces a connected subtree of T.\\nThe width of T, \\u03c7 is the number maxp\\u2208N |\\u03c7(p) \\u2212 1|.\\nThe treewidth of G, denoted by tw(G), is the minimum width over all its tree decompositions.\\nThe winner determination problem can be solved in polynomial time on item graphs having bounded treewidth [3].\\nTheorem 1 (cf. [3]).\\nAssume a k-width tree decomposition T, \\u03c7 of an item graph for H is given.\\nThen, MaxWSP(H, w) can be solved in time O(|T|2 \\u00d7(|E(H)|+1)k+1 ).\\nMany item graphs can be associated with a hypergraph.\\nAs an example, observe that the item graph in Figure 1.\\n(c) has treewidth 1, while Figure 1.\\n(d) reports an item graph whose treewidth is 2.\\nIndeed, it was an open question whether for a given constant k it can be checked in polynomial time if an item graph of treewidth k exists, and if so, whether such an item graph can be efficiently computed.\\nLet C(ig, k) denote the class of all the hypergraphs having an item graph G such that tw(G) \\u2264 k.\\nThe main result of this section is to show that the class C(ig, k) is hard to recognize.\\nTheorem 2.\\nDeciding whether a hypergraph H belongs to C(ig, 3) is NP-hard.\\nThe proof of this result relies on an elaborate reduction from the Hamiltonian path problem HP(s, t) of deciding whether there is an Hamiltonian path from a node s to a node t in a directed graph G = (N, E).\\nTo help the intuition, we report here a high-level overview of the main ingredients exploited in the proof1 .\\nThe general idea it to build a hypergraph HG such that there is an item graph G for HG with tw(G ) \\u2264 3 if and only if HP(s, t) over G has a solution.\\nFirst, we discuss the way HG is constructed.\\nSee Figure 2.\\n(a) for an illustration, where the graph G consists of the nodes s, x, y, and t, and the set of its edges is {e1 = (s, x), e2 = (x, y), e3 = (x, t), e4 = (y, t)}.\\nFrom G to HG.\\nLet G = (N, E) be a directed graph.\\nThen, the set of the nodes in HG is such that: for each x \\u2208 N, N(HG) contains the nodes bsx, btx, bx, bx, bdx; for each e = (x, y) \\u2208 E, N(HG) contains the nodes nsx, nsx, nty, nty , nse x and nte y. No other node is in N(HG).\\nHyperedges in HG are of three kinds: 1) for each x \\u2208 N, E(HG) contains the hyperedges: \\u2022 Sx = {bsx} \\u222a {nse x | e = (x, y) \\u2208 E}; \\u2022 Tx = {btx} \\u222a {nte x | e = (z, x) \\u2208 E}; \\u2022 A1 x = {bdx, bx}, A2 x = {bdx, bx}, and A3 x = {bx, bx} -notice that these hyperedges induce a clique on the nodes {bx, bx, bdx}; 1 Detailed proofs can be found in the Appendix, available at www.mat.unical.it/\\u223cggreco/papers/ca.pdf.\\n154 Figure 2: Proof of Theorem 2: (a) from G to HG - hyperedges in 1) and 2) are reported only; (b) a skeleton for a tree decomposition TD for HG.\\n\\u2022 SA1 x = {bsx, bx}, SA2 x = {bsx, bx}, SA3 x = {bsx, bdx} -notice that these hyperedges plus A1 x, A2 x, and A3 x induce a clique on the nodes {bsx, bx, bx, bdx}; \\u2022 TA1 x = {btx, bx}, TA2 x = {btx, bx}, and TA3 x = {btx, bdx} -notice that these hyperedges plus A1 x, A2 x, and A3 x induce a clique on the nodes {btx, bx, bx, bdx}; 2) for each e = (x, y) \\u2208 E, E(HG) contains the hyperedges: \\u2022 SHx = {nsx, nsx}; \\u2022 THy = {nty, nty }; \\u2022 SEe = {nsx, nse x} and SEe = {nsx, nse x} -notice that these two hyperedges plus SHx induce a clique on the nodes {nsx, nsx, nse x}; \\u2022 TEe = {nty, nte y} and TEe = {nty , nte y} -notice that these two hyperedges plus THy induce a clique on the nodes {nty, nty , nte y}.\\nNotice that each of the above hyperedges but those of the form Sx and Tx contains exactly two nodes.\\nAs an example of the hyperedges of kind 1) and 2), the reader may refer to the example construction reported in Figure 2.\\n(a), and notice, for instance, that Sx = {bsx, nse2 x , nse3 x } and that Tt = {btt, nte4 t , nte3 t }.\\n3) finally, we denote by DG the set containing the hyperedges in E(HG) of the third kind.\\nIn the reduction we are exploiting, DG can be an arbitrary set of hyperedges satisfying the four conditions that are discussed below.\\nLet PG be the set of the following |PG| \\u2264 |N| + 3 \\u00d7 |E| pairs: PG = {(bx, bx) | x \\u2208 N} \\u222a {(nsx, nsx), (nty, nty ), (nse x, nte y) | e = (x, y) \\u2208 E}.\\nAlso, let I(v) denote the set {h \\u2208 E(H) | v \\u2208 h} of the hyperedges of H that are touched by v; and, for a set V \\u2286 N(H), let I(V ) = v\\u2208V I(v).\\nThen, DG has to be a set such that: (c1) \\u2200(\\u03b1, \\u03b2) \\u2208 PG, I(\\u03b1) \\u2229 I(\\u03b2) \\u2229 DG = \\u2205; (c2) \\u2200(\\u03b1, \\u03b2) \\u2208 PG, I(\\u03b1) \\u222a I(\\u03b2) \\u2287 DG; (c3) \\u2200\\u03b1 \\u2208 N such that \\u2203\\u03b2 \\u2208 N with (\\u03b1, \\u03b2) \\u2208 PG or (\\u03b2, \\u03b1) \\u2208 PG, it holds: I(\\u03b1) \\u2229 DG = \\u2205; and, (c4) \\u2200S \\u2286 N such that |S| \\u2264 3 and where \\u2203\\u03b1, \\u03b2 \\u2208 S with (\\u03b1, \\u03b2) \\u2208 PG, it is the case that: I(S) \\u2287 DG.\\nIntuitively, the set DG is such that each of its hyperedges is touched by exactly one of the two nodes in every pair 155 of PG - cf. (c1) and (c2).\\nMoreover, hyperedges in DG touch only vertices included in at least a pair of PG - cf. (c3); and, any triple of nodes is not capable of touching all the elements of DG if none of the pairs that can be built from it belongs to PG - cf. (c4).\\nThe reader may now ask whether a set DG exists at all satisfying (c1), (c2), (c3) and (c4).\\nIn the following lemma, we positively answer this question and refer the reader to its proof for an example construction.\\nLemma 1.\\nA set DG, with |DG| = 2 \\u00d7 |PG| + 2, satisfying conditions (c1), (c2), (c3), and (c4) can be built in time O(|PG|2 ).\\nKey Ingredients.\\nWe are now in the position of presenting an overview of the key ingredients of the proof.\\nLet G be an arbitrary item graph for HG, and let TD = T, \\u03c7 be a 3-width tree decomposition of G (note that, because of the cliques, e.g., on the nodes {bsx, bx, bx, bdx}, any item graph for HG has treewidth 3 at least).\\nThere are three basic observations serving the purpose of proving the correctness of the reduction.\\nBlocks of TD: First, we observe that TD must contain some special kinds of vertex.\\nSpecifically, for each node x \\u2208 N, TD contains a vertex bs(x) such that \\u03c7(bs(x)) \\u2287 {bsx, bx, bx, bdx}, and a vertex bt(x) such that \\u03c7(bt(x)) \\u2287 {btx, bx, bx, bdx}.\\nAnd, for each edge e = (x, y) \\u2208 E, TD contains a vertex ns(x,e) such that \\u03c7(ns(x,e)) \\u2287 {nse x, nsx, nsx}, and a vertex nt(y,e) such that \\u03c7(nt(y,e)) \\u2287 {nte y, nty, nty }.\\nIntuitively, these vertices are required to cover the cliques of HG associated with the hyperedges of kind 1) and 2).\\nEach of these vertices plays a specific role in the reduction.\\nIndeed, each directed edge e = (x, y) \\u2208 E is encoded in TD by means of the vertices: ns(x,e), representing precisely that e starts from x; and, nt(y,e), representing precisely that e terminates into y. Also, each node x \\u2208 N is encoded in TD be means of the vertices: bs(x), representing the starting point of edges originating from x; and, bt(x), representing the terminating point of edges ending into x.\\nAs an example, Figure 2.\\n(b) reports the skeleton of a tree decomposition TD.\\nThe reader may notice in it the blocks defined above and how they are related with the hypergraph HG in Figure 2.\\n(a) - other blocks in it (of the form w(x,y)) are defined next.\\nConnectedness between blocks, and uniqueness of the connections: The second crucial observation is that in the path connecting a vertex of the form bs(x) (resp., bt(y)) with a vertex of the form ns(x,e) (resp., nt(y,e)) there is one special vertex of the form w(x,y) such that: \\u03c7(w(x,y)) \\u2287 {nse x , nte y }, for some edge e = (x, y) \\u2208 E. Guaranteeing the existence of one such vertex is precisely the role played by the hyperedges in DG.\\nThe arguments for the proof are as follows.\\nFirst, we observe that I(\\u03c7(bs(x))) \\u2229 I(\\u03c7(ns(x,e))) \\u2287 DG \\u222a {Sx} and I(\\u03c7(bt(y))) \\u2229 I(\\u03c7(nt(y,e))) \\u2287 DG \\u222a {Ty}.\\nThen, we show a property stating that for a pair of consecutive vertices p and q in the path connecting bs(x) and ns(x,e) (resp., bt(y) and nt(y,e)), I(\\u03c7(p) \\u2229 \\u03c7(q)) \\u2287 I(\\u03c7(bs(x))) \\u2229 I(\\u03c7(ns(x,e))) (resp., I(\\u03c7(p) \\u2229 \\u03c7(q)) \\u2287 I(\\u03c7(bt(x))) \\u2229 I(\\u03c7(nt(y,e)))).\\nThus, we have: I(\\u03c7(p) \\u2229 \\u03c7(q)) \\u2287 DG \\u222a{Sx} (resp., I(\\u03c7(p)\\u2229\\u03c7(q)) \\u2287 DG \\u222a{Ty}).\\nBased on this observation, and by exploiting the properties of the hyperedges in DG, it is not difficult to show that any pair of consecutive vertices p and q must share two nodes of HG forming a pair in PG, and must both touch Sx (resp., Ty).\\nWhen the treewidth of G is 3, we can conclude that a vertex, say w(x,y), in this path is such that \\u03c7(w(x,y)) \\u2287 {nse x , nte y }, for some edge e = (x, y) \\u2208 E - to this end, note that nse x \\u2208 Sx, nte t \\u2208 Ty, and I(\\u03c7(w(x,y))) \\u2287 DG.\\nIn particular, w(x,y) is the only kind of vertex satisfying these conditions, i.e., in the path there is no further vertex of the form w(x,z), for z = y (resp., w(z,y), for z = x).\\nTo help the intuition, we observe that having a vertex of the form w(x,y) in TD corresponds to the selection of an edge from node x to node y in the Hamiltonian path.\\nIn fact, given the uniqueness of these vertices selected for ensuring the connectivity, a one-to-one correspondence can be established between the existence of a Hamiltonian path for G and the vertices of the form w(x,y).\\nAs an example, in Figure 2.\\n(b), the vertices of the form w(s,x), w(x,y), and w(y,t) are in TD, and GT D shows the corresponding Hamiltonian path.\\nUnused blocks: Finally, the third ingredient of the proof is the observation that if a vertex of the form w(x,y), for an edge e = (x, y) \\u2208 E is not in TD (i.e., if the edge (x, y) does not belong to the Hamiltonian path), then the corresponding block ns(x,e ) (resp., nt(y,e )) can be arbitrarily appended in the subtree rooted at the block ns(x,e) (resp., nt(y,e)), where e is the edge of the form e = (x, z) (resp., e = (z, y)) such that w(x,z) (resp., w(z,y)) is in TD.\\nE.g., Figure 2.\\n(a) shows w(x,t), which is not used in TD, and Figure 2.\\n(b) shows how the blocks ns(x,e3) and nt(t,e3) can be arranged in TD for ensuring the connectedness condition.\\n3.\\nTRACTABLE CASES VIA HYPERTREE DECOMPOSITIONS Since constructing structured item graphs is intractable, it is relevant to assess whether other structural restrictions can be used to single out classes of tractable MaxWSP instances.\\nTo this end, we focus on the notion of hypertree decomposition [7], which is a natural generalization of hypergraph acyclicity and which has been profitably used in other domains, e.g, constraint satisfaction and database query evaluation, to identify tractability islands for NP-hard problems.\\nA hypertree for a hypergraph H is a triple T, \\u03c7, \\u03bb , where T = (N, E) is a rooted tree, and \\u03c7 and \\u03bb are labelling functions which associate each vertex p \\u2208 N with two sets \\u03c7(p) \\u2286 N(H) and \\u03bb(p) \\u2286 E(H).\\nIf T = (N , E ) is a subtree of T, we define \\u03c7(T ) = v\\u2208N \\u03c7(v).\\nWe denote the set of vertices N of T by vertices(T).\\nMoreover, for any p \\u2208 N, Tp denotes the subtree of T rooted at p. Definition 1.\\nA hypertree decomposition of a hypergraph H is a hypertree HD = T, \\u03c7, \\u03bb for H which satisfies all the following conditions: 1.\\nfor each edge h \\u2208 E(H), there exists p \\u2208 vertices(T) such that h \\u2286 \\u03c7(p) (we say that p covers h); 156 Figure 3: Example MaxWSP problem: (a) Hypergraph H1; (b) Hypergraph \\u00afH1; (b) A 2-width hypertree decomposition of \\u00afH1.\\n2.\\nfor each node Y \\u2208 N(H), the set {p \\u2208 vertices(T) | Y \\u2208 \\u03c7(p)} induces a (connected) subtree of T; 3.\\nfor each p \\u2208 vertices(T), \\u03c7(p) \\u2286 N(\\u03bb(p)); 4.\\nfor each p \\u2208 vertices(T), N(\\u03bb(p)) \\u2229 \\u03c7(Tp) \\u2286 \\u03c7(p).\\nThe width of a hypertree decomposition T, \\u03c7, \\u03bb is maxp\\u2208vertices(T )|\\u03bb(p)|.\\nThe HYPERTREE width hw(H) of H is the minimum width over all its hypertree decompositions.\\nA hypergraph H is acyclic if hw(H) = 1.\\nP Example 3.\\nThe hypergraph H I0,B0 reported in Figure 1.\\n(a) is an example acyclic hypergraph.\\nInstead, both the hypergraphs H1 and \\u00afH1 shown in Figure 3.\\n(a) and Figure 3.\\n(b), respectively, are not acyclic since their hypertree width is 2.\\nA 2-width hypertree decomposition for \\u00afH1 is reported in Figure 3.\\n(c).\\nIn particular, observe that H1 has been obtained by adding the two hyperedges h4 and h5 to H I0,B0 to model, for instance, that two new bids, B4 and B5, respectively, have been proposed to the auctioneer.\\n\\u00a1 In the following, rather than working on the hypergraph H associated with a MaxWSP problem, we shall deal with its dual \\u00afH, i.e., with the hypergraph such that its nodes are in one-to-one correspondence with the hyperedges of H, and where for each node x \\u2208 N(H), {h | x \\u2208 h \\u2227 h \\u2208 E(H)} is in E( \\u00afH).\\nAs an example, the reader may want to check again the hypergraph H1 in Figure 3.\\n(a) and notice that the hypergraph in Figure 3.\\n(b) is in fact its dual.\\nThe rationale for this choice is that issuing restrictions on the original hypergraph is a guarantee for the tractability only in very simple scenarios.\\nTheorem 3.\\nOn the class of acyclic hypergraphs, MaxWSP is (1) in P if each node occurs into two hyperedges at most; and, (2) NP-hard, even if each node is contained into three hyperedges at most.\\n3.1 Hypertree Decomposition on the Dual Hypergraph and Tractable Packing Problems For a fixed constant k, let C(hw, k) denote the class of all the hypergraphs whose dual hypergraphs have hypertree width bounded by k.\\nThe maximum weighted-set packing problem can be solved in polynomial time on the class C(hw, k) by means of the algorithm ComputeSetPackingk, shown in Figure 4.\\nThe algorithm receives in input a hypergraph H, a weighting function w, and a k-width hypertree decomposition HD = T=(N, E), \\u03c7, \\u03bb of \\u00afH. For each vertex v \\u2208 N, let Hv be the hypergraph whose set of nodes N(Hv) \\u2286 N(H) coincides with \\u03bb(v), and whose set of edges E(Hv) \\u2286 E(H) coincides with \\u03c7(v).\\nIn an initialization step, the algorithm equips each vertex v with all the possible packings for Hv, which are stored in the set Hv.\\nNote that the size of Hv is bounded by (|E(H)| + 1)k , since each node in \\u03bb(v) is either left uncovered in a packing or is covered with precisely one of the hyperedges in \\u03c7(v) \\u2286 E(H).\\nThen, ComputeSetPackingk is designed to filter these packings by retaining only those that conform with some packing for Hc, for each children c of v in T, as formalized next.\\nLet hv and hc be two packings for Hv and Hc, respectively.\\nWe say that hv conforms with hc, denoted by hv \\u2248 hc if: for each h \\u2208 hc \\u2229 E(Hv), h is in hv; and, for each h \\u2208 (E(Hc) \\u2212 hc), h is not in hv.\\nExample 4.\\nConsider again the hypertree decomposition of \\u00afH1 reported in Figure 3.\\n(c).\\nThen, the set of all the possible packings (which are build in the initialization step of ComputeSetPackingk), for each of its vertices, is reFigure 5: Example application of Algorithm ComputeSetPackingk.\\n157 Input: H, w, and a k-width hypertree decomposition HD = T =(N, E), \\u03c7, \\u03bb of \\u00afH; Output: A solution to MaxWSP(H, w); var Hv : set of packings for Hv, for each v \\u2208 N; h\\u2217 : packing for H; v hv : rational number, for each partial packing hv for Hv; hhv,c : partial packing for Hc, for each partial packing hv for Hv, and for each (v, c) \\u2208 E; -------------------------------------------Procedure BottomUp; begin Done := the set of all the leaves of T ; while \\u2203v \\u2208 T such that (i) v \\u2208 Done, and (ii) {c | c is child of v} \\u2286 Done do for each c such that (v, c) \\u2208 E do Hv := Hv \\u2212 {hv | \\u2203hc \\u2208 Hc s.t. hv \\u2248 hc}; for each hv \\u2208 Hv do v hv := w(hv); for each c such that (v, c) \\u2208 E do \\u00afhc := arg maxhc\\u2208Hc|hv\\u2248 hc c hc \\u2212 w(hc \\u2229 hv) ; hhv,c := \\u00afhc; (* set best packing *) v hv := v hv + c \\u00afhc \\u2212 w(\\u00afhc \\u2229 hv); end for end for Done := Done \\u222a {v}; end while end; -------------------------------------------begin (* MAIN *) for each vertex v in T do Hv := {hv packing for Hv}; BottomUp; let r be the root of T ; \\u00afhr := arg maxhr\\u2208Hr r hr ; h\\u2217 := \\u00afhr; (* include packing *) T opDown(r, hr); return h\\u2217 ; end.\\nProcedure T opDown(v : vertex of N, \\u00afhv \\u2208 Hv); begin for each c \\u2208 N s.t. (v, c) \\u2208 E do \\u00afhc := h\\u00afhv,c; h\\u2217 := h\\u2217 \\u222a \\u00afhc; (* include packing *) T opDown(c, \\u00afhc); end for end; Figure 4: Algorithm ComputeSetPackingk.\\nported in Figure 5.\\n(a).\\nFor instance, the root v1 is such that Hv1 = { {}, {h1}, {h3}, {h5} }.\\nMoreover, an arrow from a packing hc to hv denotes that hv conforms with hc.\\nFor instance, the reader may check that the packing {h3} \\u2208 Hv1 conforms with the packing {h2, h3} \\u2208 Hv3 , but do not conform with {h1} \\u2208 Hv3 .\\n\\u00a1 ComputeSetPackingk builds a solution by traversing T in two phases.\\nIn the first phase, vertices of T are processed from the leaves to the root r, by means of the procedure BottomUp.\\nFor each node v being processed, the set Hv is preliminary updated by removing all the packings hv that do not conform with any packing for some of the children of v.\\nAfter this filtering is performed, the weight hv is updated.\\nIntuitively, v hv stores the weight of the best partial packing for H computed by using only the hyperedges occurring in \\u03c7(Tv).\\nIndeed, if v is a leaf, then v hv = w(hv).\\nOtherwise, for each child c of v in T, v hv is updated with the maximum of c hc \\u2212 w(hc \\u2229 hv) over all the packings hc that conforms with hv (resolving ties arbitrarily).\\nThe packing \\u00afhc for which this maximum is achieved is stored in the variable hhv,c.\\nIn the second phase, the tree T is processed starting from the root.\\nFirstly, the packing h\\u2217 is selected that maximizes the weight equipped with the packings in Hr.\\nThen, procedure TopDown is used to extend h\\u2217 to all the other partial packings for vertices of T.\\nIn particular, at each vertex v, h\\u2217 is extended with the packing hhv,c, for each child c of v. Example 5.\\nAssume that, in our running example, w(h1) = w(h2) = w(h3) = w(h4) = 1.\\nThen, an execution of ComputeSetPackingk is graphically depicted in Figure 5.\\n(b), where an arrow from a packing hc to a packing hv is used to denote that hc = hhv,c. Specifically, the choices made during the computation are such that the packing {h2, h3} is computed.\\nIn particular, during the bottom-up phase, we have that: (1) v4 is processed, and we set v4 {h2} = v4 {h4} = 1 and v4 {} = 0; (2) v3 is processed, and we set v3 {h1} = v3 {h3} = 1 and v3 {} = 0; (3) v2 is processed, and we set v2 {h1} = v2 {h2} = v2 {h3} = v2 {h4} = 1, v2 {h2,h3} = 2 and v3 {} = 0; (4) v1 is processed and we set v1 {h1} = 1, v1 {h5} = v1 {h3} = 2 and v1 {} = 0.\\nFor instance, note that v1 {h5} = 2 since {h5} conforms with the packing {h4} of Hv2 such that v2 {h4} = 1.\\nThen, at the beginning of the top-down phase, ComputeSetPackingk selects {h3} as a packing for Hv1 and propagates this choice in the tree.\\nEquivalently, the algorithm may have chosen {h5}.\\nAs a further example, the way the solution {h1} is obtained by the algorithm when w(h1) = 5 and w(h2) = w(h3) = w(h4) = 1 is reported in Figure 5.\\n(c).\\nNotice that, this time, in the top-down phase, ComputeSetPackingk starts selecting {h1} as the best packing for Hv1 .\\n\\u00a1 Theorem 4.\\nLet H be a hypergraph and w be a weighting function for it.\\nLet HD = T, \\u03c7, \\u03bb be a complete k-width hypertree decomposition of \\u00afH. Then, ComputeSetPackingk on input H, w, and HD correctly outputs a solution for MaxWSP(H, w) in time O(|T| \\u00d7 (|E(H)| + 1)2k ).\\nProof.\\n[Sketch] We observe that h\\u2217 (computed by ComputeSetPackingk) is a packing for H. Indeed, consider a pair of hyperedges h1 and h2 in h\\u2217 , and assume, for the sake of contradiction, that h1 \\u2229 h2 = \\u2205.\\nLet v1 (resp., v2) be an arbitrary vertex of T, for which ComputeSetPackingk included h1 (resp., h2) in h\\u2217 in the bottom-down computation.\\nBy construction, we have h1 \\u2208 \\u03c7(v1) and h2 \\u2208 \\u03c7(v2).\\n158 Let I be an element in h1 \\u2229 h2.\\nIn the dual hypergraph H, I is a hyperedge in E( \\u00afH) which covers both the nodes h1 and h2.\\nHence, by condition (1) in Definition 1, there is a vertex v \\u2208 vertices(T) such that {h1, h2} \\u2286 \\u03c7(v).\\nNote that, because of the connectedness condition in Definition 1, we can also assume, w.l.o.g., that v is in the path connecting v1 and v2 in T. Let hv \\u2208 Hv denote the element added by ComputeSetPackingk into h\\u2217 during the bottom-down phase.\\nSince the elements in Hv are packings for Hv, it is the case that either h1 \\u2208 hv or h2 \\u2208 hv.\\nAssume, w.l.o.g., that h1 \\u2208 hv, and notice that each vertex w in T in the path connecting v to v1 is such that h1 \\u2208 \\u03c7(w), because of the connectedness condition.\\nHence, because of definition of conformance, the packing hw selected by ComputeSetPackingk to be added at vertex w in h\\u2217 must be such that h1 \\u2208 hw.\\nThis holds in particular for w = v1.\\nContradiction with the definition of v1.\\nTherefore, h\\u2217 is a packing for H.\\nIt remains then to show that it has the maximum weight over all the packings for H. To this aim, we can use structural induction on T to prove that, in the bottom-up phase, the variable v hv is updated to contain the weight of the packing on the edges in \\u03c7(Tv), which contains hv and which has the maximum weight over all such packings for the edges in \\u03c7(Tv).\\nThen, the result follows, since in the top-down phase, the packing hr giving the maximum weight over \\u03c7(Tr) = E(H) is first included in h\\u2217 , and then extended at each node c with the packing hhv,c conformingly with hv and such that the maximum value of v hv is achieved.\\nAs for the complexity, observe that the initialization step requires the construction of the set Hv, for each vertex v, and each set has size (|E(H)| + 1)k at most.\\nThen, the function BottomUp checks for the conformance between strategies in Hv with strategies in Hc, for each pair (v, c) \\u2208 E, and updates the weight v hv .\\nThese tasks can be carried out in time O((|E(H)| + 1)2k ) and must be repeated for each edge in T, i.e., O(|T|) times.\\nFinally, the function TopDown can be implemented in linear time in the size of T, since it just requires updating h\\u2217 by accessing the variable hhv,c.\\nThe above result shows that if a hypertree decomposition of width k is given, the MaxWSP problem can be efficiently solved.\\nMoreover, differently from the case of structured item graphs, it is well known that deciding the existence of a k-bounded hypertree decomposition and computing one (if any) are problems which can be efficiently solved in polynomial time [7].\\nTherefore, Theorem 4 witnesses that the class C(hw, k) actually constitutes a tractable class for the winner determination problem.\\nAs the following theorem shows, for large subclasses (that depend only on how the weight function is specified), MaxWSP(H, w) is even highly parallelizeable.\\nLet us call a weighting function smooth if it is logspace computable and if all weights are polynomial (and thus just require O(log n) bits for their representation).\\nRecall that LOGCFL is a parallel complexity class contained in NC2, cf. [9].\\nThe functional version of LOGCFL is LLOGCFL , which is obtained by equipping a logspace transducer with an oracle in LOGCFL.\\nTheorem 5.\\nLet H be a hypergraph in C(hw, k), and let w be a smooth weighting function for it.\\nThen, MaxWSP(H, w) is in LLOGCFL .\\n4.\\nHYPERTREE DECOMPOSITIONS VS STRUCTURED ITEM GRAPHS Given that the class C(hw, k) has been shown to be an island of tractability for the winner determination problem, and given that the class C(ig, k) has been shown not to be efficiently recognizable, one may be inclined to think that there are instances having unbounded hypertree width, but admitting an item graph of bounded tree width (so that the intractability of structured item graphs would lie in their generality).\\nSurprisingly, we establish this is not the case.\\nThe line of the proof is to first show that structured item graphs are in one-to-one correspondence with a special kind of hypertree decompositions of the dual hypergraph, which we shall call strict.\\nThen, the result will follow by proving that k-width strict hypertree decompositions are less powerful than kwith hypertree decompositions.\\n4.1 Strict Hypertree Decompositions Let H be a hypergraph, and let V \\u2286 N(H) be a set of nodes and X, Y \\u2208 N(H).\\nX is [V ]-adjacent to Y if there exists an edge h \\u2208 E(H) such that {X, Y } \\u2286 (h \\u2212 V ).\\nA [V ]-path \\u03c0 from X to Y is a sequence X = X0, ... , X = Y of variables such that: Xi is [V ]-adjacent to Xi+1, for each i \\u2208 [0... -1].\\nA set W \\u2286 N(H) of nodes is [V ]-connected if \\u2200X, Y \\u2208 W there is a [V ]-path from X to Y .\\nA [V ]-component is a maximal [V ]-connected non-empty set of nodes W \\u2286 (N(H) \\u2212 V ).\\nFor any [V ]-component C, let E(C) = {h \\u2208 E(H) | h \\u2229 C = \\u2205}.\\nDefinition 2.\\nA hypertree decomposition HD = T, \\u03c7, \\u03bb of H is strict if the following conditions hold: 1.\\nfor each pair of vertices r and s in vertices(T) such that s is a child of r, and for each [\\u03c7(r)]-component Cr s.t. Cr \\u2229 \\u03c7(Ts) = \\u2205, Cr is a [\\u03c7(r) \\u2229 N(\\u03bb(r) \\u2229 \\u03bb(s))]-component; 2.\\nfor each edge h \\u2208 E(H), there is a vertex p such that h \\u2208 \\u03bb(p) and h \\u2286 \\u03c7(p) (we say p strongly covers h); 3.\\nfor each edge h \\u2208 E(H), the set {p \\u2208 vertices(T) | h \\u2208 \\u03bb(p)} induces a (connected) subtree of T.\\nThe strict hypertree width shw(H) of H is the minimum width over all its strict hypertree decompositions.\\nP The basic relationship between nice hypertree decompositions and structured item graphs is shown in the following theorem.\\nTheorem 6.\\nLet H be a hypergraph such that for each node v \\u2208 N(H), {v} is in E(H).\\nThen, a k-width tree decomposition of an item graph for H exists if and only if \\u00afH has a (k + 1)-width strict hypertree decomposition2 .\\nNote that, as far as the maximum weighted-set packing problem is concerned, given a hypergraph H, we can always assume that for each node v \\u2208 N(H), {v} is in E(H).\\nIn fact, if this hyperedge is not in the hypergraph, then it can be added without loss of generality, by setting w({v}) = 0.\\nTherefore, letting C(shw, k) denote the class of all the hypergraphs whose dual hypergraphs (associated with maximum 2 The term +1 only plays the technical role of taking care of the different definition of width for tree decompositions and hypertree decompositions.\\n159 weighted-set packing problems) have strict hypertree width bounded by k, we have that C(shw, k + 1) = C(ig, k).\\nBy definition, strict hypertree decompositions are special hypertree decompositions.\\nIn fact, we are able to show that the additional conditions in Definition 2 induce an actual restriction on the decomposition power.\\nTheorem 7.\\nC(ig, k) = C(shw, k + 1) \\u2282 C(hw, k + 1).\\nA Game Theoretic View.\\nWe shed further lights on strict hypertree decompositions by discussing an interesting characterization based on the strict Robber and Marshals Game, defined by adapting the Robber and Marshals game defined in [6], which characterizes hypertree width.\\nThe game is played on a hypergraph H by a robber against k marshals which act in coordination.\\nMarshals move on the hyperedges of H, while the robber moves on nodes of H.\\nThe robber sees where the marshals intend to move, and reacts by moving to another node which is connected with its current position and through a path in G(H) which does not use any node contained in a hyperedge that is occupied by the marshals before and after their move-we say that these hyperedges are blocked.\\nNote that in the basic game defined in [6], the robber is not allowed to move on vertices that are occupied by the marshals before and after their move, even if they do not belong to blocked hyperedges.\\nImportantly, marshals are required to play monotonically, i.e., they cannot occupy an edge that was previously occupied in the game, and which is currently not.\\nThe marshals win the game if they capture the robber, by occupying an edge covering a node where the robber is.\\nOtherwise, the robber wins.\\nTheorem 8.\\nLet H be a hypergraph such that for each node v \\u2208 N(H), {v} is in E(H).\\nThen, \\u00afH has a k-width strict hypertree decomposition if and only if k marshals can win the strict Robber and Marshals Game on \\u00afH, no matter of the robber``s moves.\\n5.\\nCONCLUSIONS We have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario.\\nThe result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3.\\nMotivated by this result, we investigated the use of hypertree decomposition (on the dual hypergraph associated with the scenario) and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width.\\nFor some special, yet relevant cases, a highly parallelizable algorithm is also discussed.\\nInterestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width (hence, the reason of their intractability is not their generality).\\nIn particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions (on the dual hypergraph), called query decompositions (see, e.g., [7]).\\nIn the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.\\nAs a further avenue of research, it would be relevant to enhance the algorithm ComputeSetPackingk, e.g., by using specialized data structures, in order to avoid the quadratic dependency from (|E(H)| + 1)k .\\nFinally, an other interesting question is to assess whether the structural decomposition techniques discussed in the paper can be used to efficiently deal with generalizations of the winner determination problem.\\nFor instance, it might be relevant in several application scenarios to design algorithms that can find a selling strategy when several copies of the same item are available for selling, and when moreover the auctioneer is satisfied when at least a given number of copies is actually sold.\\nAcknowledgement G. Gottlob``s work was supported by the EC3 - E-Commerce Competence Center (Vienna) and by a Royal Society Wolfson Research Merit Award.\\nIn particular, this Award allowed Gottlob to invite G. Greco for a research visit to Oxford.\\nIn addition, G. Greco is supported by ICAR-CNR, and by M.I.U.R. under project TOCAI.IT.\\n6.\\nREFERENCES [1] I. Adler, G. Gottlob, and M. Grohe.\\nHypertree-Width and Related Hypergraph Invariants.\\nIn Proc.\\nof EUROCOMB``05, pages 5-10, 2005.\\n[2] C. Boutilier.\\nSolving Concisely Expressed Combinatorial Auction Problems.\\nIn Proc.\\nof AAAI``02, pages 359-366, 2002.\\n[3] V. Conitzer, J. Derryberry, and T. Sandholm.\\nCombinatorial auctions with structured item graphs.\\nIn Proc.\\nof AAAI``04, pages 212-218, 2004.\\n[4] E. M. Eschen and J. P. Sinrad.\\nAn o(n2 ) algorithm for circular-arc graph recognition.\\nIn Proc.\\nof SODA``93, pages 128-137, 1993.\\n[5] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.\\nTaming the computational complexity of combinatorial auctions: Optimal and approximate.\\nIn Proc.\\nof IJCAI``99, pages 548-553, 1999.\\n[6] G. Gottlob, N. Leone, and F. Scarcello.\\nRobbers, marshals, and guards: game theoretic and logical characterizations of hypertree width.\\nJournal of Computer and System Sciences, 66(4):775-808, 2003.\\n[7] G. Gottlob, N. Leone, and S. Scarcello.\\nHypertree decompositions and tractable queries.\\nJournal of Computer and System Sciences, 63(3):579-627, 2002.\\n[8] H. H. Hoos and C. Boutilier.\\nSolving combinatorial auctions using stochastic local search.\\nIn Proc.\\nof AAAI``00, pages 22-29, 2000.\\n[9] D. Johnson.\\nA Catalog of Complexity Classes.\\nIn P. Cramton, Y. Shoham, and R. Steinberg, editors, Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, pages 67-161.\\n1990.\\n[10] N. Korte and R. H. Mohring.\\nAn incremental linear-time algorithm for recognizing interval graphs.\\nSIAM Journal on Computing, 18(1):68-81, 1989.\\n[11] D. Lehmann, R. M\\u00a8uller, and T. Sandholm.\\nThe Winner Determination Problem.\\nIn P. Cramton, Y. Shoham, and R. Steinberg, editors, Combinatorial Auctions.\\nMIT Press, 2006.\\n[12] D. Lehmann, L. I. O``Callaghan, and Y. Shoham.\\nTruth revelation in approximately efficient 160 combinatorial auctions.\\nJ. ACM, 49(5):577-602, 2002.\\n[13] R. McAfee and J. McMillan.\\nAnalyzing the airwaves auction.\\nJournal of Economic Perspectives, 10(1):159175, 1996.\\n[14] J. McMillan.\\nSelling spectrum rights.\\nJournal of Economic Perspectives, 8(3):145-62, 1994.\\n[15] N. Nisan.\\nBidding and allocation in combinatorial auctions.\\nIn Proc.\\nof EC``00, pages 1-12, 2000.\\n[16] N. Robertson and P. Seymour.\\nGraph minors ii.\\nalgorithmic aspects of tree width.\\nJournal of Algorithms, 7:309-322, 1986.\\n[17] M. H. Rothkopf, A. Pekec, and R. M. Harstad.\\nComputationally manageable combinatorial auctions.\\nManagement Science, 44:1131-1147, 1998.\\n[18] T. Sandholm.\\nAn implementation of the contract net protocol based on marginal cost calculations.\\nIn Proc.\\nof AAAI``93, pages 256-262, 1993.\\n[19] T. Sandholm.\\nAlgorithm for optimal winner determination in combinatorial auctions.\\nArtificial Intelligence, 135(1-2):1-54, 2002.\\n[20] T. Sandholm.\\nWinner determination algorithms.\\nIn P. Cramton, Y. Shoham, and R. Steinberg, editors, Combinatorial Auctions.\\nMIT Press, 2006.\\n[21] T. Sandholm and S. Suri.\\nBob: Improved winner determination in combinatorial auctions and generalizations.\\nArtificial Intelligence, 7:33-58, 2003.\\n[22] M. Tennenholtz.\\nSome tractable combinatorial auctions.\\nIn Proc.\\nof AAAI``00, pages 98-103, 2000.\\n[23] E. Zurel and N. Nisan.\\nAn efficient approximate allocation algorithm for combinatorial auctions.\\nIn Proc.\\nof EC``01, pages 125-136, 2001.\\n161\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lvl-2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems\\n{jasonh, kansal, szahedi, mbs} @ ee.ucla.edu vijay@nec-labs.com\\nABSTRACT\\nHarvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks.\\nIn this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment.\\nThe algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energyneutrality constraint, and (c) adapting to the dynamics of the energy source at run-time.\\nWe present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data.\\nWe also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source.\\nOur methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\\n1.\\nINTRODUCTION\\nEnergy supply has always been a crucial issue in designing battery-powered wireless sensor networks because the lifetime and utility of the systems are limited by how long the batteries are able to sustain the operation.\\nThe fidelity of the data produced by a sensor network begins to degrade once sensor nodes start to run out of battery power.\\nTherefore, harvesting energy from the environment has been proposed to supplement or completely replace battery supplies to enhance system lifetime and reduce the maintenance cost of replacing batteries periodically.\\nHowever, metrics for evaluating energy harvesting systems are different from those used for battery powered systems.\\nEnvironmental energy is distinct from battery energy in two ways.\\nFirst it is an inexhaustible supply which, if appropriately used, can allow the system to last forever, unlike the battery which is a limited resource.\\nSecond, there is an uncertainty associated with its availability and measurement, compared to the energy stored in the\\nbattery which can be known deterministically.\\nThus, power management methods based on battery status are not always applicable to energy harvesting systems.\\nIn addition, most power management schemes designed for battery-powered systems only account for the dynamics of the energy consumers (e.g., CPU, radio) but not the dynamics of the energy supply.\\nConsequently, battery powered systems usually operate at the lowest performance level that meets the minimum data fidelity requirement in order to maximize the system life.\\nEnergy harvesting systems, on the other hand, can provide enhanced performance depending on the available energy.\\nIn this paper, we will study how to adapt the performance of the available energy profile.\\nThere exist many techniques to accomplish performance scaling at the node level, such as radio transmit power adjustment [1], dynamic voltage scaling [2], and the use of low power modes [3].\\nHowever, these techniques require hardware support and may not always be available on resource constrained sensor nodes.\\nAlternatively, a common performance scaling technique is duty cycling.\\nLow power devices typically provide at least one low power mode in which the node is shut down and the power consumption is negligible.\\nIn addition, the rate of duty cycling is directly related to system performance metrics such as network latency and sampling frequency.\\nWe will use duty cycle adjustment as the primitive performance scaling technique in our algorithms.\\n2.\\nRELATED WORK\\nEnergy harvesting has been explored for several different types of systems, such as wearable computers [4], [5], [6], sensor networks [7], etc. .\\nSeveral technologies to extract energy from the environment have been demonstrated including solar, motion-based, biochemical, vibration-based [8], [9], [10], [11], and others are being developed [12], [13].\\nWhile several energy harvesting sensor node platforms have been prototyped [14], [15], [16], there is a need for systematic power management techniques that provide performance guarantees during system operation.\\nThe first work to take environmental energy into account for data routing was [17], followed by [18].\\nWhile these works did demonstrate that environment aware decisions improve performance compared to battery aware decisions, their objective was not to achieve energy neutral operation.\\nOur proposed techniques attempt to maximize system performance while maintaining energy-neutral operation.\\n3.\\nSYSTEM MODEL\\nThe energy usage considerations in a harvesting system vary significantly from those in a battery powered system, as mentioned earlier.\\nWe propose the model shown in Figure 1 for designing energy management methods in a harvesting system.\\nThe functions of the various blocks shown in the figure are discussed below.\\nThe precise methods used in our system to achieve these functions will be discussed in subsequent sections.\\nHarvested Energy Tracking: This block represents the mechanisms used to measure the energy received from the harvesting device, such as the solar panel.\\nSuch information is useful for determining the energy availability profile and adapting system performance based on it.\\nCollecting this information requires that the node hardware be equipped with the facility to measure the power\\nFigure 1.\\nSystem model for an energy harvesting system.\\ngenerated from the environment, and the Heliomote platform [14] we used for evaluating the algorithms has this capability.\\nEnergy Generation Model: For wireless sensor nodes with limited storage and processing capabilities to be able to use the harvested energy data, models that represent the essential components of this information without using extensive storage are required.\\nThe purpose of this block is to provide a model for the energy available to the system in a form that may be used for making power management decisions.\\nThe data measured by the energy tracking block is used here to predict future energy availability.\\nA good prediction model should have a low prediction error and provide predicted energy values for durations long enough to make meaningful performance scaling decisions.\\nFurther, for energy sources that exhibit both long-term and short-term patterns (e.g., diurnal and climate variations vs. weather patterns for solar energy), the model must be able to capture both characteristics.\\nSuch a model can also use information from external sources such as local weather forecast service to improve its accuracy.\\nEnergy Consumption Model: It is also important to have detailed information about the energy usage characteristics of the system, at various performance levels.\\nFor general applicability of our design, we will assume that only one sleep mode is available.\\nWe assume that the power consumption in the sleep and active modes is known.\\nIt may be noted that for low power systems with more advanced capabilities such as dynamic voltage scaling (DVS), multiple low power modes, and the capability to shut down system components selectively, the power consumption in each of the states and the resultant effect on application performance should be known to make power management decisions.\\nEnergy Storage Model: This block represents the model for the energy storage technology.\\nSince all the generated energy may not be used instantaneously, the harvesting system will usually have some energy storage technology.\\nStorage technologies (e.g., batteries and ultra-capacitors) are non-ideal, in that there is some energy loss while storing and retrieving energy from them.\\nThese characteristics must be known to efficiently manage energy usage and storage.\\nThis block also includes the system capability to measure the residual stored energy.\\nMost low power systems use batteries to store energy and provide residual battery status.\\nThis is commonly based on measuring the battery voltage which is then mapped to the residual battery energy using the known charge to voltage relationship for the battery technology in use.\\nMore sophisticated methods which track the flow of energy into and out of the battery are also available.\\nHarvesting-aware Power Management: The inputs provided by the previously mentioned blocks are used here to determine the suitable power management strategy for the system.\\nPower management could be carried to meet different objectives in different applications.\\nFor instance, in some systems, the harvested energy may marginally supplement the battery supply and the objective may be to maximize the system lifetime.\\nA more interesting case is when the harvested energy is used as the primary source of energy for the system with the objective of achieving indefinitely long system lifetime.\\nIn such cases, the power management objective is to achieve energy neutral operation.\\nIn other words, the system should only use as much energy as harvested from the environment and attempt to maximize performance within this available energy budget.\\n4.\\nTHEORETICALLY OPTIMAL POWER MANAGEMENT\\nWe develop the following theory to understand the energy neutral mode of operation.\\nLet us define Ps (t) as the energy harvested from the environment at time t, and the energy being consumed by the load at that time is Pc (t).\\nFurther, we model the non-ideal storage buffer by its round-trip efficiency 11 (strictly less than 1) and a constant leakage power Pleak.\\nUsing this notation, applying the rule of energy conservation leads to the following inequality:\\nwhere B0 is the initial battery level and the function [X] + = X if X> 0 and zero otherwise.\\nDEFINITION 1 (p,61,62) function: A non-negative, continuous and bounded function P (t) is said to be a (p,61,62) function if and only if for any value of finite real number T, the following are satisfied:\\nThis function can be used to model both energy sources and loads.\\nIf the harvested energy profile Ps (t) is a (p1,61,62) function, then the average rate of available energy over long durations becomes p1, and the burstiness is bounded by 61 and 62.\\nSimilarly, Pc (t) can be modeled as a (p2,63) function, when p2 and 63 are used to place an upper bound on power consumption (the inequality on the right side) while there are no minimum power consumption constraints.\\nThe condition for energy neutrality, equation (1), leads to the following theorem, based on the energy production, consumption, and energy buffer models discussed above.\\nTHEOREM 1 (ENERGY NEUTRAL OPERATION): Consider a harvesting system in which the energy production profile is characterized by a (p1, 61, 62) function, the load is characterized by a (p2, 63) function and the energy buffer is characterized by parameters ii for storage efficiency, and Pleak for leakage power.\\nThe following conditions are sufficient for the system to achieve energy neutrality:\\nwhere B0 is the initial energy stored in the buffer and provides a lower bound on the capacity of the energy buffer B.\\nThe proof is presented in our prior work [19].\\nTo adjust the duty cycle D using our performance scaling algorithm, we assume the following relation between duty cycle and the perceived utility of the system to the user: Suppose the utility of the application to the user is represented by U (D) when the system operates at a duty cycle D. Then,\\nThis is a fairly general and simple model and the specific values of Dmin and Dmax may be determined as per application requirements.\\nAs an example, consider a sensor node designed to detect intrusion across a periphery.\\nIn this case, a linear increase in duty cycle translates into a linear increase in the detection probability.\\nThe fastest and the slowest speeds of the intruders may be known, leading to a minimum and\\nFigure 2.\\nTwo possible cases for energy calculations\\nmaximum sensing delay tolerable, which results in the relevant Dmax and Dmin for the sensor node.\\nWhile there may be cases where the relationship between utility and duty cycle may be non-linear, in this paper, we restrict our focus on applications that follow this linear model.\\nIn view of the above models for the system components and the required performance, the objective of our power management strategy is adjust the duty cycle D (i) dynamically so as to maximize the total utility U (D) over a period of time, while ensuring energy neutral operation for the sensor node.\\nBefore discussing the performance scaling methods for harvesting aware duty cycle adaptation, let us first consider the optimal power management strategy that is possible for a given energy generation profile.\\nFor the calculation of the optimal strategy, we assume complete knowledge of the energy availability profile at the node, including the availability in the future.\\nThe calculation of the optimal is a useful tool for evaluating the performance of our proposed algorithm.\\nThis is particularly useful for our algorithm since no prior algorithms are available to serve as a baseline for comparison.\\nSuppose the time axis is partitioned into discrete slots of duration \\u0394T, and the duty cycle adaptation calculation is carried out over a window of Nw such time slots.\\nWe define the following energy profile variables, with the index i ranging over {1,..., Nw}: Ps (i) is the power output from the harvested source in time slot i, averaged over the slot duration, Pc is the power consumption of the load in active mode, and D (i) is the duty cycle used in slot i, whose value is to be determined.\\nB (i) is the residual battery energy at the beginning of slot i. Following this convention, the battery energy left after the last slot in the window is represented by B (Nw +1).\\nThe values of these variables will depend on the choice of D (i).\\nThe energy used directly from the harvested source and the energy stored and used from the battery must be accounted for differently.\\nFigure 2 shows two possible cases for Ps (i) in a time slot.\\nPs (i) may either be less than or higher than Pc, as shown on the left and right respectively.\\nWhen Ps (i) is lower than Pc, some of the energy used by the load comes from the battery, while when Ps (i) is higher than Pc, all the energy used is supplied directly from the harvested source.\\nThe crosshatched area shows the energy that is available for storage into the battery while the hashed area shows the energy drawn from the battery.\\nWe can write the energy used from the battery in any slot i as:\\nIn equation (6), the first term on the right hand side measures the energy drawn from the battery when Ps (i) <Pc, the next term measures the energy stored into the battery when the node is in sleep mode, and the last term measures the energy stored into the battery in active mode if Ps (i)> Pc.\\nFor energy neutral operation, we require the battery at the end of the window of Nw slots to be greater than or equal to the starting battery.\\nClearly, battery level will go down when the harvested energy is not available and the system is operated from stored energy.\\nHowever, the window Nw is judiciously chosen such that over that duration, we expect the environmental energy availability to complete a periodic cycle.\\nFor instance, in the case of solar energy harvesting, Nw could be chosen to be a twenty-four hour duration, corresponding to the diurnal cycle in the harvested energy.\\nThis is an approximation since an ideal choice of the window size would be infinite, but a finite size must be used for analytical tractability.\\nFurther, the battery level cannot be negative at any time, and this is ensured by having a large enough initial battery level B0 such that node operation is sustained even in the case of total blackout during a window period.\\nStating the above constraints quantitatively, we can express the calculation of the optimal duty cycles as an optimization problem below:\\nThe solution to the optimization problem yields the duty cycles that must be used in every slot and the evolution of residual battery over the course of Nw slots.\\nNote that while the constraints above contain the non-linear function [x] +, the quantities occurring within that function are all known constants.\\nThe variable quantities occur only in linear terms and hence the above optimization problem can be solved using standard linear programming techniques, available in popular optimization toolboxes.\\n5.\\nHARVESTING-AWARE POWER MANAGEMENT\\nWe now present a practical algorithm for power management that may be used for adapting the performance based on harvested energy information.\\nThis algorithm attempts to achieve energy neutral operation without using knowledge of the future energy availability and maximizes the achievable performance within that constraint.\\nThe harvesting-aware power management strategy consists of three parts.\\nThe first part is an instantiation of the energy generation model which tracks past energy input profiles and uses them to predict future energy availability.\\nThe second part computes the optimal duty cycles based on the predicted energy, and this step uses our computationally tractable method to solve the optimization problem.\\nThe third part consists of a method to dynamically adapt the duty cycle in response to the observed energy generation profile in real time.\\nThis step is required since the observed energy generation may deviate significantly from the predicted energy availability and energy neutral operation must be ensured with the actual energy received rather than the predicted values.\\n5.1.\\nEnergy Prediction Model\\nWe use a prediction model based on Exponentially Weighted Moving-Average (EWMA).\\nThe method is designed to exploit the diurnal cycle in solar energy but at the same time adapt to the seasonal variations.\\nA historical summary of the energy generation profile is maintained for this purpose.\\nWhile the storage data size is limited to a vector length of Nw values in order to minimize the memory overheads of the power management algorithm, the window size is effectively infinite as each value in the history window depends on all the observed data up to that instant.\\nThe window size is chosen to be 24 hours and each time slot is taken to be 30 minutes as the variation in generated power by the solar panel using this setting is less than 10% between each adjacent slots.\\nThis yields Nw = 48.\\nSmaller slot durations may be used at the expense of a higher Nw.\\nThe historical summary maintained is derived as follows.\\nOn a typical day, we expect the energy generation to be similar to the energy generation at the same time on the previous days.\\nThe value of energy generated in a particular slot is maintained as a weighted average of the energy received in the same time-slot during all observed days.\\nThe weights are exponential, resulting in decaying contribution from older\\ndata.\\nMore specifically, the historical average maintained for each slot is given by:\\nwhere a is the value of the weighting factor, xk is the observed value of energy generated in the slot, and xk \\u2212 1 is the previously stored historical average.\\nIn this model, the importance of each day relative to the previous one remains constant because the same weighting factor was used for all days.\\nThe average value derived for a slot is treated as an estimate of predicted energy value for the slot corresponding to the subsequent day.\\nThis method helps the historical average values adapt to the seasonal variations in energy received on different days.\\nOne of the parameters to be chosen in the above prediction method is the parameter a, which is a measure of rate of shift in energy pattern over time.\\nSince this parameter is affected by the characteristics of the energy and sensor node location, the system should have a training period during which this parameter will be determined.\\nTo determine a good value of a, we collected energy data over 72 days and compared the average error of the prediction method for various values of a.\\nThe error based on the different values of a is shown in Figure 3.\\nThis curve suggests an optimum value of a = 0.15 for minimum prediction error and this value will be used in the remainder of this paper.\\nFigure 3.\\nChoice of prediction parameter.\\n5.2.\\nLow-complexity Solution\\nThe energy values predicted for the next window of Nw slots are used to calculated the desired duty cycles for the next window, assuming the predicted values match the observed values in the future.\\nSince our objective is to develop a practical algorithm for embedded computing systems, we present a simplified method to solve the linear programming problem presented in Section 4.\\nTo this end, we define the sets S and D as follows:\\nThe two sets differ by the condition that whether the node operation can be sustained entirely from environmental energy.\\nIn the case that energy produced from the environment is not sufficient, battery will be discharged to supplement the remaining energy.\\nNext we sum up both sides of (6) over the entire Nw window and rewrite it with the new notation.\\nThe term on the left hand side is actually the battery energy used over the entire window of Nw slots, which can be set to 0 for energy neutral operation.\\nAfter some algebraic manipulation, this yields:\\nThe term on the left hand side is the total energy received in Nw slots.\\nThe first term on the right hand side can be interpreted as the total energy consumed during the D slots and the second term is the total energy consumed during the S slots.\\nWe can now replace three constraints (8), (9), and (10) in the original problem with (13), restating the optimization problem as follows:\\nThis form facilitates a low complexity solution that doesn't require a general linear programming solver.\\nSince our objective is to maximize the total system utility, it is preferable to set the duty cycle to Dmin for time slots where the utility per unit energy is the least.\\nOn the other hand, we would also like the time slots with the highest Ps to operate at Dmax because of better efficiency of using energy directly from the energy source.\\nCombining these two characteristics, we define the utility co-efficient for each slot i as follows:\\nwhere W (i) is a representation of how efficient the energy usage in a particular time slot i is.\\nA larger W (i) indicates more system utility per unit energy in slot i and vice versa.\\nThe algorithm starts by assuming D (i) = Dmin for i = {1...NW} because of the minimum duty cycle requirement, and computes the remaining system energy R by:\\nA negative R concludes that the optimization problem is infeasible, meaning the system cannot achieve energy neutrality even at the minimum duty cycle.\\nIn this case, the system designer is responsible for increasing the environment energy availability (e.g., by using larger solar panels).\\nIf R is positive, it means the system has excess energy that is not being used, and this may be allocated to increase the duty cycle beyond Dmin for some slots.\\nSince our objective is to maximize the total system utility, the most efficient way to allocate the excess energy is to assign duty cycle Dmax to the slots with the highest W (i).\\nSo, the coefficients W (i) are arranged in decreasing order and duty cycle Dmax is assigned to the slots beginning with the largest coefficients until the excess energy available, R (computed by (14) in every iteration), is insufficient to assign Dmax to another slot.\\nThe remaining energy, RLast, is used to increase the duty cycle to some value between Dmin and Dmax in the slot with the next lower coefficient.\\nDenoting this slot with index j, the duty cycle is given by:\\nThe above solution to the optimization problem requires only simple arithmetic calculations and one sorting step which can be easily implemented on an embedded platform, as opposed to implementing a general linear program solver.\\n5.3.\\nSlot-by-slot continual duty cycle adaptiation.\\nThe observed energy values may vary greatly from the predicted ones, such as due to the effect of clouds or other sudden changes.\\nIt is thus important to adapt the duty cycles calculated using the predicted values, to the actual energy measurements in real time to ensure energy neutrality.\\nDenote the initial duty cycle assignments for each time slot i computed using the predicted energy values as D (i) = {1,..., Nw}.\\nFirst we compute the difference between predicted power level Ps (i) and actual power level observed, Ps' (i) in every slot i. Then, the excess energy in slot i, denoted by X, can be obtained as follows:\\nALGORITHM 1 Pseudocode for the duty-cycle adaptation algorithm The upper term accounts for the energy difference when actual received energy is more than the power drawn by the load.\\nOn the other hand, if the energy received is less than Pc, we will need to account for the extra energy used from the battery by the load, which is a function of duty cycle used in time slot i and battery efficiency factor \\u03b7.\\nWhen more energy is received than predicted, X is positive and that excess energy is available for use in the subsequent solutes, while if X is negative, that energy must be compensated from subsequent slots.\\nCASE I: X <0.\\nIn this case, we want to reduce the duty cycles used in the future slots in order to make up for this shortfall of energy.\\nSince our objective function is to maximize the total system utility, we have to reduce the duty cycles for time slots with the smallest normalized utility coefficient, W (i).\\nThis is accomplished by first sorting the coefficient W (j), where j> i. in decreasing order, and then iteratively reducing Dj to Dmin until the total reduction in energy consumption is the same as X. CASE II: X> 0.\\nHere, we want to increase the duty cycles used in the future to utilize this excess energy we received in recent time slot.\\nIn contrast to Case I, the duty cycles of future time slots with highest utility coefficient W (i) should be increased first in order to maximize the total system utility.\\nSuppose the duty cycle is changed by d in slot j. Define a quantity R (j, d) as follows: The precise procedure to adapt the duty cycle to account for the above factors is presented in Algorithm 1.\\nThis calculation is performed at the end of every slot to set the duty cycle for the next slot.\\nWe claim that our duty cycling algorithm is energy neutral because an surplus of energy at the previous time slot will always translate to additional energy opportunity for future time slots, and vice versa.\\nThe claim may be violated in cases of severe energy shortages especially towards the end of window.\\nFor example, a large deficit in energy supply can't be restored if there is no future energy input until the end of the window.\\nIn such case, this offset will be carried over to the next window so that long term energy neutrality is still maintained.\\n6.\\nEVALUATION\\nOur adaptive duty cycling algorithm was evaluated using an actual solar energy profile measured using a sensor node called Heliomote, capable of harvesting solar energy [14].\\nThis platform not only tracks the generated energy but also the energy flow into and out of the battery to provide an accurate estimate of the stored energy.\\nThe energy harvesting platform was deployed in a residential area in Los Angeles from the beginning of June through the middle of August for a total of 72 days.\\nThe sensor node used is a Mica2 mote running at a fixed 40% duty cycle with an initially full battery.\\nBattery voltage and net current from the solar panels are sampled at a period of 10 seconds.\\nThe energy generation profile for that duration, measured by tracking the output current from the solar cell is shown in Figure 4, both on continuous and diurnal scales.\\nWe can observe that although the energy profile varies from day to day, it still exhibits a general pattern over several days.\\nDay Hour\\nFigure\\n4 Solar Energy Profile (Left: Continuous, Right: Diurnal)\\n6.1.\\nPrediction Model\\nWe first evaluate the performance of the prediction model, which is judged by the amount of absolute error it made between the predicted and actual energy profile.\\nFigure 5 shows the average error of each time slot in mA over the entire 72 days.\\nGenerally, the amount of error is larger during the day time because that's when the factor of weather can cause deviations in received energy, while the prediction made for night time is mostly correct.\\nFigure\\n5.\\nAverage Predictor Error in mA\\n6.2.\\nAdaptive Duty cycling algorithm\\nPrior methods to optimize performance while achieving energy neutral operation using harvested energy are scarce.\\nInstead, we compare the performance of our algorithm against two extremes: the theoretical optimal calculated assuming complete knowledge about future energy availability and a simple approach which attempts to achieve energy neutrality using a fixed duty cycle without accounting for battery inefficiency.\\nThe optimal duty cycles are calculated for each slot using the future knowledge of actual received energy for that slot.\\nFor the simple approach, the duty cycle is kept constant within each day and is\\ncomputed by taking the ratio of the predicted energy availability and the maximum usage, and this guarantees that the senor node will never deplete its battery running at this duty cycle.\\nWe then compare the performance of our algorithm to the two extremes with varying battery efficiency.\\nFigure 6 shows the results, using Dmax = 0.8 and Dmin = 0.3.\\nThe battery efficiency was varied from 0.5 to 1 on the x-axis and solar energy utilizations achieved by the three algorithms are shown on the y-axis.\\nIt shows the fraction of net received energy that is used to perform useful work rather than lost due to storage inefficiency.\\nAs can be seen from the figure, battery efficiency factor has great impact on the performance of the three different approaches.\\nThe three approaches all converges to 100% utilization if we have a perfect battery (\\u03b7 = 1), that is, energy is not lost by storing it into the batteries.\\nWhen battery inefficiency is taken into account, both the adaptive and optimal approach have much better solar energy utilization rate than the simple one.\\nAdditionally, the result also shows that our adaptive duty cycle algorithm performs extremely close to the optimal.\\nFigure 6.\\nDuty Cycles achieved with respect to \\u03b7\\nWe also compare the performance of our algorithm with different values of Dmin and Dmax for \\u03b7 = 0.7, which is typical of NiMH batteries.\\nThese results are shown in Table 1 as the percentage of energy saved by the optimal and adaptive approaches, and this is the energy which would normally be wasted in the simple approach.\\nThe figures and table indicate that our real time algorithm is able to achieve a performance very close to the optimal feasible.\\nIn addition, these results show that environmental energy harvesting with appropriate power management can achieve much better utilization of the environmental energy.\\nTABLE 1.\\nEnergy Saved by adaptive and optimal approach.\\n7.\\nCONCLUSIONS\\nWe discussed various issues in power management for systems powered using environmentally harvested energy.\\nSpecifically, we designed a method for optimizing performance subject to the constraint of energy neutral operation.\\nWe also derived a theoretically optimal bound on the performance and showed that our proposed algorithm operated very close to the optimal.\\nThe proposals were evaluated using real data collected using an energy harvesting sensor node deployed in an outdoor environment.\\nOur method has significant advantages over currently used methods which are based on a conservative estimate of duty cycle and can only provide sub-optimal performance.\\nHowever, this work is only the first step towards optimal solutions for energy neutral operation.\\nIt is designed for a specific power scaling method based on adapting the duty cycle.\\nSeveral other power scaling methods, such as DVS, submodule power switching and the use of multiple low power modes are also available.\\nIt is thus of interest to extend our methods to exploit these advanced capabilities.\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems\\nABSTRACT\\nIn this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.\\nUnlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.\\nWe show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.\\nEMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.\\nWe exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).\\nWe show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\\n1.\\nINTRODUCTION\\nPlanning and control constitutes a central research area in multiagent systems and artificial intelligence.\\nIn recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.\\nIn this framework, the planning and control problem is often\\naddressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.\\nWhile theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].\\nWe take an alternative view of planning in stochastic environments.\\nWe do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.\\nThe idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.\\nWe call this general planning framework Dynamics Based Control (DBC).\\nIn DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.\\nAs actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].\\nHere, optimality is measured in terms of probability of deviation magnitudes.\\nIn this paper, we present the structure of Dynamics Based Control.\\nWe show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.\\nEMT is an efficient instantiation of DBC.\\nTo evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to \\\"tag\\\" a moving target (quarry) whose position is not known with certainty.\\nExperimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent's position).\\nThe paper is organized as follows.\\nIn Section 2 we motivate DBC using area-sweeping problems, and discuss related work.\\nSection 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.\\nThis is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.\\nThat section also discusses the limitations of EMT-based control relative to the general DBC framework.\\nExperimental settings and results are then presented in Section 5.\\nSection 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work.\\n2.\\nMOTIVATION AND RELATED WORK\\nMany real-life scenarios naturally have a stochastic target dynamics specification, especially those domains where there exists no ultimate goal, but rather system behavior (with specific properties) that has to be continually supported.\\nFor example, security guards perform persistent sweeps of an area to detect any sign of intrusion.\\nCunning thieves will attempt to track these sweeps, and time their operation to key points of the guards' motion.\\nIt is thus advisable to make the guards' motion dynamics appear irregular and random.\\nRecent work by Paruchuri et al. [10] has addressed such randomization in the context of single-agent and distributed POMDPs.\\nThe goal in that work was to generate policies that provide a measure of action-selection randomization, while maintaining rewards within some acceptable levels.\\nOur focus differs from this work in that DBC does not optimize expected rewards--indeed we do not consider rewards at all--but instead maintains desired dynamics (including, but not limited to, randomization).\\nThe Game of Tag is another example of the applicability of the approach.\\nIt was introduced in the work by Pineau et al. [11].\\nThere are two agents that can move about an area, which is divided into a grid.\\nThe grid may have blocked cells (holes) into which no agent can move.\\nOne agent (the hunter) seeks to move into a cell occupied by the other (the quarry), such that they are co-located (this is a \\\"successful tag\\\").\\nThe quarry seeks to avoid the hunter agent, and is always aware of the hunter's position, but does not know how the hunter will behave, which opens up the possibility for a hunter to surprise the prey.\\nThe hunter knows the quarry's probabilistic law of motion, but does not know its current location.\\nTag is an instance of a family of area-sweeping (pursuit-evasion) problems.\\nIn [11], the hunter modeled the problem using a POMDP.\\nA reward function was defined, to reflect the desire to tag the quarry, and an action policy was computed to optimize the reward collected over time.\\nDue to the intractable complexity of determining the optimal policy, the action policy computed in that paper was essentially an approximation.\\nIn this paper, instead of formulating a reward function, we use EMT to solve the problem, by directly specifying the target dynamics.\\nIn fact, any search problem with randomized motion, the socalled class of area sweeping problems, can be described through specification of such target system dynamics.\\nDynamics Based Control provides a natural approach to solving these problems.\\n3.\\nDYNAMICS BASED CONTROL\\nThe specification of Dynamics Based Control (DBC) can be broken into three interacting levels: Environment Design Level, User Level, and Agent Level.\\n\\u2022 Environment Design Level is concerned with the formal specification and modeling of the environment.\\nFor example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation constant.\\n\\u2022 User Level in turn relies on the environment model produced by Environment Design to specify the target system dynamics it wishes to observe.\\nThe User Level also specifies the estimation or learning procedure for system dynamics, and the measure of deviation.\\nIn the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping.\\n\\u2022 Agent Level in turn combines the environment model from\\nthe Environment Design level, the dynamics estimation procedure, the deviation measure and the target dynamics specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the targeted specification.\\nAs we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the deviation measurements over time.\\nTo this end, we use a probability threshold--that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.\\nSpecific action selection then depends on system formalization.\\nOne possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].\\nThe other alternative would be to rely on the estimation procedure provided by the User Level--to utilize the Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a certain dynamics has been achieved.\\nNotice that this manipulation is not direct, but via the environment.\\nThus, for strong enough estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).\\nDBC levels can also have a back-flow of information (see Figure 1).\\nFor instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system behavior.\\nData would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.\\nFigure 1: Data flow of the DBC framework\\nExtending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning algorithm.\\nFor example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.\\nIn this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.\\nIn fact, the combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9].\\n3.1 DBC for Markovian Environments\\nFor a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.\\nNotice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1):\\n\\u2022 Environment Design level is to specify a tuple <S, A, T, O, \\u03a9, s0>, where:--S is the set of all possible environment states;--s0 is the initial state of the environment (which can also be viewed as a probability distribution over S);\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791\\nity that the environment will move from state s to state s' under action a;--O is the set of all possible observations.\\nThis is what the sensor input would look like for an outside observer;\\nobserve o given that the environment has moved from state s to state s' under action a.\\n\\u2022 User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of conditional probabilities F = {\\u03c4: S \\u00d7 A--* \\u03a0 (S)}.\\nThus specification of target dynamics can be expressed by q \\u2208 F, and the learning or tracking algorithm can be represented as a function L: O \\u00d7 (A \\u00d7 O) *--* F; that is, it maps sequences of observations and actions performed so far into an estimate \\u03c4 \\u2208 F of system dynamics.\\nThere are many possible variations available at the User Level to define divergence between system dynamics; several of them are:\\nNotice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have meaningful and important interpretations.\\nFor instance, KullbackLeibler divergence is an important tool of information theory [3] that allows one to measure the \\\"price\\\" of encoding an information source governed by q, while assuming that it is governed by p.\\nThe User Level also defines the threshold of dynamics deviation probability \\u03b8.\\n\\u2022 Agent Level is then faced with a problem of selecting a control signal function a * to satisfy a minimization problem as follows:\\nwhere d (\\u03c4a, q) is a random variable describing deviation of the dynamics estimate \\u03c4a, created by L under control signal a, from the ideal dynamics q. Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment Design Level.\\n3.2 DBC View of the State Space\\nIt is important to note the complementary view that DBC and POMDPs take on the state space of the environment.\\nPOMDPs regard state as a stationary snap-shot of the environment; whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.\\nThis can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism--the POMDP policy.\\nDBC concentrates on the underlying principle of state sequencing, the system dynamics.\\nDBC's target dynamics specification can use the environment's state space as a means to describe, discern, and preserve changes that occur within the system.\\nAs a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.\\nFor example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.\\nPOMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the goal--accumulating higher reward can be reached only by faster motion.\\nAlternatively, the state space could directly include the notion of speed.\\nFor POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and indirectly within reward accumulation.\\nNow, even if the reward function would encode more, and finer, details of the properties of motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.\\nOn the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.\\nIn this situation, any further constraints, e.g., smoothness of motion, speed limits in different locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm.\\n4.\\nEMT-BASED CONTROL AS A DBC\\nRecently, a control algorithm was introduced called EMT-based Control [13], which instantiates the DBC framework.\\nAlthough it provides an approximate greedy solution in the DBC sense, initial experiments using EMT-based control have been encouraging [14, 15].\\nEMT-based control is based on the Markovian environment definition, as in the case with POMDPs, but its User and Agent Levels are of the Markovian DBC type of optimality.\\n\\u2022 User Level of EMT-based control defines a limited-case target system dynamics independent of action:\\nIt then utilizes the Kullback-Leibler divergence measure to compose a momentary system dynamics estimator--the Extended Markov Tracking (EMT) algorithm.\\nThe algorithm keeps a system dynamics estimate \\u03c4t EMT that is capable of explaining recent change in an auxiliary Bayesian system state estimator from pt-1 to pt, and updates it conservatively using Kullback-Leibler divergence.\\nSince \\u03c4t EMT and pt-1, t are respectively the conditional and marginal probabilities over the system's state space, \\\"explanation\\\" simply means that\\nand the dynamics estimate update is performed by solving a--Trace distance or L1 distance between two distributions p and q defined by\\n792 The Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07)\\nTable 1: Structure of POMDP vs. Dynamics-Based Control in Markovian Environment\\nperformance with respect to a given target.\\nIf these preference vectors are normalized, they can be combined into a single unified preference.\\nThis requires replacement of standard EMT-based action selection by the algorithm below [15]:\\n\\u2022 Given:\\n-- a set of target dynamics {qk} Kk = 1,--vector of weights w (k) \\u2022 Select action as follows \\u2022 Agent Level in EMT-based control is suboptimal with respect to DBC (though it remains within the DBC framework), performing greedy action selection based on prediction of EMT's reaction.\\nThe prediction is based on the environment model provided by the Environment Design level, so that if we denote by Ta the environment's transition function limited to action a, and pt \\u2212 1 is the auxiliary Bayesian system state estimator, then the EMT-based control choice is described by\\nNote that this follows the Markovian DBC framework precisely: the rewarding optimality of POMDPs is discarded, and in its place a dynamics estimator (EMT in this case) is manipulated via action effects on the environment to produce an estimate close to the specified target system dynamics.\\nYet as we mentioned, naive EMTbased control is suboptimal in the DBC sense, and has several additional limitations that do not exist in the general DBC framework (discussed in Section 4.2).\\n4.1 Multi-Target EMT\\nAt times, there may exist several behavioral preferences.\\nFor example, in the case of museum guards, some art items are more heavily guarded, requiring that the guards stay more often in their close vicinity.\\nOn the other hand, no corner of the museum is to be left unchecked, which demands constant motion.\\nSuccessful museum security would demand that the guards adhere to, and balance, both of these behaviors.\\nFor EMT-based control, this would mean facing several tactical targets {qk} Kk = 1, and the question becomes how to merge and balance them.\\nA balancing mechanism can be applied to resolve this issue.\\nNote that EMT-based control, while selecting an action, creates a preference vector over the set of actions based on their predicted--For each action a \\u2208 A predict the future state distribution \\u00af pat +1 = Ta \\u2217 pt;--For each action, compute\\nThe weights vector w ~ = (w1,..., wK) allows the additional \\\"tuning of importance\\\" among target dynamics without the need to redesign the targets themselves.\\nThis balancing method is also seamlessly integrated into the EMT-based control flow of operation.\\n4.2 EMT-based Control Limitations\\nEMT-based control is a sub-optimal (in the DBC sense) representative of the DBC structure.\\nIt limits the User by forcing EMT to be its dynamic tracking algorithm, and replaces Agent optimization by greedy action selection.\\nThis kind of combination, however, is common for on-line algorithms.\\nAlthough further development of EMT-based controllers is necessary, evidence so far suggests that even the simplest form of the algorithm possesses a great deal of power, and displays trends that are optimal in the DBC sense of the word.\\nThere are two further, EMT-specific, limitations to EMT-based control that are evident at this point.\\nBoth already have partial solutions and are subjects of ongoing research.\\nThe first limitation is the problem of negative preference.\\nIn the POMDP framework for example, this is captured simply, through\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793\\nthe appearance of values with different signs within the reward structure.\\nFor EMT-based control, however, negative preference means that one would like to avoid a certain distribution over system development sequences; EMT-based control, however, concentrates on getting as close as possible to a distribution.\\nAvoidance is thus unnatural in native EMT-based control.\\nThe second limitation comes from the fact that standard environment modeling can create pure sensory actions--actions that do not change the state of the world, and differ only in the way observations are received and the quality of observations received.\\nSince the world state does not change, EMT-based control would not be able to differentiate between different sensory actions.\\nNotice that both of these limitations of EMT-based control are absent from the general DBC framework, since it may have a tracking algorithm capable of considering pure sensory actions and, unlike Kullback-Leibler divergence, a distribution deviation measure that is capable of dealing with negative preference.\\n5.\\nEMT PLAYING TAG\\nThe Game of Tag was first introduced in [11].\\nIt is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.\\nAn example domain is shown in Figure 2.\\nFigure 2: Tag domain; an agent (A) attempts to seek and capture a quarry (Q)\\nThe Game of Tag extremely limits the agent's perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.\\nIn the classical version of the game, co-location leads to a special observation, and the ` Tag' action can be performed.\\nWe slightly modify this setting: the moment that both agents occupy the same cell, the game ends.\\nAs a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, North, South, East, and West.\\nThese form a formal space of actions within a Markovian environment.\\nThe state space of the formal Markovian environment is described by the cross-product of the agent and quarry's positions.\\nFor Figure 2, it would be S = {s0,..., s23} \\u00d7 {s0,..., s23}.\\nThe effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the motion of the quarry.\\nWith probability q01 it stays put, and with probability 1 \\u2212 q0 it moves to an adjacent cell further away from the 1In our experiments this was taken to be q0 = 0.2.\\nagent.\\nSo for the instance shown in Figure 2 and q0 = 0.1:\\nAlthough the evasive behavior of the quarry is known to the agent, the quarry's position is not.\\nThe only sensory information available to the agent is its own location.\\nWe use EMT and directly specify the target dynamics.\\nFor the Game of Tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.\\nThis results in the following three target dynamics:\\nNote that none of the above targets are directly achievable; for instance, if Qt = s9 and At = s11, there is no action that can move the agent to At +1 = s9 as required by the Tcatch target dynamics.\\nWe ran several experiments to evaluate EMT performance in the Tag Game.\\nThree configurations of the domain shown in Figure 3 were used, each posing a different challenge to the agent due to partial observability.\\nIn each setting, a set of 1000 runs was performed with a time limit of 100 steps.\\nIn every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarry's initial position was uniformly distributed over the entire domain cell space.\\nWe also used two variations of the environment observability function.\\nIn the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.\\nIn the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunter's location.\\nThe second version in fact utilized and expressed the fact that once hunter and quarry occupy the same cell the game ends.\\nThe results of these experiments are shown in Table 2.\\nBalancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], EMT produced stable performance in all three domains.\\nAlthough direct comparisons are difficult to make, the EMT performance displayed notable efficiency vis - ` a-vis the POMDP approach.\\nIn spite of a simple and inefficient Matlab implementation of the EMT algorithm, the decision time for any given step averaged significantly below 1 second in all experiments.\\nFor the irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.\\nThat is, over 4 \\u00d7 104 online steps took an order of magnitude less time than the offline computation of POMDP policy in [11].\\nThe significance of this differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of EMT would allow it to maintain its performance time, while the POMDP policy would need to be recomputed, requiring yet again a large overhead of computation.\\nWe also tested the behavior cell frequency entropy, empirical measures from trial data.\\nAs Figure 4 and Figure 5 show, empir\\nFigure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor\\nTable 2: Performance of the EMT-based solution in three Tag Game domains and two observability models: I) omniposition quarry, II) quarry is not at hunter's position\\nical entropy grows with the length of interaction.\\nFor runs where the quarry was not captured immediately, the entropy reaches between 0.85 and 0.952 for different runs and scenarios.\\nAs the agent actively seeks the quarry, the entropy never reaches its maximum.\\nOne characteristic of the entropy graph for the open arena scenario particularly caught our attention in the case of the omniposition quarry observation model.\\nNear the maximum limit of trial length (100 steps), entropy suddenly dropped.\\nFurther analysis of the data showed that under certain circumstances, a fluctuating behavior occurs in which the agent faces equally viable versions of quarry-following behavior.\\nSince the EMT algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.\\nIt is essentially attempting to simultaneously follow several courses of action, all of which are consistent with the target dynamics.\\nThis behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action--essentially contributing to tie-breaking among them.\\n6.\\nDISCUSSION\\nThe design of the EMT solution for the Tag Game exposes the core difference in approach to planning and control between EMT or DBC, on the one hand, and the more familiar POMDP approach, on the other.\\nPOMDP defines a reward structure to optimize, and influences system dynamics indirectly through that optimization.\\nEMT discards any reward scheme, and instead measures and influences system dynamics directly.\\n2Entropy was calculated using log base equal to the number of possible locations within the domain; this properly scales entropy expression into the range [0, 1] for all domains.\\nThus for the Tag Game, we did not search for a reward function that would encode and express our preference over the agent's behavior, but rather directly set three (heuristic) behavior preferences as the basis for target dynamics to be maintained.\\nExperimental data shows that these targets need not be directly achievable via the agent's actions.\\nHowever, the ratio between EMT performance and achievability of target dynamics remains to be explored.\\nThe tag game experiment data also revealed the different emphasis DBC and POMDPs place on the formulation of an environment state space.\\nPOMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.\\nDBC, on the other hand, has two sources of sequencing specification: through the properties of an action selection procedure, and through direct specification within the target dynamics.\\nThe importance of the second source was underlined by the Tag Game experiment data, in which the greedy EMT algorithm, applied to a POMDP-type state space specification, failed, since target description over such a state space was incapable of encoding the necessary behavior tendencies, e.g., tie-breaking and commitment to directed motion.\\nThe structural differences between DBC (and EMT in particular), and POMDPs, prohibits direct performance comparison, and places them on complementary tracks, each within a suitable niche.\\nFor instance, POMDPs could be seen as a much more natural formulation of economic sequential decision-making problems, while EMT is a better fit for continual demand for stochastic change, as happens in many robotic or embodied-agent problems.\\nThe complementary properties of POMDPs and EMT can be further exploited.\\nThere is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.\\nDBC can be an effective partner in such a hybrid solution.\\nFor instance, POMDPs have prohibitively large off-line time requirements for policy computation, but can be readily used in simpler settings to expose beneficial behavioral trends; this can serve as a form of target dynamics that are provided to EMT in a larger domain for on-line operation.\\n7.\\nCONCLUSIONS AND FUTURE WORK\\nIn this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.\\nDBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.\\nOptimality of DBC plans of action are measured\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795\\nFigure 4: Observation Model I: Omniposition quarry.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nFigure 5: Observation Model II: quarry not observed at hunter's position.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nwith respect to the deviation of actual system dynamics from the target dynamics.\\nWe show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.\\nIn fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.\\nSince EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.\\nAs enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.\\nThis prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).\\nHowever, DBC in general has no such limitations, and readily enables the formulation of evasion games.\\nIn future work, we intend to proceed with the development of dynamics-based controllers for these problems.\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions\\nABSTRACT\\nThe winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices.\\nWhile this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs).\\nFormally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph.\\nNote that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness.\\nIn fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem.\\nIn this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3.\\nMotivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions.\\nWe show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here.\\nIndeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width.\\nEven more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\\n1.\\nINTRODUCTION\\nCombinatorial auctions.\\nCombinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items.\\nThis is desirable when a bidder's valuation of a bundle of items is not equal to the sum of her valuations of the individual items.\\nThis framework is currently used to regulate agents' interactions in several application domains (cf., e.g., [21]) such as, electricity markets [13], bandwidth auctions [14], and transportation exchanges [18].\\nFormally, a combinatorial auction is a pair (Z, B), where Z = {I1,..., Im} is the set of items the auctioneer has to sell, and B = {B1,..., Bn} is the set of bids from the buyers interested in the items in Z. Each bid Bi has the form (item (Bi), pay (Bi)), where pay (Bi) is a rational number denoting the price a buyer offers for the items in item (Bi) C Z.\\nAn outcome for (Z, B) is a subset b of B such that item (Bi) n item (Bj) = 0, for each pair Bi and Bj of bids in b with i = ~ j.\\nThe winner determination problem.\\nA crucial problem for combinatorial auctions is to determine the outcome b \\u2217 that maximizes the sum of the accepted bid prices (i.e.,\\nBi \\u2208 b \\u2217 pay (Bi)) over all the possible outcomes.\\nThis problem, called winner determination problem (e.g., [11]), is known to be intractable, actually NP-hard [17], and even not approximable in polynomial time unless NP = ZPP [19].\\nHence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions (e.g., [20, 5, 2, 8, 23]) and to identify classes of instances where solving the winner determination problem is feasible in polynomial time (e.g., [15, 22, 12, 21]).\\nIn fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions.\\nItem graphs.\\nCurrently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any\\nFigure 1: Example MaxWSP problem: (a) Hypergraph H (To, go), and a packing h for it; (b) Primal graph for H (To, go); and, (c, d) Two item graphs for H (To, go).\\nbid, the items occurring in it induce a connected subgraph.\\nIndeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure [3]--formally bounded treewidth [16].\\nTo have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H (T, g) such that its set of nodes N (H (T, g)) coincides with set of items I, and where its edges E (H (T, g)) are precisely the bids of the buyers {item (Bi) | Bi \\u2208 B}.\\nA special item graph for ~ I, B ~ is the primal graph of H (T, g), denoted by G (H (T, g)), which contains an edge between any pair of nodes in some hyperedge of H (T, g).\\nThen, any item graph for H (T, g) can be viewed as a simplification of G (H (T, g)) obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge.\\nEXAMPLE 1.\\nThe hypergraph H (To, go) reported in Figure 1.\\n(a) is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = {I1,..., I5}, and item (Bi) = hi, for each 1 \\u2264 i \\u2264 3.\\nThe primal graph for H (To, go) is reported in\\nFigure 1.\\n(b), while two example item graphs are reported in Figure 1.\\n(c) and (d), where edges required for maintaining\\nthe connectivity for h1 are depicted in bold.\\n<\\nOpen Problem: Computing structured item\\ngraphs efficiently.\\nThe above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined.\\nHowever, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain (constant) treewidth exists, and if so, how to compute such a structured item graph efficiently.\\nPolynomial time algorithms to find the \\\"best\\\" simplification of the primal graph were so far only known for the cases where the item graph to be constructed is a line [10], a cycle [4], or a tree [3], but it was an important open problem (cf. [3]) whether it is tractable to check if for a combinatorial auction, an item graph of treewidth bounded by a fixed natural number k exists and can be constructed in polynomial time, if so.\\nWeighted Set Packing.\\nLet us note that the hypergraph representation H (T, g) of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear (e.g., [17]).\\nFormally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h' \\u2208 h with h = ~ h', it holds that h \\u2229 h' = \\u2205.\\nLetting w be a weighting function for H, i.e., a polynomially-time computable function from E (H) to rational numbers, the weight of a packing h is the rational number w (h) = EhCh w (h), where w ({}) = 0.\\nThen, the maximum-weighted set packing problem for H w.r.t. w, denoted by MaxWSP (H, w), is the problem of finding a packing for H having the maximum weight over all the packings for H. To see that MaxWSP is just a different formulation for the winner determination problem, given a combinatorial auction ~ I, B ~, it is sufficient to define the weighting function w (T, g) (item (Bi)) = pay (Bi).\\nThen, the set of the solutions for the weighted set packing problem for H (T, g) w.r.t. w (T, g) coincides with the set of the solutions for the winner determination problem on ~ I, B ~.\\nEXAMPLE 2.\\nConsider again the hypergraph H (To, go) reported in Figure 1.\\n(a).\\nAn example packing for H (To, go) is h = {h1}, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1.\\nBy assuming that bids B1, B2, and B3 are such that pay (B1) = pay (B2) = pay (B3), the packing h is not a solution for the problem MaxWSP (H (To, go), w (To, go)).\\nIndeed, the packing\\nContributions\\nThe primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable.\\nTowards this aim, we first study structured item graphs and solve the open problem in [3].\\nThe result is very bad news: \\u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3.\\nMore formally, letting C (ig, k) denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph (associated with a combinatorial auction problem) belongs to C (ig, 3) is NP-complete.\\nIn the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem.\\nOur investigations, this time, led to very good news which are summarized below:\\n\\u25ba For a hypergraph H, its dual H \\u00af = (V, E) is such that nodes in V are in one-to-one correspondence with hyperedges in H, and for each node x \\u2208 N (H), {h | x \\u2208 h \\u2227 h \\u2208\\nE (H)} is in E.\\nWe show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width [7] bounded by k (short: class C (hw, k) of hypergraphs).\\nNote that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \\u00af instead of the auction hypergraph H.\\nIn fact, we can show that MaxWSP remains NP-hard even when H is acyclic (i.e., when it has hypertree width 1), even when each node is contained in 3 hyperedges at most.\\n\\u25ba For some relevant special classes of hypergraphs in C (hw, k), we design a higly-parallelizeable algorithm for MaxWSP.\\nSpecifically, if the weighting functions can be computed in logarithmic space and weights are polynomial (e.g., when all the hyperegdes have unitary weights and one is interested in finding the packing with the maximum number of edges), we show that MaxWSP can be solved by a LOGCFL algorithm.\\nRecall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P (see, e.g., [9]).\\n\\u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs.\\nTo the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs.\\nIn fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach.\\nIntuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition.\\n\\u25ba The proof of the above results give us some interesting insight into the notion of structured item graph.\\nIndeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions.\\nA game-characterization for the notion of strict hypertree width is also proposed, which specializes the Robber and Marshals game in [6] (proposed to characterize the hypertree width), and which makes it clear the further requirements on hypertree decompositions.\\nThe rest of the paper is organized as follows.\\nSection 2 discusses the intractability of structured item graphs.\\nSection 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable.\\nThe comparison between the classes C (ig, k) and C (hw, k) is discussed in Section 4.\\nFinally, in Section 5 we draw our conclusions by also outlining directions for further research.\\n2.\\nCOMPLEXITY OF STRUCTURED ITEM GRAPHS\\nLet H be a hypergraph.\\nA graph G = (V, E) is an item graph for H if V = N (H) and, for each h \\u2208 E (H), the subgraph of G induced over the nodes in h is connected.\\nAn important class of item graphs is that of structured item graphs, i.e., of those item graphs having bounded treewidth as formalized below.\\nA tree decomposition [16] of a graph G = (V, E) is a pair (T, \\u03c7), where T = (N, F) is a tree, and \\u03c7 is a labelling function assigning to each vertex p \\u2208 N a set of vertices \\u03c7 (p) C _ V, such that the following conditions are satisfied: (1) for each vertex b of G, there exists p \\u2208 N such that b \\u2208 \\u03c7 (p); (2) for each edge {b, d} \\u2208 E, there exists p \\u2208 N such that {b, d} C _ \\u03c7 (p); (3) for each vertex b of G, the set {p \\u2208 N | b \\u2208 \\u03c7 (p)} induces a connected subtree of T.\\nThe width of (T, \\u03c7) is the number maxpEN | \\u03c7 (p) \\u2212 1 |.\\nThe treewidth of G, denoted by tw (G), is the minimum width over all its tree decompositions.\\nThe winner determination problem can be solved in polynomial time on item graphs having bounded treewidth [3].\\nTHEOREM 1 (CF. [3]).\\nAssume a k-width tree decomposition (T, \\u03c7) of an item graph for H is given.\\nThen, MaxWSP (H, w) can be solved in time O (| T | 2 \\u00d7 (| E (H) | +1) k +1).\\nMany item graphs can be associated with a hypergraph.\\nAs an example, observe that the item graph in Figure 1.\\n(c) has treewidth 1, while Figure 1.\\n(d) reports an item graph whose treewidth is 2.\\nIndeed, it was an open question whether for a given constant k it can be checked in polynomial time if an item graph of treewidth k exists, and if so, whether such an item graph can be efficiently computed.\\nLet C (ig, k) denote the class of all the hypergraphs having an item graph G such that tw (G) <k.\\nThe main result of this section is to show that the class C (ig, k) is hard to recognize.\\nThe proof of this result relies on an elaborate reduction from the Hamiltonian path problem HP (s, t) of deciding whether there is an Hamiltonian path from a node s to a node t in a directed graph G = (N, E).\\nTo help the intuition, we report here a high-level overview of the main ingredients exploited in the proof1.\\nThe general idea it to build a hypergraph HG such that there is an item graph G' for HG with tw (G') <3 if and only if HP (s, t) over G has a solution.\\nFirst, we discuss the way HG is constructed.\\nSee Figure 2.\\n(a) for an illustration, where the graph G consists of the nodes s, x, y, and t, and the set of its edges is {e1 = (s, x), e2 = (x, y), e3 = (x, t), e4 = (y, t)}.\\nFrom G to HG.\\nLet G = (N, E) be a directed graph.\\nThen, the set of the nodes in HG is such that: for each x \\u2208 N, N (HG) contains the nodes bsx, btx, b' x, b' ' x, bdx; for each e = (x, y) \\u2208 E, N (HG) contains the nodes ns' x, ns' ' x, nt' y, nt' ' y, nsex and ntey.\\nNo other node is in N (HG).\\nHyperedges in HG are of three kinds:\\n1) for each x \\u2208 N, E (HG) contains the hyperedges: \\u2022 Sx = {bsx} \\u222a {nsex | e = (x, y) \\u2208 E}; \\u2022 Tx = {btx} \\u222a {ntex | e = (z, x) \\u2208 E}; \\u2022 A1x = {bdx, b' x}, A2x = {bdx, b' ' x}, and A3x = {b' x, b' ' x}--notice that these hyperedges induce a clique on the nodes {b' x, b' ' x, bdx};\\nFigure 2: Proof of Theorem 2: (a) from G to HG--hyperedges in 1) and 2) are reported only; (b) a skeleton for a tree decomposition TD for HG.\\n\\u2022 SA1x = {bsx, b' x}, SA2x = {bsx, b' ' x}, SA3x = {bsx, bdx}--notice that these hyperedges plus A1x, A2x, and A3x induce a clique on the nodes {bsx, b' x, b' ' x, bdx}; \\u2022 TA1x = {btx, b' x}, TA2x = {btx, b' ' x}, and TA3x = {btx, bdx}--notice that these hyperedges plus A1x, A2x, and A3x induce a clique on the nodes {btx, b' x, b' ' x, bdx}; 2) for each e = (x, y) \\u2208 E, E (HG) contains the hyperedges: \\u2022 SHx = {ns' x, ns' ' x}; \\u2022 THy = {nt' y, nt' ' y}; \\u2022 SE'e = {ns' x, nsex} and SE' ' e = {ns' ' x, nsex}--notice that these two hyperedges plus SHx induce a clique on the nodes {ns' x, ns' ' x, nsxe}; \\u2022 TE'e = {nt' y, ntey} and TE' ' e = {nt' ' y, ntey}--notice that these two hyperedges plus THy induce a clique on the nodes {nt' y, nt' ' y, ntey}.\\nNotice that each of the above hyperedges but those of the form Sx and Tx contains exactly two nodes.\\nAs an example of the hyperedges of kind 1) and 2), the reader may refer to the example construction reported in Figure 2.\\n(a), and notice, for instance, that Sx = {bsx, nse2 x, nse3\\n3) finally, we denote by DG the set containing the hyperedges in E (HG) of the third kind.\\nIn the reduction we are exploiting, DG can be an arbitrary set of hyperedges satisfying the four conditions that are discussed below.\\nLet PG be the set of the following | PG | \\u2264 | N | + 3 \\u00d7 | E | pairs: PG = {(b' x, b' ' x) | x \\u2208 N} \\u222a {(ns' x, ns' ' x), (nt' y, nt' ' y), (nsex, nte y) | e = (x, y) \\u2208 E}.\\nAlso, let I (v) denote the set {h \\u2208 E (H) | v \\u2208 h} of the hyperedges of H that are touched by v; and, for a set V \\u2286 N (H), let I (V) = UvcV I (v).\\nThen, DG has to be a set such that:\\n(c1) \\u2200 (\\u03b1,,3) \\u2208 PG, I (\\u03b1) \\u2229 I (,3) \\u2229 DG = \\u2205; (c2) \\u2200 (\\u03b1,,3) \\u2208 PG, I (\\u03b1) \\u222a I (,3) \\u2287 DG; (c3) \\u2200 \\u03b1 \\u2208 N such that ~ \\u2203,3 \\u2208 N with (\\u03b1,,3) \\u2208 PG or (,3, \\u03b1) \\u2208 PG, it holds: I (\\u03b1) \\u2229 DG = \\u2205; and, (c4) \\u2200 S \\u2286 N such that | S | \\u2264 3 and where ~ \\u2203 \\u03b1,,3 \\u2208 S with (\\u03b1,,3) \\u2208 PG, it is the case that: I (S) \\u2287 ~ DG.\\nIntuitively, the set DG is such that each of its hyperedges is touched by exactly one of the two nodes in every pair\\nof PG--cf. (c1) and (c2).\\nMoreover, hyperedges in DG touch only vertices included in at least a pair of PG--cf. (c3); and, any triple of nodes is not capable of touching all the elements of DG if none of the pairs that can be built from it belongs to PG--cf. (c4).\\nThe reader may now ask whether a set DG exists at all satisfying (c1), (c2), (c3) and (c4).\\nIn the following lemma, we positively answer this question and refer the reader to its proof for an example construction.\\nLEMMA 1.\\nA set DG, with | DG | = 2 \\u00d7 | PG | + 2, satisfying conditions (c1), (c2), (c3), and (c4) can be built in time O (| PG | 2).\\nKey Ingredients.\\nWe are now in the position of presenting an overview of the key ingredients of the proof.\\nLet G' be an arbitrary item graph for HG, and let TD = ~ T, \\u03c7 ~ be a 3-width tree decomposition of G' (note that, because of the cliques, e.g., on the nodes {bsx, b' x, b' ' x, bdx}, any item graph for HG has treewidth 3 at least).\\nThere are three basic observations serving the purpose of proving the correctness of the reduction.\\n\\\"Blocks\\\" of TD: First, we observe that TD must contain some special kinds of vertex.\\nSpecifically, for each node x \\u2208 N, TD contains a vertex bs (x) such that\\nIntuitively, these vertices are required to cover the cliques of HG associated with the hyperedges of kind 1) and 2).\\nEach of these vertices plays a specific role in the reduction.\\nIndeed, each directed edge e = (x, y) \\u2208 E is encoded in TD by means of the vertices: ns (x, e), representing precisely that e starts from x; and, nt (y, e), representing precisely that e terminates into y. Also, each node x \\u2208 N is encoded in TD be means of the vertices: bs (x), representing the starting point of edges originating from x; and, bt (x), representing the terminating point of edges ending into x.\\nAs an example, Figure 2.\\n(b) reports the \\\"skeleton\\\" of a tree decomposition TD.\\nThe reader may notice in it the blocks defined above and how they are related with the hypergraph HG in Figure 2.\\n(a)--other blocks in it (of the form w (x, y)) are defined next.\\nConnectedness between blocks,\\nand uniqueness of the connections: The second crucial observation is that in the path connecting a vertex of the form bs (x) (resp., bt (y)) with a vertex of the form ns (x, e) (resp., nt (y, e)) there is one special vertex of the form w (x, y) such that: \\u03c7 (w (x, y)) \\u2287 {nseix, ntei y}, for some edge e' = (x, y) \\u2208 E. Guaranteeing the existence of one such vertex is precisely the role played by the hyperedges in DG.\\nThe arguments for the proof are as follows.\\nFirst, we observe that I (\\u03c7 (bs (x))) \\u2229 I (\\u03c7 (ns (x, e))) \\u2287 DG \\u222a {Sx} and I (\\u03c7 (bt (y))) \\u2229 I (\\u03c7 (nt (y, e))) \\u2287 DG \\u222a {Ty}.\\nThen, we show a property stating that for a pair of consecutive vertices p and q in the path connecting bs (x) and ns (x, e) (resp., bt (y) and nt (y, e)), I (\\u03c7 (p) \\u2229 \\u03c7 (q)) \\u2287 I (\\u03c7 (bs (x))) \\u2229 I (\\u03c7 (ns (x, e))) (resp., I (\\u03c7 (p) \\u2229 \\u03c7 (q)) \\u2287\\nBased on this observation, and by exploiting the properties of the hyperedges in DG, it is not difficult to show that any pair of consecutive vertices p and q must share two nodes of HG forming a pair in PG, and must both touch Sx (resp., Ty).\\nWhen the treewidth of G' is 3, we can conclude that a vertex, say w (x, y), in this path is such that \\u03c7 (w (x, y)) \\u2287 {nseix, ntei y}, for some edge e' = (x, y) \\u2208 E--to this end, note that \\u2208 Ty, and I (\\u03c7 (w (x, y))) \\u2287 DG.\\nIn particular, w (x, y) is the only kind of vertex satisfying these conditions, i.e., in the path there is no further vertex of the form w (x, z), for z = ~ y (resp., w (z, y), for z = ~ x).\\nTo help the intuition, we observe that having a vertex of the form w (x, y) in TD corresponds to the selection of an edge from node x to node y in the Hamiltonian path.\\nIn fact, given the uniqueness of these vertices selected for ensuring the connectivity, a one-to-one correspondence can be established between the existence of a Hamiltonian path for G and the vertices of the form w (x, y).\\nAs an example, in Figure 2.\\n(b), the vertices of the form w (s, x), w (x, y), and w (y, t) are in TD, and GTD shows the corresponding Hamiltonian path.\\nUnused blocks: Finally, the third ingredient of the proof is the observation that if a vertex of the form w (x, y), for an edge e' = (x, y) \\u2208 E is not in TD (i.e., if the edge (x, y) does not belong to the Hamiltonian path), then the corresponding block ns (x, ei) (resp., nt (y, ei)) can be arbitrarily appended in the subtree rooted at the block ns (x, e) (resp., nt (y, e)), where e is the edge of the form e = (x, z) (resp., e = (z, y)) such that w (x, z) (resp., w (z, y)) is in TD.\\nE.g., Figure 2.\\n(a) shows w (x, t), which is not used in TD, and Figure 2.\\n(b) shows how the blocks ns (x, e3) and nt (t, e3) can be arranged in TD for ensuring the connectedness condition.\\n3.\\nTRACTABLE CASES VIA HYPERTREE DECOMPOSITIONS\\nSince constructing structured item graphs is intractable, it is relevant to assess whether other structural restrictions can be used to single out classes of tractable MaxWSP instances.\\nTo this end, we focus on the notion of hypertree decomposition [7], which is a natural generalization of hypergraph acyclicity and which has been profitably used in other domains, e.g, constraint satisfaction and database query evaluation, to identify tractability islands for NP-hard problems.\\nA hypertree for a hypergraph H is a triple ~ T, \\u03c7, \\u03bb ~, where T = (N, E) is a rooted tree, and \\u03c7 and \\u03bb are labelling functions which associate each vertex p \\u2208 N with two sets \\u03c7 (p) \\u2286 N (H) and \\u03bb (p) \\u2286 E (H).\\nIf T' = (N', E') is a subtree of T, we define \\u03c7 (T') = UvCNi \\u03c7 (v).\\nWe denote the set of vertices N of T by vertices (T).\\nMoreover, for any p \\u2208 N, Tp denotes the subtree of T rooted at p.\\nDEFINITION 1.\\nA hypertree decomposition of a hypergraph H is a hypertree HD = ~ T, \\u03c7, \\u03bb ~ for H which satisfies all the following conditions: 1.\\nfor each edge h \\u2208 E (H), there exists p \\u2208 vertices (T) such that h \\u2286 \\u03c7 (p) (we say that p covers h);\\nFigure 3: Example MaxWSP problem: (a) Hypergraph H1; (b) Hypergraph \\u00af H1; (b) A 2-width hypertree decomposition of \\u00af H1.\\n2.\\nfor each node Y \\u2208 N (H), the set {p \\u2208 vertices (T) | Y \\u2208 \\u03c7 (p)} induces a (connected) subtree of T; 3.\\nfor each p \\u2208 vertices (T), \\u03c7 (p) C _ N (\\u03bb (p)); 4.\\nfor each p \\u2208 vertices (T), N (\\u03bb (p)) \\u2229 \\u03c7 (Tp) C _ \\u03c7 (p).\\nThe width of a hypertree decomposition (T, \\u03c7, \\u03bb) is maxp \\u2208 vertices (T) | \\u03bb (p) |.\\nThe HYPERTREE width hw (H) of H is the minimum width over all its hypertree decompositions.\\nA hypergraph H is acyclic if hw (H) = 1.\\n\\u2737 EXAMPLE 3.\\nThe hypergraph H ~ I0, B0 ~ reported in Figure 1.\\n(a) is an example acyclic hypergraph.\\nInstead, both the hypergraphs H1 and \\u00af H1 shown in Figure 3.\\n(a) and Figure 3.\\n(b), respectively, are not acyclic since their hypertree width is 2.\\nA 2-width hypertree decomposition for \\u00af H1 is reported in Figure 3.\\n(c).\\nIn particular, observe that H1 has been obtained by adding the two hyperedges h4 and h5 to H ~ I0, B0 ~ to model, for instance, that two new bids, B4 and B5, respectively, have been proposed to the auctioneer.\\n\\u2701 In the following, rather than working on the hypergraph H associated with a MaxWSP problem, we shall deal with its dual \\u00af H, i.e., with the hypergraph such that its nodes are in one-to-one correspondence with the hyperedges of H, and where for each node x \\u2208 N (H), {h | x \\u2208 h \\u2227 h \\u2208 E (H)} is in E (\\u00af H).\\nAs an example, the reader may want to check again the hypergraph H1 in Figure 3.\\n(a) and notice that the hypergraph in Figure 3.\\n(b) is in fact its dual.\\nThe rationale for this choice is that issuing restrictions on the original hypergraph is a guarantee for the tractability only in very simple scenarios.\\n3.1 Hypertree Decomposition on the Dual Hypergraph and Tractable Packing Problems\\nFor a fixed constant k, let C (hw, k) denote the class of all the hypergraphs whose dual hypergraphs have hypertree width bounded by k.\\nThe maximum weighted-set packing problem can be solved in polynomial time on the class C (hw, k) by means of the algorithm ComputeSetPackingk, shown in Figure 4.\\nThe algorithm receives in input a hypergraph H, a weighting function w, and a k-width hypertree decomposition HD = (T = (N, E), \\u03c7, \\u03bb) of \\u00af H. For each vertex v \\u2208 N, let Hv be the hypergraph whose set of nodes N (Hv) C _ N (H) coincides with \\u03bb (v), and whose set of edges E (Hv) C _ E (H) coincides with \\u03c7 (v).\\nIn an initialization step, the algorithm equips each vertex v with all the possible packings for Hv, which are stored in the set Hv.\\nNote that the size of Hv is bounded by (| E (H) | + 1) k, since each node in \\u03bb (v) is either left uncovered in a packing or is covered with precisely one of the hyperedges in \\u03c7 (v) C _ E (H).\\nThen, ComputeSetPackingk is designed to filter these packings by retaining only those that \\\"conform\\\" with some packing for Hc, for each children c of v in T, as formalized next.\\nLet hv and hc be two packings for Hv and Hc, respectively.\\nWe say that hv conforms with hc, denoted by hv \\u2248 hc if: for each h \\u2208 hc \\u2229 E (Hv), h is in hv; and, for each h \\u2208 (E (Hc) \\u2212 hc), h is not in hv.\\nEXAMPLE 4.\\nConsider again the hypertree decomposition of \\u00af H1 reported in Figure 3.\\n(c).\\nThen, the set of all the possible packings (which are build in the initialization step of ComputeSetPackingk), for each of its vertices, is re\\nFigure 5: Example application of Algorithm ComputeSetPackingk.\\nFigure 4: Algorithm ComputeSetPackingk.\\nMoreover, an arrow from a packing hc to hv denotes that hv conforms with hc.\\nFor instance, the reader may check that the packing {h3} \\u2208 Hv1 conforms with the packing {h2, h3} \\u2208 Hv3, but do not conform with {h1} \\u2208 Hv3.\\n\\u2701 ComputeSetPackingk builds a solution by traversing T in two phases.\\nIn the first phase, vertices of T are processed from the leaves to the root r, by means of the procedure BottomUp.\\nFor each node v being processed, the set Hv is preliminary updated by removing all the packings hv that do not conform with any packing for some of the children of v.\\nAfter this filtering is performed, the weight $hv is updated.\\nIntuitively, $vhv stores the weight of the best partial packing for H computed by using only the hyperedges occurring in \\u03c7 (Tv).\\nIndeed, if v is a leaf, then $vhv = w (hv).\\nOtherwise, for each child c of v in T, $vhv is updated with the maximum of $c hc \\u2212 w (hc \\u2229 hv) over all the packings hc that conforms with hv (resolving ties arbitrarily).\\nThe packing \\u00af hc for which this maximum is achieved is stored in the variable hhv, c.\\nIn the second phase, the tree T is processed starting from the root.\\nFirstly, the packing h \\u2217 is selected that maximizes the weight equipped with the packings in Hr.\\nThen, procedure TopDown is used to extend h \\u2217 to all the other partial packings for vertices of T.\\nIn particular, at each vertex v, h \\u2217 is extended with the packing hhv, c, for each child c of v. EXAMPLE 5.\\nAssume that, in our running example, w (h1) = w (h2) = w (h3) = w (h4) = 1.\\nThen, an execution of ComputeSetPackingk is graphically depicted in Figure 5.\\n(b), where an arrow from a packing hc to a packing hv is used to denote that hc = hhv, c. Specifically, the choices made during the computation are such that the packing {h2, h3} is computed.\\nIn particular, during the bottom-up phase, we have that:\\nand we set $v1\\ninstance, note that $v1 {h5} = 2 since {h5} conforms with the packing {h4} of Hv2 such that $v2 {h4} = 1.\\nThen, at the beginning of the top-down phase, ComputeSetPackingk selects {h3} as a packing for Hv1 and propagates this choice in the tree.\\nEquivalently, the algorithm may have chosen {h5}.\\nAs a further example, the way the solution {h1} is obtained by the algorithm when w (h1) = 5 and w (h2) = w (h3) = w (h4) = 1 is reported in Figure 5.\\n(c).\\nNotice that, this time, in the top-down phase, ComputeSetPackingk starts selecting {h1} as the best packing for Hv1.\\n\\u2701 THEOREM 4.\\nLet H be a hypergraph and w be a weighting function for it.\\nLet HD = (T, \\u03c7, \\u03bb) be a complete k-width hypertree decomposition of \\u00af H. Then, ComputeSetPackingk on input H, w, and HD correctly outputs a solution for MaxWSP (H, w) in time O (| T | \\u00d7 (| E (H) | + 1) 2k).\\nPROOF.\\n[Sketch] We observe that h \\u2217 (computed by ComputeSetPackingk) is a packing for H. Indeed, consider a pair of hyperedges h1 and h2 in h \\u2217, and assume, for the sake of contradiction, that h1 \\u2229 h2 = ~ \\u2205.\\nLet v1 (resp., v2) be an arbitrary vertex of T, for which ComputeSetPackingk included h1 (resp., h2) in h \\u2217 in the bottom-down computation.\\nBy construction, we have h1 \\u2208 \\u03c7 (v1) and h2 \\u2208 \\u03c7 (v2).\\nLet I be an element in h1 \\u2229 h2.\\nIn the dual hypergraph H, I is a hyperedge in E (\\u00af H) which covers both the nodes h1 and h2.\\nHence, by condition (1) in Definition 1, there is a vertex v \\u2208 vertices (T) such that {h1, h2} \\u2286 \\u03c7 (v).\\nNote that, because of the connectedness condition in Definition 1, we can also assume, w.l.o.g., that v is in the path connecting v1 and v2 in T. Let hv \\u2208 Hv denote the element added by ComputeSetPackingk into h \\u2217 during the bottom-down phase.\\nSince the elements in Hv are packings for Hv, it is the case that either h1 \\u2208 hv or h2 \\u2208 hv.\\nAssume, w.l.o.g., that h1 \\u2208 ~ hv, and notice that each vertex w in T in the path connecting v to v1 is such that h1 \\u2208 \\u03c7 (w), because of the connectedness condition.\\nHence, because of definition of conformance, the packing hw selected by ComputeSetPackingk to be added at vertex w in h \\u2217 must be such that h1 \\u2208 ~ hw.\\nThis holds in particular for w = v1.\\nContradiction with the definition of v1.\\nTherefore, h \\u2217 is a packing for H.\\nIt remains then to show that it has the maximum weight over all the packings for H. To this aim, we can use structural induction on T to prove that, in the bottom-up phase, the variable $vhv is updated to contain the weight of the packing on the edges in \\u03c7 (Tv), which contains hv and which has the maximum weight over all such packings for the edges in \\u03c7 (Tv).\\nThen, the result follows, since in the top-down phase, the packing hr giving the maximum weight over \\u03c7 (Tr) = E (H) is first included in h \\u2217, and then extended at each node c with the packing hhv, c conformingly with hv and such that the maximum value of ~ vhv is achieved.\\nAs for the complexity, observe that the initialization step requires the construction of the set Hv, for each vertex v, and each set has size (| E (H) | + 1) k at most.\\nThen, the function BottomUp checks for the conformance between strategies in Hv with strategies in Hc, for each pair (v, c) \\u2208 E, and updates the weight rhv.\\nThese tasks can be carried out in time O ((| E (H) | + 1) 2k) and must be repeated for each edge in T, i.e., O (| T |) times.\\nFinally, the function TopDown can be implemented in linear time in the size of T, since it just requires updating h \\u2217 by accessing the variable hhv, c.\\nThe above result shows that if a hypertree decomposition of width k is given, the MaxWSP problem can be efficiently solved.\\nMoreover, differently from the case of structured item graphs, it is well known that deciding the existence of a k-bounded hypertree decomposition and computing one (if any) are problems which can be efficiently solved in polynomial time [7].\\nTherefore, Theorem 4 witnesses that the class C (hw, k) actually constitutes a tractable class for the winner determination problem.\\nAs the following theorem shows, for large subclasses (that depend only on how the weight function is specified), MaxWSP (H, w) is even highly parallelizeable.\\nLet us call a weighting function smooth if it is logspace computable and if all weights are polynomial (and thus just require O (log n) bits for their representation).\\nRecall that LOGCFL is a parallel complexity class contained in NC2, cf. [9].\\nThe functional version of LOGCFL is LLOGCFL, which is obtained by equipping a logspace transducer with an oracle in LOGCFL.\\nTHEOREM 5.\\nLet H be a hypergraph in C (hw, k), and let w be a smooth weighting function for it.\\nThen, MaxWSP (H, w) is in LLOGCFL.\\n4.\\nHYPERTREE DECOMPOSITIONS VS STRUCTURED ITEM GRAPHS\\nGiven that the class C (hw, k) has been shown to be an island of tractability for the winner determination problem, and given that the class C (ig, k) has been shown not to be efficiently recognizable, one may be inclined to think that there are instances having unbounded hypertree width, but admitting an item graph of bounded tree width (so that the intractability of structured item graphs would lie in their generality).\\nSurprisingly, we establish this is not the case.\\nThe line of the proof is to first show that structured item graphs are in one-to-one correspondence with a special kind of hypertree decompositions of the dual hypergraph, which we shall call strict.\\nThen, the result will follow by proving that k-width strict hypertree decompositions are less powerful than kwith hypertree decompositions.\\n4.1 Strict Hypertree Decompositions\\nLet H be a hypergraph, and let V \\u2286 N (H) be a set of nodes and X, Y \\u2208 N (H).\\nX is [V] - adjacent to Y if there exists an edge h \\u2208 E (H) such that {X, Y} \\u2286 (h \\u2212 V).\\nA [V] - path \\u03c0 from X to Y is a sequence X = X0,..., X cents = Y of variables such that: Xi is [V] - adjacent to Xi +1, for each i \\u2208 [0...2-1].\\nA set W \\u2286 N (H) of nodes is [V] - connected if \\u2200 X, Y \\u2208 W there is a [V] - path from X to Y.\\nA [V] - component is a maximal [V] - connected non-empty set of nodes W \\u2286 (N (H) \\u2212 V).\\nFor any [V] - component C, let E (C) = {h \\u2208 E (H) | h \\u2229 C = ~ \\u2205}.\\nDEFINITION 2.\\nA hypertree decomposition HD = ~ T, \\u03c7, A ~ of H is strict if the following conditions hold: 1.\\nfor each pair of vertices r and s in vertices (T) such that s is a child of r, and for each [\\u03c7 (r)] - component Cr s.t. Cr \\u2229 \\u03c7 (Ts) = ~ \\u2205, Cr is a [\\u03c7 (r) \\u2229 N (A (r) \\u2229 A (s))] - component; 2.\\nfor each edge h \\u2208 E (H), there is a vertex p such that h \\u2208 A (p) and h \\u2286 \\u03c7 (p) (we say p strongly covers h); 3.\\nfor each edge h \\u2208 E (H), the set {p \\u2208 vertices (T) | h \\u2208 A (p)} induces a (connected) subtree of T.\\nThe strict hypertree width shw (H) of H is the minimum width over all its strict hypertree decompositions.\\n\\u2737 The basic relationship between nice hypertree decompositions and structured item graphs is shown in the following theorem.\\nNote that, as far as the maximum weighted-set packing problem is concerned, given a hypergraph H, we can always assume that for each node v \\u2208 N (H), {v} is in E (H).\\nIn fact, if this hyperedge is not in the hypergraph, then it can be added without loss of generality, by setting w ({v}) = 0.\\nTherefore, letting C (shw, k) denote the class of all the hypergraphs whose dual hypergraphs (associated with maximum 2The term \\\"+1\\\" only plays the technical role of taking care of the different definition of width for tree decompositions and hypertree decompositions.\\nweighted-set packing problems) have strict hypertree width bounded by k, we have that C (shw, k + 1) = C (ig, k).\\nBy definition, strict hypertree decompositions are special hypertree decompositions.\\nIn fact, we are able to show that the additional conditions in Definition 2 induce an actual restriction on the decomposition power.\\nTHEOREM 7.\\nC (ig, k) = C (shw, k + 1) \\u2282 C (hw, k + 1).\\nA Game Theoretic View.\\nWe shed further lights on strict hypertree decompositions by discussing an interesting characterization based on the strict Robber and Marshals Game, defined by adapting the Robber and Marshals game defined in [6], which characterizes hypertree width.\\nThe game is played on a hypergraph H by a robber against k marshals which act in coordination.\\nMarshals move on the hyperedges of H, while the robber moves on nodes of H.\\nThe robber sees where the marshals intend to move, and reacts by moving to another node which is connected with its current position and through a path in G (H) which does not use any node contained in a hyperedge that is occupied by the marshals before and after their move--we say that these hyperedges are blocked.\\nNote that in the basic game defined in [6], the robber is not allowed to move on vertices that are occupied by the marshals before and after their move, even if they do not belong to blocked hyperedges.\\nImportantly, marshals are required to play monotonically, i.e., they cannot occupy an edge that was previously occupied in the game, and which is currently not.\\nThe marshals win the game if they capture the robber, by occupying an edge covering a node where the robber is.\\nOtherwise, the robber wins.\\nTHEOREM 8.\\nLet H be a hypergraph such that for each node v \\u2208 N (H), {v} is in E (H).\\nThen, H \\u00af has a k-width strict hypertree decomposition if and only if k marshals can win the strict Robber and Marshals Game on \\u00af H, no matter of the robber's moves.\\n5.\\nCONCLUSIONS\\nWe have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario.\\nThe result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3.\\nMotivated by this result, we investigated the use of hypertree decomposition (on the dual hypergraph associated with the scenario) and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width.\\nFor some special, yet relevant cases, a highly parallelizable algorithm is also discussed.\\nInterestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width (hence, the reason of their intractability is not their generality).\\nIn particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions (on the dual hypergraph), called query decompositions (see, e.g., [7]).\\nIn the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.\\nAs a further avenue of research, it would be relevant to enhance the algorithm ComputeSetPackingk, e.g., by using specialized data structures, in order to avoid the quadratic dependency from (| E (H) | + 1) k. Finally, an other interesting question is to assess whether the structural decomposition techniques discussed in the paper can be used to efficiently deal with generalizations of the winner determination problem.\\nFor instance, it might be relevant in several application scenarios to design algorithms that can find a selling strategy when several copies of the same item are available for selling, and when moreover the auctioneer is satisfied when at least a given number of copies is actually sold.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lvl-3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems\\n{jasonh, kansal, szahedi, mbs} @ ee.ucla.edu vijay@nec-labs.com\\nABSTRACT\\nHarvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks.\\nIn this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment.\\nThe algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energyneutrality constraint, and (c) adapting to the dynamics of the energy source at run-time.\\nWe present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data.\\nWe also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source.\\nOur methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\\n1.\\nINTRODUCTION\\nEnergy supply has always been a crucial issue in designing battery-powered wireless sensor networks because the lifetime and utility of the systems are limited by how long the batteries are able to sustain the operation.\\nThe fidelity of the data produced by a sensor network begins to degrade once sensor nodes start to run out of battery power.\\nTherefore, harvesting energy from the environment has been proposed to supplement or completely replace battery supplies to enhance system lifetime and reduce the maintenance cost of replacing batteries periodically.\\nHowever, metrics for evaluating energy harvesting systems are different from those used for battery powered systems.\\nEnvironmental energy is distinct from battery energy in two ways.\\nFirst it is an inexhaustible supply which, if appropriately used, can allow the system to last forever, unlike the battery which is a limited resource.\\nSecond, there is an uncertainty associated with its availability and measurement, compared to the energy stored in the\\nbattery which can be known deterministically.\\nThus, power management methods based on battery status are not always applicable to energy harvesting systems.\\nIn addition, most power management schemes designed for battery-powered systems only account for the dynamics of the energy consumers (e.g., CPU, radio) but not the dynamics of the energy supply.\\nConsequently, battery powered systems usually operate at the lowest performance level that meets the minimum data fidelity requirement in order to maximize the system life.\\nEnergy harvesting systems, on the other hand, can provide enhanced performance depending on the available energy.\\nIn this paper, we will study how to adapt the performance of the available energy profile.\\nThere exist many techniques to accomplish performance scaling at the node level, such as radio transmit power adjustment [1], dynamic voltage scaling [2], and the use of low power modes [3].\\nHowever, these techniques require hardware support and may not always be available on resource constrained sensor nodes.\\nAlternatively, a common performance scaling technique is duty cycling.\\nLow power devices typically provide at least one low power mode in which the node is shut down and the power consumption is negligible.\\nIn addition, the rate of duty cycling is directly related to system performance metrics such as network latency and sampling frequency.\\nWe will use duty cycle adjustment as the primitive performance scaling technique in our algorithms.\\n2.\\nRELATED WORK\\nEnergy harvesting has been explored for several different types of systems, such as wearable computers [4], [5], [6], sensor networks [7], etc. .\\nSeveral technologies to extract energy from the environment have been demonstrated including solar, motion-based, biochemical, vibration-based [8], [9], [10], [11], and others are being developed [12], [13].\\nWhile several energy harvesting sensor node platforms have been prototyped [14], [15], [16], there is a need for systematic power management techniques that provide performance guarantees during system operation.\\nThe first work to take environmental energy into account for data routing was [17], followed by [18].\\nWhile these works did demonstrate that environment aware decisions improve performance compared to battery aware decisions, their objective was not to achieve energy neutral operation.\\nOur proposed techniques attempt to maximize system performance while maintaining energy-neutral operation.\\n3.\\nSYSTEM MODEL\\n4.\\nTHEORETICALLY OPTIMAL POWER MANAGEMENT\\n5.\\nHARVESTING-AWARE POWER MANAGEMENT\\n5.1.\\nEnergy Prediction Model\\n5.2.\\nLow-complexity Solution\\n5.3.\\nSlot-by-slot continual duty cycle adaptiation.\\n6.\\nEVALUATION\\nDay Hour\\n4 Solar Energy Profile (Left: Continuous, Right: Diurnal)\\n6.1.\\nPrediction Model\\n5.\\nAverage Predictor Error in mA\\n6.2.\\nAdaptive Duty cycling algorithm\\n7.\\nCONCLUSIONS\\nWe discussed various issues in power management for systems powered using environmentally harvested energy.\\nSpecifically, we designed a method for optimizing performance subject to the constraint of energy neutral operation.\\nWe also derived a theoretically optimal bound on the performance and showed that our proposed algorithm operated very close to the optimal.\\nThe proposals were evaluated using real data collected using an energy harvesting sensor node deployed in an outdoor environment.\\nOur method has significant advantages over currently used methods which are based on a conservative estimate of duty cycle and can only provide sub-optimal performance.\\nHowever, this work is only the first step towards optimal solutions for energy neutral operation.\\nIt is designed for a specific power scaling method based on adapting the duty cycle.\\nSeveral other power scaling methods, such as DVS, submodule power switching and the use of multiple low power modes are also available.\\nIt is thus of interest to extend our methods to exploit these advanced capabilities.\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems\\nABSTRACT\\nIn this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.\\nUnlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.\\nWe show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.\\nEMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.\\nWe exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).\\nWe show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\\n1.\\nINTRODUCTION\\nPlanning and control constitutes a central research area in multiagent systems and artificial intelligence.\\nIn recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.\\nIn this framework, the planning and control problem is often\\naddressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.\\nWhile theoretically attractive, the complexity of optimally solving a POMDP is prohibitive [8, 7].\\nWe take an alternative view of planning in stochastic environments.\\nWe do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.\\nThe idea here is to view planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.\\nWe call this general planning framework Dynamics Based Control (DBC).\\nIn DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.\\nAs actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].\\nHere, optimality is measured in terms of probability of deviation magnitudes.\\nIn this paper, we present the structure of Dynamics Based Control.\\nWe show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.\\nEMT is an efficient instantiation of DBC.\\nTo evaluate DBC, we carried out a set of experiments applying multi-target EMT to the Tag Game [11]; this is a variant on the area sweeping problem, where an agent is trying to \\\"tag\\\" a moving target (quarry) whose position is not known with certainty.\\nExperimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent's position).\\nThe paper is organized as follows.\\nIn Section 2 we motivate DBC using area-sweeping problems, and discuss related work.\\nSection 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.\\nThis is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.\\nThat section also discusses the limitations of EMT-based control relative to the general DBC framework.\\nExperimental settings and results are then presented in Section 5.\\nSection 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work.\\n2.\\nMOTIVATION AND RELATED WORK\\n3.\\nDYNAMICS BASED CONTROL\\n3.1 DBC for Markovian Environments\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791\\n3.2 DBC View of the State Space\\n4.\\nEMT-BASED CONTROL AS A DBC\\n792 The Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07)\\n4.1 Multi-Target EMT\\n4.2 EMT-based Control Limitations\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 793\\n5.\\nEMT PLAYING TAG\\n6.\\nDISCUSSION\\n7.\\nCONCLUSIONS AND FUTURE WORK\\nIn this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.\\nDBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.\\nOptimality of DBC plans of action are measured\\nThe Sixth Intl. .\\nJoint Conf.\\non Autonomous Agents and Multi-Agent Systems (AAMAS 07) 795\\nFigure 4: Observation Model I: Omniposition quarry.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nFigure 5: Observation Model II: quarry not observed at hunter's position.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nwith respect to the deviation of actual system dynamics from the target dynamics.\\nWe show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.\\nIn fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.\\nSince EMT exhibits the key features of the general DBC framework, as well as polynomial time complexity, we used the multitarget version of EMT [15] to demonstrate that the class of area sweeping problems naturally lends itself to dynamics-based descriptions, as instantiated by our experiments in the Tag Game domain.\\nAs enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.\\nThis prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).\\nHowever, DBC in general has no such limitations, and readily enables the formulation of evasion games.\\nIn future work, we intend to proceed with the development of dynamics-based controllers for these problems.\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions\\nABSTRACT\\nThe winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices.\\nWhile this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs).\\nFormally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph.\\nNote that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness.\\nIn fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem.\\nIn this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3.\\nMotivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions.\\nWe show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here.\\nIndeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width.\\nEven more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\\n1.\\nINTRODUCTION\\nCombinatorial auctions.\\nCombinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items.\\nThis is desirable when a bidder's valuation of a bundle of items is not equal to the sum of her valuations of the individual items.\\nThis framework is currently used to regulate agents' interactions in several application domains (cf., e.g., [21]) such as, electricity markets [13], bandwidth auctions [14], and transportation exchanges [18].\\nFormally, a combinatorial auction is a pair (Z, B), where Z = {I1,..., Im} is the set of items the auctioneer has to sell, and B = {B1,..., Bn} is the set of bids from the buyers interested in the items in Z. Each bid Bi has the form (item (Bi), pay (Bi)), where pay (Bi) is a rational number denoting the price a buyer offers for the items in item (Bi) C Z.\\nAn outcome for (Z, B) is a subset b of B such that item (Bi) n item (Bj) = 0, for each pair Bi and Bj of bids in b with i = ~ j.\\nThe winner determination problem.\\nA crucial problem for combinatorial auctions is to determine the outcome b \\u2217 that maximizes the sum of the accepted bid prices (i.e.,\\nBi \\u2208 b \\u2217 pay (Bi)) over all the possible outcomes.\\nThis problem, called winner determination problem (e.g., [11]), is known to be intractable, actually NP-hard [17], and even not approximable in polynomial time unless NP = ZPP [19].\\nHence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions (e.g., [20, 5, 2, 8, 23]) and to identify classes of instances where solving the winner determination problem is feasible in polynomial time (e.g., [15, 22, 12, 21]).\\nIn fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions.\\nItem graphs.\\nCurrently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any\\nFigure 1: Example MaxWSP problem: (a) Hypergraph H (To, go), and a packing h for it; (b) Primal graph for H (To, go); and, (c, d) Two item graphs for H (To, go).\\nbid, the items occurring in it induce a connected subgraph.\\nIndeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure [3]--formally bounded treewidth [16].\\nTo have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H (T, g) such that its set of nodes N (H (T, g)) coincides with set of items I, and where its edges E (H (T, g)) are precisely the bids of the buyers {item (Bi) | Bi \\u2208 B}.\\nA special item graph for ~ I, B ~ is the primal graph of H (T, g), denoted by G (H (T, g)), which contains an edge between any pair of nodes in some hyperedge of H (T, g).\\nThen, any item graph for H (T, g) can be viewed as a simplification of G (H (T, g)) obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge.\\nEXAMPLE 1.\\nThe hypergraph H (To, go) reported in Figure 1.\\n(a) is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = {I1,..., I5}, and item (Bi) = hi, for each 1 \\u2264 i \\u2264 3.\\nThe primal graph for H (To, go) is reported in\\nFigure 1.\\n(b), while two example item graphs are reported in Figure 1.\\n(c) and (d), where edges required for maintaining\\nthe connectivity for h1 are depicted in bold.\\n<\\nOpen Problem: Computing structured item\\ngraphs efficiently.\\nThe above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined.\\nHowever, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain (constant) treewidth exists, and if so, how to compute such a structured item graph efficiently.\\nPolynomial time algorithms to find the \\\"best\\\" simplification of the primal graph were so far only known for the cases where the item graph to be constructed is a line [10], a cycle [4], or a tree [3], but it was an important open problem (cf. [3]) whether it is tractable to check if for a combinatorial auction, an item graph of treewidth bounded by a fixed natural number k exists and can be constructed in polynomial time, if so.\\nWeighted Set Packing.\\nLet us note that the hypergraph representation H (T, g) of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear (e.g., [17]).\\nFormally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h' \\u2208 h with h = ~ h', it holds that h \\u2229 h' = \\u2205.\\nLetting w be a weighting function for H, i.e., a polynomially-time computable function from E (H) to rational numbers, the weight of a packing h is the rational number w (h) = EhCh w (h), where w ({}) = 0.\\nThen, the maximum-weighted set packing problem for H w.r.t. w, denoted by MaxWSP (H, w), is the problem of finding a packing for H having the maximum weight over all the packings for H. To see that MaxWSP is just a different formulation for the winner determination problem, given a combinatorial auction ~ I, B ~, it is sufficient to define the weighting function w (T, g) (item (Bi)) = pay (Bi).\\nThen, the set of the solutions for the weighted set packing problem for H (T, g) w.r.t. w (T, g) coincides with the set of the solutions for the winner determination problem on ~ I, B ~.\\nEXAMPLE 2.\\nConsider again the hypergraph H (To, go) reported in Figure 1.\\n(a).\\nAn example packing for H (To, go) is h = {h1}, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1.\\nBy assuming that bids B1, B2, and B3 are such that pay (B1) = pay (B2) = pay (B3), the packing h is not a solution for the problem MaxWSP (H (To, go), w (To, go)).\\nIndeed, the packing\\nContributions\\nThe primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable.\\nTowards this aim, we first study structured item graphs and solve the open problem in [3].\\nThe result is very bad news: \\u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3.\\nMore formally, letting C (ig, k) denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph (associated with a combinatorial auction problem) belongs to C (ig, 3) is NP-complete.\\nIn the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem.\\nOur investigations, this time, led to very good news which are summarized below:\\n\\u25ba For a hypergraph H, its dual H \\u00af = (V, E) is such that nodes in V are in one-to-one correspondence with hyperedges in H, and for each node x \\u2208 N (H), {h | x \\u2208 h \\u2227 h \\u2208\\nE (H)} is in E.\\nWe show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width [7] bounded by k (short: class C (hw, k) of hypergraphs).\\nNote that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \\u00af instead of the auction hypergraph H.\\nIn fact, we can show that MaxWSP remains NP-hard even when H is acyclic (i.e., when it has hypertree width 1), even when each node is contained in 3 hyperedges at most.\\n\\u25ba For some relevant special classes of hypergraphs in C (hw, k), we design a higly-parallelizeable algorithm for MaxWSP.\\nSpecifically, if the weighting functions can be computed in logarithmic space and weights are polynomial (e.g., when all the hyperegdes have unitary weights and one is interested in finding the packing with the maximum number of edges), we show that MaxWSP can be solved by a LOGCFL algorithm.\\nRecall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P (see, e.g., [9]).\\n\\u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs.\\nTo the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs.\\nIn fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach.\\nIntuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition.\\n\\u25ba The proof of the above results give us some interesting insight into the notion of structured item graph.\\nIndeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions.\\nA game-characterization for the notion of strict hypertree width is also proposed, which specializes the Robber and Marshals game in [6] (proposed to characterize the hypertree width), and which makes it clear the further requirements on hypertree decompositions.\\nThe rest of the paper is organized as follows.\\nSection 2 discusses the intractability of structured item graphs.\\nSection 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable.\\nThe comparison between the classes C (ig, k) and C (hw, k) is discussed in Section 4.\\nFinally, in Section 5 we draw our conclusions by also outlining directions for further research.\\n2.\\nCOMPLEXITY OF STRUCTURED ITEM GRAPHS\\nConnectedness between blocks,\\n3.\\nTRACTABLE CASES VIA HYPERTREE DECOMPOSITIONS\\n3.1 Hypertree Decomposition on the Dual Hypergraph and Tractable Packing Problems\\n4.\\nHYPERTREE DECOMPOSITIONS VS STRUCTURED ITEM GRAPHS\\n4.1 Strict Hypertree Decompositions\\n5.\\nCONCLUSIONS\\nWe have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario.\\nThe result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3.\\nMotivated by this result, we investigated the use of hypertree decomposition (on the dual hypergraph associated with the scenario) and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width.\\nFor some special, yet relevant cases, a highly parallelizable algorithm is also discussed.\\nInterestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width (hence, the reason of their intractability is not their generality).\\nIn particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions (on the dual hypergraph), called query decompositions (see, e.g., [7]).\\nIn the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.\\nAs a further avenue of research, it would be relevant to enhance the algorithm ComputeSetPackingk, e.g., by using specialized data structures, in order to avoid the quadratic dependency from (| E (H) | + 1) k. Finally, an other interesting question is to assess whether the structural decomposition techniques discussed in the paper can be used to efficiently deal with generalizations of the winner determination problem.\\nFor instance, it might be relevant in several application scenarios to design algorithms that can find a selling strategy when several copies of the same item are available for selling, and when moreover the auctioneer is satisfied when at least a given number of copies is actually sold.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lvl-4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems\\n{jasonh, kansal, szahedi, mbs} @ ee.ucla.edu vijay@nec-labs.com\\nABSTRACT\\nHarvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks.\\nIn this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment.\\nThe algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energyneutrality constraint, and (c) adapting to the dynamics of the energy source at run-time.\\nWe present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data.\\nWe also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source.\\nOur methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\\n1.\\nINTRODUCTION\\nEnergy supply has always been a crucial issue in designing battery-powered wireless sensor networks because the lifetime and utility of the systems are limited by how long the batteries are able to sustain the operation.\\nThe fidelity of the data produced by a sensor network begins to degrade once sensor nodes start to run out of battery power.\\nTherefore, harvesting energy from the environment has been proposed to supplement or completely replace battery supplies to enhance system lifetime and reduce the maintenance cost of replacing batteries periodically.\\nHowever, metrics for evaluating energy harvesting systems are different from those used for battery powered systems.\\nEnvironmental energy is distinct from battery energy in two ways.\\nSecond, there is an uncertainty associated with its availability and measurement, compared to the energy stored in the\\nbattery which can be known deterministically.\\nThus, power management methods based on battery status are not always applicable to energy harvesting systems.\\nIn addition, most power management schemes designed for battery-powered systems only account for the dynamics of the energy consumers (e.g., CPU, radio) but not the dynamics of the energy supply.\\nConsequently, battery powered systems usually operate at the lowest performance level that meets the minimum data fidelity requirement in order to maximize the system life.\\nEnergy harvesting systems, on the other hand, can provide enhanced performance depending on the available energy.\\nIn this paper, we will study how to adapt the performance of the available energy profile.\\nHowever, these techniques require hardware support and may not always be available on resource constrained sensor nodes.\\nAlternatively, a common performance scaling technique is duty cycling.\\nIn addition, the rate of duty cycling is directly related to system performance metrics such as network latency and sampling frequency.\\nWe will use duty cycle adjustment as the primitive performance scaling technique in our algorithms.\\n2.\\nRELATED WORK\\nEnergy harvesting has been explored for several different types of systems, such as wearable computers [4], [5], [6], sensor networks [7], etc. .\\nWhile several energy harvesting sensor node platforms have been prototyped [14], [15], [16], there is a need for systematic power management techniques that provide performance guarantees during system operation.\\nThe first work to take environmental energy into account for data routing was [17], followed by [18].\\nWhile these works did demonstrate that environment aware decisions improve performance compared to battery aware decisions, their objective was not to achieve energy neutral operation.\\nOur proposed techniques attempt to maximize system performance while maintaining energy-neutral operation.\\n7.\\nCONCLUSIONS\\nWe discussed various issues in power management for systems powered using environmentally harvested energy.\\nSpecifically, we designed a method for optimizing performance subject to the constraint of energy neutral operation.\\nThe proposals were evaluated using real data collected using an energy harvesting sensor node deployed in an outdoor environment.\\nOur method has significant advantages over currently used methods which are based on a conservative estimate of duty cycle and can only provide sub-optimal performance.\\nHowever, this work is only the first step towards optimal solutions for energy neutral operation.\\nIt is designed for a specific power scaling method based on adapting the duty cycle.\\nSeveral other power scaling methods, such as DVS, submodule power switching and the use of multiple low power modes are also available.\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems\\nABSTRACT\\nIn this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments.\\nUnlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics.\\nWe show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC.\\nEMT employs greedy action selection to provide an efficient control algorithm in Markovian environments.\\nWe exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets).\\nWe show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\\n1.\\nINTRODUCTION\\nPlanning and control constitutes a central research area in multiagent systems and artificial intelligence.\\nIn recent years, Partially Observable Markov Decision Processes (POMDPs) [12] have become a popular formal basis for planning in stochastic environments.\\nIn this framework, the planning and control problem is often\\nWe take an alternative view of planning in stochastic environments.\\nWe do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.\\nWe call this general planning framework Dynamics Based Control (DBC).\\nIn DBC, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific (potentially stochastic) target dynamics.\\nAs actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].\\nHere, optimality is measured in terms of probability of deviation magnitudes.\\nIn this paper, we present the structure of Dynamics Based Control.\\nWe show that the recently developed Extended Markov Tracking (EMT) approach [13, 14, 15] is subsumed by DBC, with EMT employing greedy action selection, which is a specific parameterization among the options possible within DBC.\\nEMT is an efficient instantiation of DBC.\\nThe paper is organized as follows.\\nIn Section 2 we motivate DBC using area-sweeping problems, and discuss related work.\\nSection 3 introduces the Dynamics Based Control (DBC) structure, and its specialization to Markovian environments.\\nThis is followed by a review of the Extended Markov Tracking (EMT) approach as a DBC-structured control regimen in Section 4.\\nThat section also discusses the limitations of EMT-based control relative to the general DBC framework.\\nExperimental settings and results are then presented in Section 5.\\nSection 6 provides a short discussion of the overall approach, and Section 7 gives some concluding remarks and directions for future work.\\n7.\\nCONCLUSIONS AND FUTURE WORK\\nIn this paper, we have presented a novel perspective on the process of planning and control in stochastic environments, in the form of the Dynamics Based Control (DBC) framework.\\nDBC formulates the task of planning as support of a specified target system dynamics, which describes the necessary properties of change within the environment.\\nOptimality of DBC plans of action are measured\\nThe Sixth Intl. .\\nJoint Conf.\\nFigure 4: Observation Model I: Omniposition quarry.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nFigure 5: Observation Model II: quarry not observed at hunter's position.\\nEntropy development with length of Tag Game for the three experiment scenarios: a) multiple dead-end, b) irregular open arena, c) circular corridor.\\nwith respect to the deviation of actual system dynamics from the target dynamics.\\nWe show that a recently developed technique of Extended Markov Tracking (EMT) [13] is an instantiation of DBC.\\nIn fact, EMT can be seen as a specific case of DBC parameterization, which employs a greedy action selection procedure.\\nAs enumerated in Section 4.2, EMT has a number of limitations, such as difficulty in dealing with negative dynamic preference.\\nThis prevents direct application of EMT to such problems as Rendezvous-Evasion Games (e.g., [6]).\\nHowever, DBC in general has no such limitations, and readily enables the formulation of evasion games.\\nIn future work, we intend to proceed with the development of dynamics-based controllers for these problems.\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions\\nABSTRACT\\nThe winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices.\\nWhile this problem is in general NPhard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs).\\nFormally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph.\\nNote that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness.\\nIn fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem.\\nIn this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3.\\nMotivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions.\\nWe show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here.\\nIndeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width.\\nEven more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\\n1.\\nINTRODUCTION\\nCombinatorial auctions.\\nCombinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items.\\nThis is desirable when a bidder's valuation of a bundle of items is not equal to the sum of her valuations of the individual items.\\nAn outcome for (Z, B) is a subset b of B such that item (Bi) n item (Bj) = 0, for each pair Bi and Bj of bids in b with i = ~ j.\\nThe winner determination problem.\\nA crucial problem for combinatorial auctions is to determine the outcome b \\u2217 that maximizes the sum of the accepted bid prices (i.e.,\\nBi \\u2208 b \\u2217 pay (Bi)) over all the possible outcomes.\\nThis problem, called winner determination problem (e.g., [11]), is known to be intractable, actually NP-hard [17], and even not approximable in polynomial time unless NP = ZPP [19].\\nHence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions (e.g., [20, 5, 2, 8, 23]) and to identify classes of instances where solving the winner determination problem is feasible in polynomial time (e.g., [15, 22, 12, 21]).\\nIn fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions.\\nItem graphs.\\nCurrently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any\\nFigure 1: Example MaxWSP problem: (a) Hypergraph H (To, go), and a packing h for it; (b) Primal graph for H (To, go); and, (c, d) Two item graphs for H (To, go).\\nbid, the items occurring in it induce a connected subgraph.\\nIndeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure [3]--formally bounded treewidth [16].\\nTo have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction ~ I, B ~ can be represented by means of a hypergraph H (T, g) such that its set of nodes N (H (T, g)) coincides with set of items I, and where its edges E (H (T, g)) are precisely the bids of the buyers {item (Bi) | Bi \\u2208 B}.\\nA special item graph for ~ I, B ~ is the primal graph of H (T, g), denoted by G (H (T, g)), which contains an edge between any pair of nodes in some hyperedge of H (T, g).\\nThen, any item graph for H (T, g) can be viewed as a simplification of G (H (T, g)) obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge.\\nEXAMPLE 1.\\nThe hypergraph H (To, go) reported in Figure 1.\\n(a) is an encoding for a combinatorial auction ~ I0, B0 ~, where I0 = {I1,..., I5}, and item (Bi) = hi, for each 1 \\u2264 i \\u2264 3.\\nThe primal graph for H (To, go) is reported in\\nFigure 1.\\n(b), while two example item graphs are reported in Figure 1.\\n(c) and (d), where edges required for maintaining\\nthe connectivity for h1 are depicted in bold.\\n<\\nOpen Problem: Computing structured item\\ngraphs efficiently.\\nThe above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined.\\nHowever, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain (constant) treewidth exists, and if so, how to compute such a structured item graph efficiently.\\nWeighted Set Packing.\\nLet us note that the hypergraph representation H (T, g) of a combinatorial auction ~ I, B ~ is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear (e.g., [17]).\\nFormally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h' \\u2208 h with h = ~ h', it holds that h \\u2229 h' = \\u2205.\\nThen, the set of the solutions for the weighted set packing problem for H (T, g) w.r.t. w (T, g) coincides with the set of the solutions for the winner determination problem on ~ I, B ~.\\nEXAMPLE 2.\\nConsider again the hypergraph H (To, go) reported in Figure 1.\\n(a).\\nAn example packing for H (To, go) is h = {h1}, which intuitively corresponds to an outcome for ~ I0, B0 ~, where the auctioneer accepted the bid B1.\\nIndeed, the packing\\nContributions\\nThe primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable.\\nTowards this aim, we first study structured item graphs and solve the open problem in [3].\\nThe result is very bad news: \\u25ba It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3.\\nMore formally, letting C (ig, k) denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph (associated with a combinatorial auction problem) belongs to C (ig, 3) is NP-complete.\\nIn the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem.\\nE (H)} is in E.\\nWe show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width [7] bounded by k (short: class C (hw, k) of hypergraphs).\\nNote that a key issue of the tractability is to consider the hypertree width of the dual hypergraph H \\u00af instead of the auction hypergraph H.\\nIn fact, we can show that MaxWSP remains NP-hard even when H is acyclic (i.e., when it has hypertree width 1), even when each node is contained in 3 hyperedges at most.\\n\\u25ba For some relevant special classes of hypergraphs in C (hw, k), we design a higly-parallelizeable algorithm for MaxWSP.\\nRecall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL C _ NC2 C _ P (see, e.g., [9]).\\n\\u25ba Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs.\\nTo the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs.\\nIn fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach.\\nIntuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition.\\n\\u25ba The proof of the above results give us some interesting insight into the notion of structured item graph.\\nIndeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions.\\nThe rest of the paper is organized as follows.\\nSection 2 discusses the intractability of structured item graphs.\\nSection 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable.\\nThe comparison between the classes C (ig, k) and C (hw, k) is discussed in Section 4.\\nFinally, in Section 5 we draw our conclusions by also outlining directions for further research.\\n5.\\nCONCLUSIONS\\nWe have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario.\\nThe result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3.\\nMotivated by this result, we investigated the use of hypertree decomposition (on the dual hypergraph associated with the scenario) and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width.\\nFor some special, yet relevant cases, a highly parallelizable algorithm is also discussed.\\nInterestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width (hence, the reason of their intractability is not their generality).\\nIn particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions (on the dual hypergraph), called query decompositions (see, e.g., [7]).\\nIn the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["se2010_df = all_se2010_df[['title', 'abstract', 'keyphrases']]"],"metadata":{"id":"M4yDn3wdu8-S","executionInfo":{"status":"ok","timestamp":1714621730280,"user_tz":-420,"elapsed":13,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["se2010_df = se2010_df.rename(columns={'keyphrases': 'keyword'})"],"metadata":{"id":"AwE5q0edv1pI","executionInfo":{"status":"ok","timestamp":1714621730280,"user_tz":-420,"elapsed":12,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["se2010_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"aRrV-p_2vLKt","executionInfo":{"status":"ok","timestamp":1714621730280,"user_tz":-420,"elapsed":12,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"42afbfec-65a2-41fa-8946-06379d7fc7af"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               title  \\\n","0  The Sequential Auction Problem on eBay: An Emp...   \n","1     Approximate and Online Multi-Issue Negotiation   \n","2  On Opportunistic Techniques for Solving Decent...   \n","3  Searching for Joint Gains in Automated Negotia...   \n","4                  Multi-Attribute Coalitional Games   \n","\n","                                            abstract  \\\n","0  Bidders on eBay have no dominant bidding strat...   \n","1  This paper analyzes bilateral multi-issue nego...   \n","2  Decentralized Markov Decision Processes (DEC-M...   \n","3  It is well established by conflict theorists a...   \n","4  We study coalitional games where the value of ...   \n","\n","                                             keyword  \n","0  [sequenti auction problem, empir analysi, bid ...  \n","1  [approxim, negoti, time constraint, equilibriu...  \n","2  [decentr markov decis process, decentr markov ...  \n","3  [autom negoti, negoti, creat valu, claim valu,...  \n","4  [multi-attribut coalit game, coalit game, coop...  "],"text/html":["\n","  <div id=\"df-7748df83-9bc6-4757-96d1-9afd500223a0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Sequential Auction Problem on eBay: An Emp...</td>\n","      <td>Bidders on eBay have no dominant bidding strat...</td>\n","      <td>[sequenti auction problem, empir analysi, bid ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Approximate and Online Multi-Issue Negotiation</td>\n","      <td>This paper analyzes bilateral multi-issue nego...</td>\n","      <td>[approxim, negoti, time constraint, equilibriu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>On Opportunistic Techniques for Solving Decent...</td>\n","      <td>Decentralized Markov Decision Processes (DEC-M...</td>\n","      <td>[decentr markov decis process, decentr markov ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Searching for Joint Gains in Automated Negotia...</td>\n","      <td>It is well established by conflict theorists a...</td>\n","      <td>[autom negoti, negoti, creat valu, claim valu,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Multi-Attribute Coalitional Games</td>\n","      <td>We study coalitional games where the value of ...</td>\n","      <td>[multi-attribut coalit game, coalit game, coop...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7748df83-9bc6-4757-96d1-9afd500223a0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7748df83-9bc6-4757-96d1-9afd500223a0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7748df83-9bc6-4757-96d1-9afd500223a0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-86203695-fa35-4605-8867-945e7d19fa50\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86203695-fa35-4605-8867-945e7d19fa50')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-86203695-fa35-4605-8867-945e7d19fa50 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"se2010_df","summary":"{\n  \"name\": \"se2010_df\",\n  \"rows\": 244,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Adaptive Duty Cycling for Energy Harvesting Systems\",\n          \"Dynamics Based Control with an Application to Area-Sweeping Problems\",\n          \"On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 244,\n        \"samples\": [\n          \"Harvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks. In this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment. The algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energy-neutrality constraint, and (c) adapting to the dynamics of the energy source at run-time. We present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data. We also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source. Our methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.\",\n          \"In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multitarget EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.\",\n          \"The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NP-hard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs). Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem. In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["print(se2010_df.iloc[0]['title'])\n","print(se2010_df.iloc[0]['abstract'])\n","print(se2010_df.iloc[0]['keyword'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drdz-kHivOge","executionInfo":{"status":"ok","timestamp":1714621730281,"user_tz":-420,"elapsed":12,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"79e1c89e-e992-4092-f626-eba9ff59187f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["The Sequential Auction Problem on eBay: An Empirical Analysis and a Solution\n","Bidders on eBay have no dominant bidding strategy when faced with multiple auctions each offering an item of interest. As seen through an analysis of 1,956 auctions on eBay for a Dell E193FP LCD monitor, some bidders win auctions at prices higher than those of other available auctions, while others never win an auction despite placing bids in losing efforts that are greater than the closing prices of other available auctions. These misqueues in strategic behavior hamper the efficiency of the system, and in so doing limit the revenue potential for sellers. This paper proposes a novel options-based extension to eBay's proxy-bidding system that resolves this strategic issue for buyers in commoditized markets. An empirical analysis of eBay provides a basis for computer simulations that investigate the market effects of the options-based scheme, and demonstrates that the options-based scheme provides greater efficiency than eBay, while also increasing seller revenue.\n","['sequenti auction problem', 'empir analysi', 'bid strategi', 'multipl auction', 'strateg behavior', 'commodit market', 'comput simul', 'market effect', 'ebai', 'option-base extens', 'proxi-bid system', 'trade opportun', 'electron marketplac', 'busi-to-consum auction', 'autom trade agent', 'onlin auction', 'option', 'proxi bid']\n"]}]},{"cell_type":"markdown","source":["### SE-2017"],"metadata":{"id":"zzXm4P-6eSka"}},{"cell_type":"code","source":["se2017_ds = load_dataset(\"midas/semeval2017\")"],"metadata":{"id":"P2RdYEVBeV-G","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["eef63402359f487aa7c67c03497cff8d","afd0ec6aa6ea485a81382e798db51818","ef366be79f3d4a7995ebe5245889663e","141c22687d714cdcbe1a28b442d9a4dc","968a6e4cf19f46cb81f5229c99a6c90b","f80fa94e9578452a8b4189b68703930f","bb477b1017b241cab74b73b599cffb9f","22e95415150e41da8e2e6b875f425de3","fe09f32dd8ba407a8f51a25fbab159d2","e8f00e5d81b047b5852f3b9cc548ab84","f28b850b4ec24d7dbd8beaa1ff990682","ed83cab240be44e294e88a119b5c57ac","b0f3df92253b4e7488e74b24ae7125d9","06ece1b06cef420bb911c9e13f975c98","bf95bbc30a674ad9b662a0c110b28c0f","e044a70208624955b4b809a6144b5fb6","bdc24e0eccc7401a87a10d4f00c44652","6a94809527ec426db0eedff8f0dd67be","9493e91559b34f798e5d700c7699f4e2","897f4f20bcde4f669ee9fef52f0ebebe","f1e301c920b44e2ebaefcbb271a7c7bc","254412c46ca64bcbb46224de4223a2c4","58392037e2c744c1a427ca606d23947c","f58e4a9132664067ab51da423104e2eb","48cf83dcdbaf4d94a872fd919a4dd42c","1f9d8f9fcbb14afc9cd92003d2a0715f","d8e63ca503fb4f0ca9858d85f9f67654","7e00aed41ea34a76b74708141bb8dc5e","bdb86c09c69447219cfdc7b2ec97b2a1","41a3d288068948dd929d5786297262fa","57d3764dd2354f1db8eeb6d6273864ae","eb3f8865e71940879a998537c858fe7c","35bb487ea8ef475f866f0cad6e733177","08217b6344ce4a99a4ab4e795b6a0162","44120d9e04004cf0a93b3d7fb3c711b6","33cad541472144e99b874c6c751792f3","b8be985d66d248c6b1ab9c1a20bcb962","ce5ab5bc756e4869bceab1655ad8f6f2","14e58c7a9e4e491aa4d94ebb7644fe6c","440f1d8dc023484eb383520f924cb4c4","e08e0413bd954d148946861203e35efc","caa0e31b0e1341f28140df134e28f821","ce5b5f49fce04c3888c0c337e4a6052f","4812bb99ce5c45498088136928ba7c25","1a4c3fcef2434ea29f2980ce6148bf7c","034059e662534469ba9c60b6fa83e735","a615dd46ea054f13b61f4d220da7a88c","4311ff7c08fd41128ecd13dd454b34f0","c6346f52f12a4bef8da14f65d535b168","c998bff59cfc4daa9762b47ea25fa8ff","d039052175664a85a164df1390578830","8727d7f7c7fe4a619bdd1d2ee2486af6","617cc7196d2f467fa996a80b669e53b2","147a77cb7a96439a8d607992c5b6adad","b38e3a4d875f4894a66bcacd8b6d93d9","76dce3d478a3489f91c12abc873d2db4","f1b3b2f98be34807b8a31942ee6e0917","d0fb9a4cc1a94e3d9a26fb315de8aa61","f22fec2e982e4c979be4537208671dea","af493325bccc4c5a99fe70b9f41bf499","4bc1fcd3452c46b3bde01875e210e18d","316061152b03413d9a4e43435de573d0","7c19d2947d3d42f2b67ed72921890ab3","4550c30b5467461ab8a449dc3a8d10f8","35df4bda9fd149ef9ad2b5fee10285ed","4b4268076448459fa5c6433b8495fadd","40dd6043e1bf43d981ddda95436a7855","b2e3ce5a2a394de6b4c499df0831c67f","7a473a291e75457596dc53ebfaf125ff","ad60b6e5aa284523aac81d7d59be48ca","428a34588ce64d999a61873d482a3ffe","288ed44e1fb44c7c98c68d884790c533","b1079f260c014837857c788fe4c0268f","7d20e0762bf34aeba125d9c2d0cf2e8d","1606b6fd265549a696e4d83318441f6f","99f0bfc71b3e43bb860deafb0f1cf97e","4a8ee0d2c332434887e9df5b6abcffa9","5c9e11bff2b043e0bd35c33e03ce5027","8288c4232a9e436b9d44de4bb5f5c5af","66c98d11501748869e89ebffa7be3507","4d9d06beab9c4344b95ac75770ddb925","71835a3622a34b52b80e6f382c849379","9711d6eb6ff44ac0b202b7ff551f47d9","5e41e910f6f147b2921a87c9e6445738","c369b24123b844e0a3d06ddb83465154","24ae707eeba145adaa94363731a99af6","04c79e97f93c43ed840730b0cfd62673","3f46caa2446645e2aa15160ab8e3fb27"],"height":0},"executionInfo":{"status":"ok","timestamp":1714621737962,"user_tz":-420,"elapsed":7692,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"77a34cb1-152a-44f5-94c2-666baeb12df8"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/datasets/load.py:1486: FutureWarning: The repository for midas/semeval2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/midas/semeval2017\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.56k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef63402359f487aa7c67c03497cff8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/22.8k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed83cab240be44e294e88a119b5c57ac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Repo card metadata block was not found. Setting CardData to empty.\n","WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/337k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58392037e2c744c1a427ca606d23947c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08217b6344ce4a99a4ab4e795b6a0162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/175k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4c3fcef2434ea29f2980ce6148bf7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76dce3d478a3489f91c12abc873d2db4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40dd6043e1bf43d981ddda95436a7855"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c9e11bff2b043e0bd35c33e03ce5027"}},"metadata":{}}]},{"cell_type":"code","source":["print(se2017_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vimo5HpbwP2P","executionInfo":{"status":"ok","timestamp":1714621737962,"user_tz":-420,"elapsed":20,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"3e5f32a3-08db-4101-eb8b-ab807794f2c4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 350\n","    })\n","    test: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 100\n","    })\n","    validation: Dataset({\n","        features: ['id', 'document', 'doc_bio_tags'],\n","        num_rows: 50\n","    })\n","})\n"]}]},{"cell_type":"code","source":["train_se2017_df = pd.DataFrame(se2017_ds['train'])\n","validation_se2017_df = pd.DataFrame(se2017_ds['validation'])\n","test_se2017_df = pd.DataFrame(se2017_ds['test'])"],"metadata":{"id":"LBkaYKwRwYjg","executionInfo":{"status":"ok","timestamp":1714621737963,"user_tz":-420,"elapsed":16,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["all_se2017_df = pd.concat([train_se2017_df, validation_se2017_df, test_se2017_df], ignore_index=True)"],"metadata":{"id":"3LJOzKG4whPF","executionInfo":{"status":"ok","timestamp":1714621737963,"user_tz":-420,"elapsed":15,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["all_se2017_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"OCsuWTycwqNE","executionInfo":{"status":"ok","timestamp":1714621737963,"user_tz":-420,"elapsed":15,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"8bf4dcd8-b035-4c86-8c94-30b22d3f6ffd"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  id                                           document  \\\n","0  S0370269304007208  [It, is, well, known, that, one, of, the, long...   \n","1  S0032386109006612  [In, contrast, with, polymers,, which, are, ty...   \n","2  S1071581916300854  [We, have, developed, a, systematic,, quantifi...   \n","3  S0997754612001318  [Many, applications, in, fluid, mechanics, hav...   \n","4  S0038092X15001681  [For, the, reverse, current, analysis,, for, b...   \n","\n","                                        doc_bio_tags  \n","0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...  \n","1  [O, O, O, B, O, O, O, B, I, I, I, I, B, O, B, ...  \n","2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","3  [O, O, O, B, I, O, O, O, B, I, O, O, O, O, O, ...  \n","4  [O, O, B, I, O, O, O, O, O, O, O, O, O, O, O, ...  "],"text/html":["\n","  <div id=\"df-0fb876cf-036c-44d4-835c-bab669f3829c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>doc_bio_tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S0370269304007208</td>\n","      <td>[It, is, well, known, that, one, of, the, long...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S0032386109006612</td>\n","      <td>[In, contrast, with, polymers,, which, are, ty...</td>\n","      <td>[O, O, O, B, O, O, O, B, I, I, I, I, B, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S1071581916300854</td>\n","      <td>[We, have, developed, a, systematic,, quantifi...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S0997754612001318</td>\n","      <td>[Many, applications, in, fluid, mechanics, hav...</td>\n","      <td>[O, O, O, B, I, O, O, O, B, I, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S0038092X15001681</td>\n","      <td>[For, the, reverse, current, analysis,, for, b...</td>\n","      <td>[O, O, B, I, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fb876cf-036c-44d4-835c-bab669f3829c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0fb876cf-036c-44d4-835c-bab669f3829c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0fb876cf-036c-44d4-835c-bab669f3829c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1f8b8f31-3dae-478b-9f2b-15e5aab8a34a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f8b8f31-3dae-478b-9f2b-15e5aab8a34a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1f8b8f31-3dae-478b-9f2b-15e5aab8a34a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"all_se2017_df","summary":"{\n  \"name\": \"all_se2017_df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 495,\n        \"samples\": [\n          \"S0021999116303291\",\n          \"S0370269304009530\",\n          \"S0022311515301069\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doc_bio_tags\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Tạo dataframe mới\n","se2017_df = pd.DataFrame()\n","\n","# Cột abstract là ghép các phần tử trong document thành chuỗi\n","se2017_df['abstract'] = all_se2017_df['document'].apply(' '.join)\n","\n","# Cột keyword lấy các từ có doc_bio_tags = 'B'\n","se2017_df['keyword'] = all_se2017_df.apply(lambda x: keyword_list(x), axis=1)"],"metadata":{"id":"HppLke2hwp5X","executionInfo":{"status":"ok","timestamp":1714621738720,"user_tz":-420,"elapsed":771,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["se2017_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"m9szR9epw8Cn","executionInfo":{"status":"ok","timestamp":1714621738721,"user_tz":-420,"elapsed":20,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"56543022-e7e9-4353-8446-186f9faf3e1a"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  It is well known that one of the long standing...   \n","1  In contrast with polymers, which are typically...   \n","2  We have developed a systematic, quantified und...   \n","3  Many applications in fluid mechanics have show...   \n","4  For the reverse current analysis, for both sce...   \n","\n","                                             keyword  \n","0  [quantum theory calculation of the static ener...  \n","1  [the reaction of a gaseous carbon compound as ...  \n","2  [designing services,, input devices, bespoke a...  \n","3  [surface suction, numerical and asymptotic app...  \n","4  [standard silicon system, high efficiency syst...  "],"text/html":["\n","  <div id=\"df-32d9a3f0-6ff0-400f-9bf5-9072284c4e38\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>It is well known that one of the long standing...</td>\n","      <td>[quantum theory calculation of the static ener...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In contrast with polymers, which are typically...</td>\n","      <td>[the reaction of a gaseous carbon compound as ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We have developed a systematic, quantified und...</td>\n","      <td>[designing services,, input devices, bespoke a...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Many applications in fluid mechanics have show...</td>\n","      <td>[surface suction, numerical and asymptotic app...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>For the reverse current analysis, for both sce...</td>\n","      <td>[standard silicon system, high efficiency syst...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32d9a3f0-6ff0-400f-9bf5-9072284c4e38')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-32d9a3f0-6ff0-400f-9bf5-9072284c4e38 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-32d9a3f0-6ff0-400f-9bf5-9072284c4e38');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-20ae5cc8-fd6e-4cda-ac9f-f5e15adbe5e4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-20ae5cc8-fd6e-4cda-ac9f-f5e15adbe5e4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-20ae5cc8-fd6e-4cda-ac9f-f5e15adbe5e4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"se2017_df","summary":"{\n  \"name\": \"se2017_df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 494,\n        \"samples\": [\n          \"DPD was first proposed in order to recover the properties of isotropy (and Galilean invariance) that were broken in the so-called lattice-gas automata method [5]. In DPD, each body is regarded as a coarse-grained particle. These particles interact in a soft (and short-ranged) potential, allowing larger integration timesteps than would be possible in MD, while simultaneously decreasing the number of degrees of freedom required. As in Langevin dynamics, a thermostat consisting of well-balanced damping and stochastic terms is applied to each particle. However, unlike in Langevin dynamics, both terms are pairwise and the damping term is based on relative velocities, leading to the conservation of both the angular momentum and the linear momentum. The property of Galilean invariance (i.e., the dependence on the relative velocity) makes DPD a profile-unbiased thermostat (PUT) [6,7] by construction and thus it is an ideal thermostat for nonequilibrium molecular dynamics (NEMD) [8]. The momentum is expected to propagate locally (while global momentum is conserved) and thus the correct hydrodynamics is expected in DPD [8], as demonstrated previously in [9]. Due to the aforementioned properties, DPD has been widely used to recover thermodynamic, dynamical, and rheological properties of complex fluids, with applications in polymer solutions [10], colloidal suspensions [11], multiphase flows [12], and biological systems [13]. DPD has been compared with Langevin dynamics for out-of-equilibrium simulations of polymeric systems in [14], where as expected the correct dynamic fluctuations of the polymers were obtained with the former but not with the latter.\",\n          \"If signals suggesting supersymmetry (SUSY) are discovered at the LHC then it will be vital to measure the spins of the new particles to demonstrate that they are indeed the predicted super-partners. A method is discussed by which the spins of some of the SUSY particles can be determined. Angular distributions in sparticle decays lead to charge asymmetry in lepton-jet invariant mass distributions. The size of the asymmetry is proportional to the primary production asymmetry between squarks and anti-squarks. Monte Carlo simulations are performed for a particular mSUGRA model point at the LHC. The resultant asymmetry distributions are consistent with a spin-0 slepton and a spin-12\\u03c7\\u02dc20, but are not consistent with both particles being scalars.\",\n          \"The fluence of each capsule was determined by using activation monitor sets. These monitor sets consist of different metal wire pieces that have an activation reaction at a specific energy range. The different activation energies are chosen in such a way that the spectrum can be reconstructed. In BODEX, each capsule contained a flux monitor set on the \\u2018back side\\u2019 (as seen from the core) and one on the front side, positioned at the central height of the capsules. Additionally, one detector was placed at the top and one at the bottom, resulting in a total of 6 monitor sets per leg. The fluence in each capsule was determined as the average between the two flux monitor located in each capsule. The sets have been analysed by determining the activation of each wire piece, which indicates the fluence of a specific energy range. Table 3 show the values of the fluences for the two capsules containing molybdenum.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["print(se2017_df.iloc[0]['abstract'])\n","print(se2017_df.iloc[0]['keyword'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XDR4XJww1p5","executionInfo":{"status":"ok","timestamp":1714621738721,"user_tz":-420,"elapsed":18,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"b43cce01-6c6c-4a95-c7b3-49d8e92cffc5"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed [1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type [2] (1)L=14w2−12w−FμνaFaμν, where Fμνa=∂μAνa−∂νAμa+gfabcAμbAνc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=εμναβ∂μAναβ. The Aναβ equation of motion leads to (3)εμναβ∂βw−−FγδaFaγδ=0, which is then integrated to (4)w=−FμνaFaμν+M. It is easy to verify that the Aaμ equation of motion leads us to (5)∇μFaμν+MFaμν−FαβbFbαβ=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=−14FμνaFaμν+M2−FμνaFaμν. Spherically symmetric solutions of Eq. (5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior [1]. It is well known that the square root part describes string like solutions [3,4].\n","['quantum theory calculation of the static energy between two charges', 'understanding the confinement physics from first principles.', 'string like solutions', 'the effective Lagrangian', 'Spherically symmetric solutions', 'confining part.', 'Coulomb piece', 'Aaμ equation of motion', 'develop analytical approaches', 'Aναβ equation of motion leads', 'spontaneous symmetry breaking of scale invariance']\n"]}]},{"cell_type":"markdown","source":["### Full Data"],"metadata":{"id":"h4brzEflxgbu"}},{"cell_type":"code","source":["se2017_df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kUnIMADykZS","executionInfo":{"status":"ok","timestamp":1714621738722,"user_tz":-420,"elapsed":18,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"b12c0e6a-47b0-4c55-9d6e-257d293da4fd"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['abstract', 'keyword'], dtype='object')"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["# dataset_names\n","cols = list(se2017_df.columns)\n","# print(cols)"],"metadata":{"id":"0tgVECL7x2RZ","executionInfo":{"status":"ok","timestamp":1714621786178,"user_tz":-420,"elapsed":476,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["kp20k_df_cols = kp20k_df[cols]\n","inspec_df_cols = inspec_df[cols]\n","se2010_df_cols = se2010_df[cols]\n","se2017_df_cols = se2017_df[cols]"],"metadata":{"id":"F6igt2iqyz1R","executionInfo":{"status":"ok","timestamp":1714621789572,"user_tz":-420,"elapsed":649,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["merged_df = pd.concat([inspec_df_cols, se2010_df_cols, se2017_df_cols], ignore_index=True)"],"metadata":{"id":"-AjRWGWR7iuh","executionInfo":{"status":"ok","timestamp":1714621790510,"user_tz":-420,"elapsed":2,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["merged_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"GUq6otuO7q6w","executionInfo":{"status":"ok","timestamp":1714621792091,"user_tz":-420,"elapsed":4,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"5ceafddf-64a3-489a-bbf8-8eee2cc1cbf5"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  A conflict between language and atomistic info...   \n","1  Selective representing and world-making We dis...   \n","2  Does classicism explain universality ? Argumen...   \n","3  Separate accounts go mainstream -LSB- investme...   \n","4  Evolving receptive-field controllers for mobil...   \n","\n","                                             keyword  \n","0  [Content Atomism, philosophy of mind, IBS, cog...  \n","1  [realism, Selective representing, selective re...  \n","2  [human cognition, connectionist models, univer...  \n","3           [independent money managers, investment]  \n","4  [nonlinear interactions, evolutionary methods,...  "],"text/html":["\n","  <div id=\"df-1fda8df5-67f2-4e40-a02e-59a266b9436e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A conflict between language and atomistic info...</td>\n","      <td>[Content Atomism, philosophy of mind, IBS, cog...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Selective representing and world-making We dis...</td>\n","      <td>[realism, Selective representing, selective re...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does classicism explain universality ? Argumen...</td>\n","      <td>[human cognition, connectionist models, univer...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Separate accounts go mainstream -LSB- investme...</td>\n","      <td>[independent money managers, investment]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Evolving receptive-field controllers for mobil...</td>\n","      <td>[nonlinear interactions, evolutionary methods,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fda8df5-67f2-4e40-a02e-59a266b9436e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1fda8df5-67f2-4e40-a02e-59a266b9436e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1fda8df5-67f2-4e40-a02e-59a266b9436e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cfc16a13-468d-424e-9b90-1e1f3d781f6e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfc16a13-468d-424e-9b90-1e1f3d781f6e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cfc16a13-468d-424e-9b90-1e1f3d781f6e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"merged_df","summary":"{\n  \"name\": \"merged_df\",\n  \"rows\": 2744,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2738,\n        \"samples\": [\n          \"Stochastic optimization of acoustic response - a numerical and experimental comparison The objective of the work presented is to compare results from numerical optimization with experimental data and to highlight and discuss the differences between two fundamentally different optimization methods . The problem domain is minimization of acoustic emission and the structure used in the work is a closed cylinder with forced vibration of one end . The optimization method used in this paper is simulated annealing -LRB- SA -RRB- , a stochastic method . The results are compared with those from a gradient-based method used on the same structure in an earlier paper -LRB- Tinnsten , 2000 -RRB-\",\n          \"Hysteretic threshold logic and quasi-delay insensitive asynchronous design We introduce the class of hysteretic linear-threshold -LRB- HLT -RRB- logic functions as a novel extension of linear threshold logic , and prove their general applicability for constructing state-holding Boolean functions . We then demonstrate a fusion of HLT logic with the quasi-delay insensitive style of asynchronous circuit design , complete with logical design examples . Future research directions are also identified\",\n          \"On a general constitutive description for the inelastic and failure behavior of fibrous laminates . II . Laminate theory and applications For pt . I see ibid. , pp. 1159-76 . The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms . The constitutive relationship is established micromechanically , through layer-by-layer analysis . Namely , only the properties of the constituent fiber and matrix materials of the composites are required as input data . In the previous part lamina theory was presented . Three fundamental quantities of the laminae , i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix , with different fiber preform -LRB- including woven , braided , and knitted fabric -RRB- reinforcements were explicitly obtained by virtue of the bridging micromechanics model . In this paper , the laminate stress analysis is shown . The purpose of this analysis is to determine the load shared by each lamina in the laminate , so that the lamina theory can be applied . Incorporation of the constitutive equations into an FEM software package is illustrated . A number of application examples are given to demonstrate the efficiency of the constitutive theory . The predictions made include : failure envelopes of multidirectional laminates subjected to biaxial in-plane loads , thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate , S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue , and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["len(merged_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUQp959I7xSi","executionInfo":{"status":"ok","timestamp":1714621794882,"user_tz":-420,"elapsed":3,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"eff6ed34-6d8f-43b2-a8f6-a135bf044f64"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2744"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["print(merged_df.iloc[0]['abstract'])\n","print(merged_df.iloc[0]['keyword'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUUkruY28q-0","executionInfo":{"status":"ok","timestamp":1714621795628,"user_tz":-420,"elapsed":2,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"1986916c-e701-4a5f-b776-3923f1fbcdac"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\n","['Content Atomism', 'philosophy of mind', 'IBS', 'cognitive states', 'LOT', 'beliefs', 'desires', 'Language of Thought']\n"]}]},{"cell_type":"markdown","source":["## Prepare Data"],"metadata":{"id":"UhHxv8TVbNo4"}},{"cell_type":"markdown","source":["### Main"],"metadata":{"id":"9vpMR5BD4fVV"}},{"cell_type":"markdown","source":["*Nên hay không nên bỏ title vào abstract*"],"metadata":{"id":"3RAVVf9xxNUU"}},{"cell_type":"code","source":["bert_model_name = \"google-bert/bert-base-uncased\""],"metadata":{"id":"GwCMs5vKJuvn","executionInfo":{"status":"ok","timestamp":1714621808136,"user_tz":-420,"elapsed":1070,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"],"metadata":{"id":"LRxQ8esRJt0D","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["0df9d3df214346f787b290a2310948d9","58d8e5d6b2a84d34ba0f58ace8274856","dc7c37f4d54540e1903f27ef18a35634","6c66bea2b4a2496ebb9b32827f9d182b","c094af9bca8e4c118d8246e0f4356a64","37c80adea0964de88faefe492a3d23f9","9a71af8dd1714817addc8dc2649fa054","5a2deb70746b4b4ebf4596facef53c76","c3d952e4a8524e0c854d0275875e2b39","105ea0ec11d24578880f76705dad0d82","39f7820db1c34b8891886123c6160bff","2cc1bfc9b1c648ffab9a2687d4a751e7","17c5adc983c64c60a60dc6cffbd9659e","96a648e6594c404b91df95e9b937fd2e","9b17588eaca84e95a3f4462835e95dab","4ea990e887ae44c19e0c10f9ac53a3fb","1c5f8dbf1d2748babdac9f95c29ff1ce","581ed6aae14e400084795bc4f2b503d0","7b0b0e7c71634baf8ba8b9bee18279cf","aeee6896b65a4f178067c1115544f4f1","9d12ae979e994ace8405e4b9334da70c","6dbd3f2b9bb44b1db994224a28b9485c","41a694f139b1423c820dc6405f267cf9","cab16b0cc3924216bb3947f0652136a0","e2045f0eeb294778a2a81c881a53b311","d2c3d45857f54685ba348fae60c1b777","f4c157927bbe45289d439abe9a7240f4","4d977fbff9dc4c01994201fa25b8d468","b60ee8a711274cc0a723cdba239e05b8","fb676dcfdb774d798e1185a2efa96333","3f3d2fcacb9946578e35e366081daeda","4233a54f56904bf096ea0112ef98e320","fb38363f579941529f5de9f6f45528e5","c11ae8d992e045ec9d5c4928cefa2b02","494761c4d8b64f80bd9c69436b029ace","981976df85cf422395205200ab6a41c8","8c4bef2ae68742dc9a448743701e0695","c3b3422be59541cc9d05558aace32379","852b9ebaf275492d82ac31ee98384afe","f4568a803ff94a039ee7f3d71ed47737","55e2c16cd1cd47c1880a984c43f54ccb","93f11afb14314306817bdf54489488b2","a011c2245be84c06b0e95a0820490310","877f6de33f9c43f2b32844ca44ad7e2c"]},"executionInfo":{"status":"ok","timestamp":1714621811418,"user_tz":-420,"elapsed":3284,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"862fd55c-e2bf-4584-d2c1-f304338d2406"},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0df9d3df214346f787b290a2310948d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc1bfc9b1c648ffab9a2687d4a751e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a694f139b1423c820dc6405f267cf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c11ae8d992e045ec9d5c4928cefa2b02"}},"metadata":{}}]},{"cell_type":"code","source":["merged_df.iloc[0]['abstract']"],"metadata":{"id":"DlGYtpCyKawO","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1714621811419,"user_tz":-420,"elapsed":8,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"ac9f7c16-bfe9-4162-9d6e-8f53bd807362"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# encoding = tokenizer(merged_df.iloc[0]['abstract'], return_tensors='pt')\n","# decoding = [tokenizer.decode(idx) for idx in encoding['input_ids'][0]]\n","# print(decoding[1:-1])\n","# print(len(decoding[1:-1]))"],"metadata":{"id":"q3c98p5nKKeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_BIO_labels(abstract_tokens, keyword_tokens):\n","    # print(\"abstract_tokens\", abstract_tokens)\n","    # print(\"keyword_tokens\", keyword_tokens)\n","    BIO_labels = []\n","\n","    # Tạo một danh sách các từ khóa mở đầu từ danh sách keyword_tokens\n","    start_keywords = [token[0] for token in keyword_tokens]\n","    # print(start_keywords)\n","\n","\n","    # Duyệt qua từng từ trong abstract_tokens\n","    idx_token = 0\n","    while idx_token<len(abstract_tokens):\n","        token = abstract_tokens[idx_token]\n","        # print(1)\n","        # print(\"token\", token)\n","        # kiểm tra token có phải là token mở đầu không\n","        if token in start_keywords:\n","          # print(2)\n","          # tìm các vị trí trong keyword list bắt đầu bằng token trên\n","          # print(\"start_keywords\", start_keywords)\n","          idx_keywords =[i for i, keyword in enumerate(start_keywords) if keyword==token]\n","          # print(\"idx_keywords\",idx_keywords)\n","          # nếu không có keyword nào match thì sẽ gán cho token đó nhãn O\n","          correct = 0\n","          # với mỗi vị trí trong danh sách vị trí đã tìm được ở trên\n","          for idx_keyword in idx_keywords:\n","            # print(3)\n","            # lấy keyword match với index đã tìm thấy\n","            keyword_match = keyword_tokens[idx_keyword]\n","            # khởi tạo các biến vị trí của keyword_match và abstract_tokens đang xét\n","            k = 1\n","            new_idx = idx_token + 1\n","            if len(keyword_match) == 1 or len(abstract_tokens)==new_idx:\n","              BIO_labels.append('B')\n","              # print(BIO_labels)\n","              correct = 1\n","              break\n","            # print(len(abstract_tokens))\n","            # print(new_idx)\n","            # print(len(keyword_match))\n","            # print(k)\n","            # print(abstract_tokens[new_idx])\n","            # print(keyword_match[k])\n","            while abstract_tokens[new_idx]==keyword_match[k]:\n","              k+=1\n","              new_idx+=1\n","              if (k==len(keyword_match) or new_idx==len(abstract_tokens)):\n","                # print(4)\n","                BIO_labels.append('B')\n","                BIO_labels.extend(['I']*(k-1))\n","                idx_token += k-1\n","                # print(BIO_labels)\n","                correct = 1\n","                break\n","            else:\n","              BIO_labels.append('B')\n","              # print(BIO_labels)\n","              correct = 1\n","              break\n","            if correct==1:\n","              # print(5)\n","              # print('correct')\n","              break\n","        else:\n","            BIO_labels.append('O')\n","            # print(BIO_labels)\n","        idx_token += 1\n","        # print(BIO_labels)\n","        # print(len(BIO_labels))\n","        # print(idx_token)\n","\n","    return BIO_labels\n","\n","# Input\n","# abstract_token = ['selective', 'representing', 'and', 'world', 'making', 'we', 'discuss', 'the', 'thesis', 'of', 'selective', 'representing', 'the', 'idea', 'that', 'the', 'contents', 'of', 'the', 'mental', 'representations', 'had', 'by', 'organisms', 'are', 'highly', 'constrained', 'by', 'the', 'biological', 'niches', 'within', 'which', 'the', 'organisms', 'evolved', '.', 'while', 'such', 'a', 'thesis', 'has', 'been', 'defended', 'by', 'several', 'authors', 'elsewhere', ',', 'our', 'primary', 'concern', 'here', 'is', 'to', 'take', 'up', 'the', 'issue', 'of', 'the', 'compatibility', 'of', 'selective', 'representing', 'and', 'realism', '.', 'we', 'hope', 'to', 'show', 'three', 'things', '.', 'first', ',', 'that', 'the', 'notion', 'of', 'selective', 'representing', 'is', 'fully', 'consistent', 'with', 'the', 'realist', 'idea', 'of', 'a', 'mind', 'independent', 'world', '.', 'second', ',', 'that', 'not', 'only', 'are', 'these', 'two', 'consistent', ',', 'but', 'that', 'the', 'latter', 'lrb', 'the', 'realist', 'conception', 'of', 'a', 'mind', 'independent', 'world', 'rrb', 'provides', 'the', 'most', 'powerful', 'perspective', 'from', 'which', 'to', 'motivate', 'and', 'understand', 'the', 'differing', 'perceptual', 'and', 'cognitive', 'profiles', 'themselves', '.', 'third', ',', 'that', 'the', 'lrb', 'genuine', 'and', 'important', 'rrb', 'sense', 'in', 'which', 'organism', 'and', 'environment', 'may', 'together', 'constitute', 'an', 'integrated', 'system', 'of', 'scientific', 'interest', 'poses', 'no', 'additional', 'threat', 'to', 'the', 'realist', 'conception']\n","# keyword_tokens = [['mental', 'representations'], ['selective', 'representing'], ['selective', 'representing'], ['organisms'], ['cognitive', 'profiles'], ['realism']]\n","\n","# abstract_token = ['temp', 'it', 'chief', 'rallies', 'troops', 'lsb', 'mori', 'rsb', 'the', 'appointment', 'of', 'a', 'highly', 'qualified', 'interim', 'it', 'manager', 'enabled', 'market', 'research', 'company', 'mori', 'to', 'rapidly', 'restructure', 'its', 'it', 'department', '.', 'now', 'the', 'resulting', 'improvements', 'are', 'allowing', 'it', 'to', 'support', 'an', 'increasing', 'role', 'for', 'technology', 'in', 'the', 'assimilation', 'and', 'analysis', 'of', 'market', 'research']\n","# keyword_tokens = [['mori'], ['interim', 'it', 'manager'], ['market', 'research', 'company']]\n","# Tạo BIO labels\n","# BIO_labels = generate_BIO_labels(abstract_token, keyword_tokens)\n","\n","# In kết quả\n","# print(BIO_labels)"],"metadata":{"id":"m-vq5HTpY8HW","executionInfo":{"status":"ok","timestamp":1714622943369,"user_tz":-420,"elapsed":532,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def token_process(token):\n","  if (token[-1]=='.' and len(token)!=1 and '.' not in token[:-1]):\n","    # print('after', [token[:-1], '.'])\n","    return token_process(token[:-1])+['.']\n","  elif token[-1] in [',', ':', '!', '?', ';', '\"', \"'\", ')', ']', '}'] and token[0] in [\"'\", '\"', '(', '[', '{'] and len(token)>2:\n","    # print('after', [token[0], token[1:-1], token[-1]])\n","    return [token[0], token[1:-1], token[-1]]\n","  elif token[-1] in [',', ':', '!', '?', ';', '\"', \"'\", ')', ']', '}'] and len(token)!=1:\n","    # print('after', [token[:-1], token[-1]])\n","    return [token[:-1], token[-1]]\n","  elif token[0] in [\"'\", '\"', '(', '[', '{'] and len(token)!=1:\n","    # print('after', [token[0], token[1:]])\n","    return [token[0], token[1:]]\n","  return [token]\n","\n","\n","def mySplit(text):\n","  # chia text thành danh sách các từ bằng dấu cách ' '\n","  text = text.lower()\n","  text = text.replace('-', ' ')\n","  tokens = text.split(' ')\n","  # loại bỏ các token rỗng và strip token\n","  tokens = [token.strip() for token in tokens if token!=\"\"]\n","  # nếu từ đó kết thúc bằng dấu chấm '.' và bên trong từ đó không còn dấu chấm nào thì sẽ tách ra thành 2 từ đó và dấu chấm\n","  dot_tokens = []\n","  for token in tokens:\n","    # print('before', token)\n","    # print(token_process(token))\n","    dot_tokens.extend(token_process(token))\n","  return dot_tokens\n","\n","def tokenizerSplit(text):\n","  encoding = tokenizer(text, return_tensors='pt')\n","  decoding = [tokenizer.decode(idx) for idx in encoding['input_ids'][0]]\n","  return decoding[1:-1]\n","\n","def convInputOutput(abstract, keyword):\n","  # # chia abstract thành danh sách các từ bằng dấu cách ' '\n","  # abstract = abstract.replace('-', ' ')\n","  # tokens = abstract.split(' ')\n","  # # loại bỏ các token rỗng và strip token\n","  # tokens = [token.strip() for token in tokens if token!=\"\"]\n","  # # nếu từ đó kết thúc bằng dấu chấm '.' và bên trong từ đó không còn dấu chấm nào thì sẽ tách ra thành 2 từ đó và dấu chấm\n","  # dot_tokens = []\n","  # for token in tokens:\n","  #   # print('before', token)\n","  #   # print(token_process(token))\n","  #   dot_tokens.extend(token_process(token))\n","\n","  abstract_tokens = tokenizerSplit(abstract)\n","  keyword_tokens = [tokenizerSplit(keyword_iter) for keyword_iter in keyword]\n","  BIO_labels = generate_BIO_labels(abstract_tokens, keyword_tokens)\n","  return abstract_tokens, BIO_labels\n","\n","kq1 = list(convInputOutput(merged_df.iloc[0]['abstract'], merged_df.iloc[0]['keyword']))\n","print(len(kq1[0]), len(kq1[1]))\n","kq2 = list(convInputOutput(merged_df.iloc[1]['abstract'], merged_df.iloc[1]['keyword']))\n","print(len(kq2[0]), len(kq2[1]))"],"metadata":{"id":"7dlmp0EhOvGg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714622947673,"user_tz":-420,"elapsed":4,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"1dec1268-b453-4dd3-e80a-c6a814f83e32"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["265 265\n","196 196\n"]}]},{"cell_type":"code","source":["merged_df.iloc[0]['abstract']"],"metadata":{"id":"XT7ohOj034Wu","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1714622949592,"user_tz":-420,"elapsed":6,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"67d47c17-64ec-4580-a06f-f3136f82db03"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["Từ df có 2 cột là abstract (abstract của bài báo KH) và keyword (danh sách các keyword nằm trong bài báo đó).\n","\n","Chuyển thành df mới có 2 cột lần lượt là input (từng chữ trong abstract sau khi được tách riêng ra bằng dấu cách hoặc dấu chấm) và output (ứng với mỗi từ trong abstract đó sẽ có một nhãn thuộc BIO)."],"metadata":{"id":"Flgf3PT8Numy"}},{"cell_type":"code","source":["merged_df"],"metadata":{"id":"dlecTK40XSXL","colab":{"base_uri":"https://localhost:8080/","height":597},"executionInfo":{"status":"ok","timestamp":1714622953262,"user_tz":-420,"elapsed":505,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"c6cb6017-09f5-4287-99a8-7d03ba3111a7"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               abstract  \\\n","0     A conflict between language and atomistic info...   \n","1     Selective representing and world-making We dis...   \n","2     Does classicism explain universality ? Argumen...   \n","3     Separate accounts go mainstream -LSB- investme...   \n","4     Evolving receptive-field controllers for mobil...   \n","...                                                 ...   \n","2739  Similar numerical oscillations to those descri...   \n","2740  It is interesting to quantify the effects of t...   \n","2741  Numerical simulation of the gas flow through s...   \n","2742  After all micro elements reach a relaxed stead...   \n","2743  The test cases confirm that the high-order dis...   \n","\n","                                                keyword  \\\n","0     [Content Atomism, philosophy of mind, IBS, cog...   \n","1     [realism, Selective representing, selective re...   \n","2     [human cognition, connectionist models, univer...   \n","3              [independent money managers, investment]   \n","4     [nonlinear interactions, evolutionary methods,...   \n","...                                                 ...   \n","2739  [Lagrangian description, IBM kernels, kernel i...   \n","2740  [gas exchange, Schmidt, chemical reaction, pra...   \n","2741  [(DSMC), direct simulation Monte Carlo method,...   \n","2742  [velocity, polynomials, Irving–Kirkwood relati...   \n","2743  [meshes, true surface, cardiac problem,, mesh ...   \n","\n","                                        Abstract_inputs  \\\n","0     [a, conflict, between, language, and, atom, ##...   \n","1     [selective, representing, and, world, -, makin...   \n","2     [does, classic, ##ism, explain, universal, ##i...   \n","3     [separate, accounts, go, mainstream, -, l, ##s...   \n","4     [evolving, rec, ##eptive, -, field, controller...   \n","...                                                 ...   \n","2739  [similar, numerical, os, ##ci, ##llation, ##s,...   \n","2740  [it, is, interesting, to, quan, ##tify, the, e...   \n","2741  [numerical, simulation, of, the, gas, flow, th...   \n","2742  [after, all, micro, elements, reach, a, relaxe...   \n","2743  [the, test, cases, confirm, that, the, high, -...   \n","\n","                                             BIO_labels  \n","0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","1     [B, I, O, O, O, O, O, O, O, O, O, B, I, O, O, ...  \n","2     [O, B, I, O, B, I, O, O, O, O, O, B, I, I, I, ...  \n","3     [O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...  \n","4     [O, O, O, O, O, O, O, B, I, O, O, O, B, I, O, ...  \n","...                                                 ...  \n","2739  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...  \n","2740  [O, O, O, O, O, O, O, O, O, O, B, O, O, O, B, ...  \n","2741  [B, I, O, O, B, I, O, O, O, O, O, O, O, O, O, ...  \n","2742  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","2743  [O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...  \n","\n","[2744 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-a388e18f-298b-4cca-80ea-263cd5f38626\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>keyword</th>\n","      <th>Abstract_inputs</th>\n","      <th>BIO_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A conflict between language and atomistic info...</td>\n","      <td>[Content Atomism, philosophy of mind, IBS, cog...</td>\n","      <td>[a, conflict, between, language, and, atom, ##...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Selective representing and world-making We dis...</td>\n","      <td>[realism, Selective representing, selective re...</td>\n","      <td>[selective, representing, and, world, -, makin...</td>\n","      <td>[B, I, O, O, O, O, O, O, O, O, O, B, I, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does classicism explain universality ? Argumen...</td>\n","      <td>[human cognition, connectionist models, univer...</td>\n","      <td>[does, classic, ##ism, explain, universal, ##i...</td>\n","      <td>[O, B, I, O, B, I, O, O, O, O, O, B, I, I, I, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Separate accounts go mainstream -LSB- investme...</td>\n","      <td>[independent money managers, investment]</td>\n","      <td>[separate, accounts, go, mainstream, -, l, ##s...</td>\n","      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Evolving receptive-field controllers for mobil...</td>\n","      <td>[nonlinear interactions, evolutionary methods,...</td>\n","      <td>[evolving, rec, ##eptive, -, field, controller...</td>\n","      <td>[O, O, O, O, O, O, O, B, I, O, O, O, B, I, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2739</th>\n","      <td>Similar numerical oscillations to those descri...</td>\n","      <td>[Lagrangian description, IBM kernels, kernel i...</td>\n","      <td>[similar, numerical, os, ##ci, ##llation, ##s,...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2740</th>\n","      <td>It is interesting to quantify the effects of t...</td>\n","      <td>[gas exchange, Schmidt, chemical reaction, pra...</td>\n","      <td>[it, is, interesting, to, quan, ##tify, the, e...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, B, O, O, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2741</th>\n","      <td>Numerical simulation of the gas flow through s...</td>\n","      <td>[(DSMC), direct simulation Monte Carlo method,...</td>\n","      <td>[numerical, simulation, of, the, gas, flow, th...</td>\n","      <td>[B, I, O, O, B, I, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2742</th>\n","      <td>After all micro elements reach a relaxed stead...</td>\n","      <td>[velocity, polynomials, Irving–Kirkwood relati...</td>\n","      <td>[after, all, micro, elements, reach, a, relaxe...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2743</th>\n","      <td>The test cases confirm that the high-order dis...</td>\n","      <td>[meshes, true surface, cardiac problem,, mesh ...</td>\n","      <td>[the, test, cases, confirm, that, the, high, -...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2744 rows × 4 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a388e18f-298b-4cca-80ea-263cd5f38626')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a388e18f-298b-4cca-80ea-263cd5f38626 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a388e18f-298b-4cca-80ea-263cd5f38626');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e875057e-35a9-440f-883c-ce945cfc579e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e875057e-35a9-440f-883c-ce945cfc579e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e875057e-35a9-440f-883c-ce945cfc579e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_7dc006ec-c56e-46b1-b068-df0a819c2b9f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('merged_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_7dc006ec-c56e-46b1-b068-df0a819c2b9f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('merged_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"merged_df","summary":"{\n  \"name\": \"merged_df\",\n  \"rows\": 2744,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2738,\n        \"samples\": [\n          \"Stochastic optimization of acoustic response - a numerical and experimental comparison The objective of the work presented is to compare results from numerical optimization with experimental data and to highlight and discuss the differences between two fundamentally different optimization methods . The problem domain is minimization of acoustic emission and the structure used in the work is a closed cylinder with forced vibration of one end . The optimization method used in this paper is simulated annealing -LRB- SA -RRB- , a stochastic method . The results are compared with those from a gradient-based method used on the same structure in an earlier paper -LRB- Tinnsten , 2000 -RRB-\",\n          \"Hysteretic threshold logic and quasi-delay insensitive asynchronous design We introduce the class of hysteretic linear-threshold -LRB- HLT -RRB- logic functions as a novel extension of linear threshold logic , and prove their general applicability for constructing state-holding Boolean functions . We then demonstrate a fusion of HLT logic with the quasi-delay insensitive style of asynchronous circuit design , complete with logical design examples . Future research directions are also identified\",\n          \"On a general constitutive description for the inelastic and failure behavior of fibrous laminates . II . Laminate theory and applications For pt . I see ibid. , pp. 1159-76 . The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms . The constitutive relationship is established micromechanically , through layer-by-layer analysis . Namely , only the properties of the constituent fiber and matrix materials of the composites are required as input data . In the previous part lamina theory was presented . Three fundamental quantities of the laminae , i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix , with different fiber preform -LRB- including woven , braided , and knitted fabric -RRB- reinforcements were explicitly obtained by virtue of the bridging micromechanics model . In this paper , the laminate stress analysis is shown . The purpose of this analysis is to determine the load shared by each lamina in the laminate , so that the lamina theory can be applied . Incorporation of the constitutive equations into an FEM software package is illustrated . A number of application examples are given to demonstrate the efficiency of the constitutive theory . The predictions made include : failure envelopes of multidirectional laminates subjected to biaxial in-plane loads , thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate , S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue , and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keyword\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract_inputs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BIO_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["# def new_function(a, b):\n","#   print(a)\n","#   print(b)\n","#   return a, b"],"metadata":{"id":"cDO6263sYAFg","executionInfo":{"status":"ok","timestamp":1714621924676,"user_tz":-420,"elapsed":487,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["merged_df['Abstract_inputs'] = merged_df.apply(lambda row: convInputOutput(row['abstract'], row['keyword'])[0], axis=1)\n","merged_df['BIO_labels'] = merged_df.apply(lambda row: convInputOutput(row['abstract'], row['keyword'])[1], axis=1)"],"metadata":{"id":"0WMXe0rXUSzY","executionInfo":{"status":"ok","timestamp":1714623001328,"user_tz":-420,"elapsed":45520,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["final_df = merged_df[['abstract', 'Abstract_inputs', 'BIO_labels']]"],"metadata":{"id":"tNHIY0HCaSha","executionInfo":{"status":"ok","timestamp":1714623001328,"user_tz":-420,"elapsed":3,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["final_df"],"metadata":{"id":"rjKVMUk8al5L","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1714623001869,"user_tz":-420,"elapsed":543,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"cb2483eb-1b76-42cc-ebff-114b69d4a25d"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               abstract  \\\n","0     A conflict between language and atomistic info...   \n","1     Selective representing and world-making We dis...   \n","2     Does classicism explain universality ? Argumen...   \n","3     Separate accounts go mainstream -LSB- investme...   \n","4     Evolving receptive-field controllers for mobil...   \n","...                                                 ...   \n","2739  Similar numerical oscillations to those descri...   \n","2740  It is interesting to quantify the effects of t...   \n","2741  Numerical simulation of the gas flow through s...   \n","2742  After all micro elements reach a relaxed stead...   \n","2743  The test cases confirm that the high-order dis...   \n","\n","                                        Abstract_inputs  \\\n","0     [a, conflict, between, language, and, atom, ##...   \n","1     [selective, representing, and, world, -, makin...   \n","2     [does, classic, ##ism, explain, universal, ##i...   \n","3     [separate, accounts, go, mainstream, -, l, ##s...   \n","4     [evolving, rec, ##eptive, -, field, controller...   \n","...                                                 ...   \n","2739  [similar, numerical, os, ##ci, ##llation, ##s,...   \n","2740  [it, is, interesting, to, quan, ##tify, the, e...   \n","2741  [numerical, simulation, of, the, gas, flow, th...   \n","2742  [after, all, micro, elements, reach, a, relaxe...   \n","2743  [the, test, cases, confirm, that, the, high, -...   \n","\n","                                             BIO_labels  \n","0     [O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...  \n","1     [B, I, O, O, O, O, O, O, O, O, O, B, I, O, O, ...  \n","2     [O, B, I, O, B, I, O, O, O, O, O, B, I, I, I, ...  \n","3     [O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...  \n","4     [O, O, O, O, O, O, O, B, I, O, O, O, B, I, O, ...  \n","...                                                 ...  \n","2739  [O, O, B, O, O, O, O, O, O, O, O, O, O, O, B, ...  \n","2740  [O, O, O, O, O, O, O, O, O, O, B, O, O, O, B, ...  \n","2741  [B, I, O, B, B, I, O, O, O, O, O, O, O, O, O, ...  \n","2742  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n","2743  [O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...  \n","\n","[2744 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-cd8097a5-d709-4c64-84e9-d257b7a5035a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>Abstract_inputs</th>\n","      <th>BIO_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A conflict between language and atomistic info...</td>\n","      <td>[a, conflict, between, language, and, atom, ##...</td>\n","      <td>[O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Selective representing and world-making We dis...</td>\n","      <td>[selective, representing, and, world, -, makin...</td>\n","      <td>[B, I, O, O, O, O, O, O, O, O, O, B, I, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does classicism explain universality ? Argumen...</td>\n","      <td>[does, classic, ##ism, explain, universal, ##i...</td>\n","      <td>[O, B, I, O, B, I, O, O, O, O, O, B, I, I, I, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Separate accounts go mainstream -LSB- investme...</td>\n","      <td>[separate, accounts, go, mainstream, -, l, ##s...</td>\n","      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Evolving receptive-field controllers for mobil...</td>\n","      <td>[evolving, rec, ##eptive, -, field, controller...</td>\n","      <td>[O, O, O, O, O, O, O, B, I, O, O, O, B, I, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2739</th>\n","      <td>Similar numerical oscillations to those descri...</td>\n","      <td>[similar, numerical, os, ##ci, ##llation, ##s,...</td>\n","      <td>[O, O, B, O, O, O, O, O, O, O, O, O, O, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2740</th>\n","      <td>It is interesting to quantify the effects of t...</td>\n","      <td>[it, is, interesting, to, quan, ##tify, the, e...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, B, O, O, O, B, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2741</th>\n","      <td>Numerical simulation of the gas flow through s...</td>\n","      <td>[numerical, simulation, of, the, gas, flow, th...</td>\n","      <td>[B, I, O, B, B, I, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2742</th>\n","      <td>After all micro elements reach a relaxed stead...</td>\n","      <td>[after, all, micro, elements, reach, a, relaxe...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2743</th>\n","      <td>The test cases confirm that the high-order dis...</td>\n","      <td>[the, test, cases, confirm, that, the, high, -...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B, I, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2744 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd8097a5-d709-4c64-84e9-d257b7a5035a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cd8097a5-d709-4c64-84e9-d257b7a5035a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cd8097a5-d709-4c64-84e9-d257b7a5035a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c93664cf-d760-49dc-b733-6fd90d61026f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c93664cf-d760-49dc-b733-6fd90d61026f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c93664cf-d760-49dc-b733-6fd90d61026f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_80b94523-f564-4536-b1be-6c50bb9d2d2f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_80b94523-f564-4536-b1be-6c50bb9d2d2f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('final_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"final_df","summary":"{\n  \"name\": \"final_df\",\n  \"rows\": 2744,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2738,\n        \"samples\": [\n          \"Stochastic optimization of acoustic response - a numerical and experimental comparison The objective of the work presented is to compare results from numerical optimization with experimental data and to highlight and discuss the differences between two fundamentally different optimization methods . The problem domain is minimization of acoustic emission and the structure used in the work is a closed cylinder with forced vibration of one end . The optimization method used in this paper is simulated annealing -LRB- SA -RRB- , a stochastic method . The results are compared with those from a gradient-based method used on the same structure in an earlier paper -LRB- Tinnsten , 2000 -RRB-\",\n          \"Hysteretic threshold logic and quasi-delay insensitive asynchronous design We introduce the class of hysteretic linear-threshold -LRB- HLT -RRB- logic functions as a novel extension of linear threshold logic , and prove their general applicability for constructing state-holding Boolean functions . We then demonstrate a fusion of HLT logic with the quasi-delay insensitive style of asynchronous circuit design , complete with logical design examples . Future research directions are also identified\",\n          \"On a general constitutive description for the inelastic and failure behavior of fibrous laminates . II . Laminate theory and applications For pt . I see ibid. , pp. 1159-76 . The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms . The constitutive relationship is established micromechanically , through layer-by-layer analysis . Namely , only the properties of the constituent fiber and matrix materials of the composites are required as input data . In the previous part lamina theory was presented . Three fundamental quantities of the laminae , i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix , with different fiber preform -LRB- including woven , braided , and knitted fabric -RRB- reinforcements were explicitly obtained by virtue of the bridging micromechanics model . In this paper , the laminate stress analysis is shown . The purpose of this analysis is to determine the load shared by each lamina in the laminate , so that the lamina theory can be applied . Incorporation of the constitutive equations into an FEM software package is illustrated . A number of application examples are given to demonstrate the efficiency of the constitutive theory . The predictions made include : failure envelopes of multidirectional laminates subjected to biaxial in-plane loads , thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate , S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue , and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract_inputs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BIO_labels\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["print(merged_df.iloc[0]['abstract'])\n","print(merged_df.iloc[0]['Abstract_inputs'])\n","print(merged_df.iloc[0]['BIO_labels'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gA3wwkeabwBh","executionInfo":{"status":"ok","timestamp":1714623001870,"user_tz":-420,"elapsed":9,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"4bee1162-b1a0-4764-b132-9d84f7b64611"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\n","['a', 'conflict', 'between', 'language', 'and', 'atom', '##istic', 'information', 'fred', 'dr', '##ets', '##ke', 'and', 'jerry', 'f', '##od', '##or', 'are', 'responsible', 'for', 'popular', '##izing', 'three', 'well', '-', 'known', 'these', '##s', 'in', 'contemporary', 'philosophy', 'of', 'mind', ':', 'the', 'thesis', 'of', 'information', '-', 'based', 'semantics', '-', 'l', '##rb', '-', 'ib', '##s', '-', 'rr', '##b', '-', ',', 'the', 'thesis', 'of', 'content', 'atom', '##ism', '-', 'l', '##rb', '-', 'atom', '##ism', '-', 'rr', '##b', '-', 'and', 'the', 'thesis', 'of', 'the', 'language', 'of', 'thought', '-', 'l', '##rb', '-', 'lot', '-', 'rr', '##b', '-', '.', 'lot', 'concerns', 'the', 'semantic', '##ally', 'relevant', 'structure', 'of', 'representations', 'involved', 'in', 'cognitive', 'states', 'such', 'as', 'beliefs', 'and', 'desires', '.', 'it', 'maintains', 'that', 'all', 'such', 'representations', 'must', 'have', 'syn', '##ta', '##ctic', 'structures', 'mirror', '##ing', 'the', 'structure', 'of', 'their', 'contents', '.', 'ib', '##s', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'relations', 'that', 'connect', 'cognitive', 'representations', 'and', 'their', 'parts', 'to', 'their', 'contents', '-', 'l', '##rb', '-', 'semantic', 'relations', '-', 'rr', '##b', '-', '.', 'it', 'holds', 'that', 'these', 'relations', 'super', '##ven', '##e', 'solely', 'on', 'relations', 'of', 'the', 'kind', 'that', 'support', 'information', 'content', ',', 'perhaps', 'with', 'some', 'help', 'from', 'logical', 'principles', 'of', 'combination', '.', 'atom', '##ism', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'content', 'of', 'simple', 'symbols', '.', 'it', 'holds', 'that', 'each', 'substantive', 'simple', 'symbol', 'possesses', 'its', 'content', 'independently', 'of', 'all', 'other', 'symbols', 'in', 'the', 'representation', '##al', 'system', '.', 'i', 'argue', 'that', 'dr', '##ets', '##ke', \"'\", 's', 'and', 'f', '##od', '##or', \"'\", 's', 'theories', 'are', 'false', 'and', 'that', 'their', 'false', '##hood', 'results', 'from', 'a', 'conflict', 'ib', '##s', 'and', 'atom', '##ism', ',', 'on', 'the', 'one', 'hand', ',', 'and', 'lot', ',', 'on', 'the', 'other']\n","['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n"]}]},{"cell_type":"code","source":["final_df.to_csv('SaveData/final_df_new.csv', index=False)"],"metadata":{"id":"kqiZk90m5nhd","executionInfo":{"status":"ok","timestamp":1714623001870,"user_tz":-420,"elapsed":7,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":["### Nháp"],"metadata":{"id":"SKRjTinwOqUX"}},{"cell_type":"code","source":["# import nltk\n","# from nltk.corpus import stopwords\n","# from nltk.tokenize import word_tokenize\n","\n","# # Download stopwords list (only need to do this once)\n","# nltk.download('stopwords')\n","# nltk.download('punkt')\n","\n","# # Sample text\n","# text = \"This paper proposes using virtual reality to enhance the perception of actions by distant users on a shared application. Here, distance may refer either to space ( e.g. in a remote synchronous collaboration) or time ( e.g. during playback of recorded actions). Our approach consists in immersing the application in a virtual inhabited 3D space and mimicking user actions by animating avatars. We illustrate this approach with two applications, the one for remote collaboration on a shared application and the other to playback recorded sequences of user actions. We suggest this could be a low cost enhancement for telepresence.\"\n","\n","# # Tokenize the text\n","# words = word_tokenize(text)\n","\n","# # Remove stopwords\n","# stop_words = set(stopwords.words('english'))\n","# filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","# # Join the words back into a single string\n","# filtered_text = ' '.join(filtered_words)\n","\n","# print(filtered_text)"],"metadata":{"id":"zkzJ2p-YbQQW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load and Preprocess Data"],"metadata":{"id":"I0bNx-eYbRQo"}},{"cell_type":"code","source":["final_df = pd.read_csv('SaveData/final_df_new.csv')"],"metadata":{"id":"-NvARm9v6ILT","executionInfo":{"status":"ok","timestamp":1714623002360,"user_tz":-420,"elapsed":496,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["final_df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"M_Z6tqEDsgQm","executionInfo":{"status":"ok","timestamp":1714623046884,"user_tz":-420,"elapsed":6,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"678a19da-09d6-43d4-8d8e-f8abc3cb4ce4"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            abstract  \\\n","0  A conflict between language and atomistic info...   \n","1  Selective representing and world-making We dis...   \n","2  Does classicism explain universality ? Argumen...   \n","3  Separate accounts go mainstream -LSB- investme...   \n","4  Evolving receptive-field controllers for mobil...   \n","\n","                                     Abstract_inputs  \\\n","0  ['a', 'conflict', 'between', 'language', 'and'...   \n","1  ['selective', 'representing', 'and', 'world', ...   \n","2  ['does', 'classic', '##ism', 'explain', 'unive...   \n","3  ['separate', 'accounts', 'go', 'mainstream', '...   \n","4  ['evolving', 'rec', '##eptive', '-', 'field', ...   \n","\n","                                          BIO_labels  \n","0  ['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', ...  \n","1  ['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n","2  ['O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', ...  \n","3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', ...  \n","4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', ...  "],"text/html":["\n","  <div id=\"df-dec35864-f3c2-45a1-a820-458334e747c9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstract</th>\n","      <th>Abstract_inputs</th>\n","      <th>BIO_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A conflict between language and atomistic info...</td>\n","      <td>['a', 'conflict', 'between', 'language', 'and'...</td>\n","      <td>['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Selective representing and world-making We dis...</td>\n","      <td>['selective', 'representing', 'and', 'world', ...</td>\n","      <td>['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Does classicism explain universality ? Argumen...</td>\n","      <td>['does', 'classic', '##ism', 'explain', 'unive...</td>\n","      <td>['O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Separate accounts go mainstream -LSB- investme...</td>\n","      <td>['separate', 'accounts', 'go', 'mainstream', '...</td>\n","      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Evolving receptive-field controllers for mobil...</td>\n","      <td>['evolving', 'rec', '##eptive', '-', 'field', ...</td>\n","      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dec35864-f3c2-45a1-a820-458334e747c9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dec35864-f3c2-45a1-a820-458334e747c9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dec35864-f3c2-45a1-a820-458334e747c9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-13069a13-f9ea-4b95-b295-d42f57a353da\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13069a13-f9ea-4b95-b295-d42f57a353da')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-13069a13-f9ea-4b95-b295-d42f57a353da button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"final_df","summary":"{\n  \"name\": \"final_df\",\n  \"rows\": 2744,\n  \"fields\": [\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2738,\n        \"samples\": [\n          \"Stochastic optimization of acoustic response - a numerical and experimental comparison The objective of the work presented is to compare results from numerical optimization with experimental data and to highlight and discuss the differences between two fundamentally different optimization methods . The problem domain is minimization of acoustic emission and the structure used in the work is a closed cylinder with forced vibration of one end . The optimization method used in this paper is simulated annealing -LRB- SA -RRB- , a stochastic method . The results are compared with those from a gradient-based method used on the same structure in an earlier paper -LRB- Tinnsten , 2000 -RRB-\",\n          \"Hysteretic threshold logic and quasi-delay insensitive asynchronous design We introduce the class of hysteretic linear-threshold -LRB- HLT -RRB- logic functions as a novel extension of linear threshold logic , and prove their general applicability for constructing state-holding Boolean functions . We then demonstrate a fusion of HLT logic with the quasi-delay insensitive style of asynchronous circuit design , complete with logical design examples . Future research directions are also identified\",\n          \"On a general constitutive description for the inelastic and failure behavior of fibrous laminates . II . Laminate theory and applications For pt . I see ibid. , pp. 1159-76 . The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms . The constitutive relationship is established micromechanically , through layer-by-layer analysis . Namely , only the properties of the constituent fiber and matrix materials of the composites are required as input data . In the previous part lamina theory was presented . Three fundamental quantities of the laminae , i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix , with different fiber preform -LRB- including woven , braided , and knitted fabric -RRB- reinforcements were explicitly obtained by virtue of the bridging micromechanics model . In this paper , the laminate stress analysis is shown . The purpose of this analysis is to determine the load shared by each lamina in the laminate , so that the lamina theory can be applied . Incorporation of the constitutive equations into an FEM software package is illustrated . A number of application examples are given to demonstrate the efficiency of the constitutive theory . The predictions made include : failure envelopes of multidirectional laminates subjected to biaxial in-plane loads , thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate , S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue , and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract_inputs\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2738,\n        \"samples\": [\n          \"['st', '##och', '##astic', 'optimization', 'of', 'acoustic', 'response', '-', 'a', 'numerical', 'and', 'experimental', 'comparison', 'the', 'objective', 'of', 'the', 'work', 'presented', 'is', 'to', 'compare', 'results', 'from', 'numerical', 'optimization', 'with', 'experimental', 'data', 'and', 'to', 'highlight', 'and', 'discuss', 'the', 'differences', 'between', 'two', 'fundamentally', 'different', 'optimization', 'methods', '.', 'the', 'problem', 'domain', 'is', 'mini', '##mi', '##zation', 'of', 'acoustic', 'emission', 'and', 'the', 'structure', 'used', 'in', 'the', 'work', 'is', 'a', 'closed', 'cylinder', 'with', 'forced', 'vibration', 'of', 'one', 'end', '.', 'the', 'optimization', 'method', 'used', 'in', 'this', 'paper', 'is', 'simulated', 'anne', '##aling', '-', 'l', '##rb', '-', 'sa', '-', 'rr', '##b', '-', ',', 'a', 'st', '##och', '##astic', 'method', '.', 'the', 'results', 'are', 'compared', 'with', 'those', 'from', 'a', 'gradient', '-', 'based', 'method', 'used', 'on', 'the', 'same', 'structure', 'in', 'an', 'earlier', 'paper', '-', 'l', '##rb', '-', 'tin', '##nst', '##en', ',', '2000', '-', 'rr', '##b', '-']\",\n          \"['h', '##yst', '##ere', '##tic', 'threshold', 'logic', 'and', 'quasi', '-', 'delay', 'ins', '##ens', '##itive', 'as', '##yn', '##ch', '##ron', '##ous', 'design', 'we', 'introduce', 'the', 'class', 'of', 'h', '##yst', '##ere', '##tic', 'linear', '-', 'threshold', '-', 'l', '##rb', '-', 'h', '##lt', '-', 'rr', '##b', '-', 'logic', 'functions', 'as', 'a', 'novel', 'extension', 'of', 'linear', 'threshold', 'logic', ',', 'and', 'prove', 'their', 'general', 'app', '##lica', '##bility', 'for', 'constructing', 'state', '-', 'holding', 'boo', '##lean', 'functions', '.', 'we', 'then', 'demonstrate', 'a', 'fusion', 'of', 'h', '##lt', 'logic', 'with', 'the', 'quasi', '-', 'delay', 'ins', '##ens', '##itive', 'style', 'of', 'as', '##yn', '##ch', '##ron', '##ous', 'circuit', 'design', ',', 'complete', 'with', 'logical', 'design', 'examples', '.', 'future', 'research', 'directions', 'are', 'also', 'identified']\",\n          \"['on', 'a', 'general', 'con', '##sti', '##tu', '##tive', 'description', 'for', 'the', 'in', '##ela', '##stic', 'and', 'failure', 'behavior', 'of', 'fi', '##bro', '##us', 'lam', '##inates', '.', 'ii', '.', 'lam', '##inate', 'theory', 'and', 'applications', 'for', 'pt', '.', 'i', 'see', 'ib', '##id', '.', ',', 'pp', '.', '115', '##9', '-', '76', '.', 'the', 'two', 'papers', 'report', 'systematically', 'a', 'con', '##sti', '##tu', '##tive', 'description', 'for', 'the', 'in', '##ela', '##stic', 'and', 'strength', 'behavior', 'of', 'lam', '##inated', 'composite', '##s', 'reinforced', 'with', 'various', 'fiber', 'pre', '##forms', '.', 'the', 'con', '##sti', '##tu', '##tive', 'relationship', 'is', 'established', 'micro', '##me', '##chan', '##ically', ',', 'through', 'layer', '-', 'by', '-', 'layer', 'analysis', '.', 'namely', ',', 'only', 'the', 'properties', 'of', 'the', 'constituent', 'fiber', 'and', 'matrix', 'materials', 'of', 'the', 'composite', '##s', 'are', 'required', 'as', 'input', 'data', '.', 'in', 'the', 'previous', 'part', 'lam', '##ina', 'theory', 'was', 'presented', '.', 'three', 'fundamental', 'quantities', 'of', 'the', 'lam', '##inae', ',', 'i', '.', 'e', '.', 'the', 'internal', 'stresses', 'generated', 'in', 'the', 'constituent', 'fiber', 'and', 'matrix', 'materials', 'and', 'the', 'instant', '##aneous', 'compliance', 'matrix', ',', 'with', 'different', 'fiber', 'pre', '##form', '-', 'l', '##rb', '-', 'including', 'woven', ',', 'braid', '##ed', ',', 'and', 'knit', '##ted', 'fabric', '-', 'rr', '##b', '-', 'reinforcements', 'were', 'explicitly', 'obtained', 'by', 'virtue', 'of', 'the', 'br', '##id', '##ging', 'micro', '##me', '##chan', '##ics', 'model', '.', 'in', 'this', 'paper', ',', 'the', 'lam', '##inate', 'stress', 'analysis', 'is', 'shown', '.', 'the', 'purpose', 'of', 'this', 'analysis', 'is', 'to', 'determine', 'the', 'load', 'shared', 'by', 'each', 'lam', '##ina', 'in', 'the', 'lam', '##inate', ',', 'so', 'that', 'the', 'lam', '##ina', 'theory', 'can', 'be', 'applied', '.', 'incorporation', 'of', 'the', 'con', '##sti', '##tu', '##tive', 'equations', 'into', 'an', 'fe', '##m', 'software', 'package', 'is', 'illustrated', '.', 'a', 'number', 'of', 'application', 'examples', 'are', 'given', 'to', 'demonstrate', 'the', 'efficiency', 'of', 'the', 'con', '##sti', '##tu', '##tive', 'theory', '.', 'the', 'predictions', 'made', 'include', ':', 'failure', 'envelope', '##s', 'of', 'multi', '##di', '##re', '##ction', '##al', 'lam', '##inates', 'subjected', 'to', 'bi', '##ax', '##ial', 'in', '-', 'plane', 'loads', ',', 'the', '##rm', '##ome', '##chan', '##ical', 'cycling', 'stress', '-', 'strain', 'curves', 'of', 'a', 'titanium', 'metal', 'matrix', 'composite', 'lam', '##inate', ',', 's', '-', 'n', 'curves', 'of', 'multi', '##layer', 'knit', '##ted', 'fabric', 'reinforced', 'lam', '##inates', 'under', 'tens', '##ile', 'fatigue', ',', 'and', 'bending', 'load', '-', 'def', '##le', '##ction', 'plots', 'and', 'ultimate', 'bending', 'strengths', 'of', 'lam', '##inated', 'braid', '##ed', 'fabric', 'reinforced', 'beams', 'subjected', 'to', 'lateral', 'loads']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BIO_labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2741,\n        \"samples\": [\n          \"['B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\",\n          \"['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\",\n          \"['O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["def load_imdb_data(data_file):\n","    df = pd.read_csv(data_file)\n","    abstracts = df['abstract'].tolist()\n","    texts = df['Abstract_inputs'].apply(lambda x: ast.literal_eval(x)).tolist()\n","    labels = df['BIO_labels'].apply(lambda x: ast.literal_eval(x)).tolist()\n","    return abstracts, texts, labels"],"metadata":{"id":"o1LwlNA8bX-g","executionInfo":{"status":"ok","timestamp":1714623059966,"user_tz":-420,"elapsed":2,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["data_file = 'SaveData/final_df.csv'\n","abstracts, texts, labels = load_imdb_data(data_file)"],"metadata":{"id":"zimtI3Ef7-f2","executionInfo":{"status":"ok","timestamp":1714623065314,"user_tz":-420,"elapsed":2175,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["print(len(abstracts))\n","print(len(texts))\n","print(len(labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BPBzjaqxgmaI","executionInfo":{"status":"ok","timestamp":1714623067874,"user_tz":-420,"elapsed":507,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"8063e504-618c-4613-99c9-d147c0b44bf8"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["2744\n","2744\n","2744\n"]}]},{"cell_type":"code","source":["print(abstracts[0:10])\n","print(texts[0:10])\n","print(labels[0:10])"],"metadata":{"id":"ofT4czp-8HUW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714623092661,"user_tz":-420,"elapsed":557,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"4eb7da1c-e7bc-4039-95cb-bd90b1c725fa"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind : the thesis of Information-Based Semantics -LRB- IBS -RRB- , the thesis of Content Atomism -LRB- Atomism -RRB- and the thesis of the Language of Thought -LRB- LOT -RRB- . LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires . It maintains that all such representations must have syntactic structures mirroring the structure of their contents . IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents -LRB- semantic relations -RRB- . It holds that these relations supervene solely on relations of the kind that support information content , perhaps with some help from logical principles of combination . Atomism is a thesis about the nature of the content of simple symbols . It holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system . I argue that Dretske 's and Fodor 's theories are false and that their falsehood results from a conflict IBS and Atomism , on the one hand , and LOT , on the other\", 'Selective representing and world-making We discuss the thesis of selective representing-the idea that the contents of the mental representations had by organisms are highly constrained by the biological niches within which the organisms evolved . While such a thesis has been defended by several authors elsewhere , our primary concern here is to take up the issue of the compatibility of selective representing and realism . We hope to show three things . First , that the notion of selective representing is fully consistent with the realist idea of a mind-independent world . Second , that not only are these two consistent , but that the latter -LRB- the realist conception of a mind-independent world -RRB- provides the most powerful perspective from which to motivate and understand the differing perceptual and cognitive profiles themselves . Third , that the -LRB- genuine and important -RRB- sense in which organism and environment may together constitute an integrated system of scientific interest poses no additional threat to the realist conception', \"Does classicism explain universality ? Arguments against a pure classical component of mind One of the hallmarks of human cognition is the capacity to generalize over arbitrary constituents . Marcus -LRB- Cognition 66 , p. 153 ; Cognitive Psychology 37 , p. 243 , 1998 -RRB- argued that this capacity , called `` universal generalization '' -LRB- universality -RRB- , is not supported by connectionist models . Instead , universality is best explained by classical symbol systems , with connectionism as its implementation . Here it is argued that universality is also a problem for classicism in that the syntax-sensitive rules that are supposed to provide causal explanations of mental processes are either too strict , precluding possible generalizations ; or too lax , providing no information as to the appropriate alternative . Consequently , universality is not explained by a classical theory\", 'Separate accounts go mainstream -LSB- investment -RSB- New entrants are shaking up the separate-account industry by supplying Web-based platforms that give advisers the tools to pick independent money managers', 'Evolving receptive-field controllers for mobile robots The use of evolutionary methods to generate controllers for real-world autonomous agents has attracted attention . Most of the pertinent research has employed genetic algorithms or variations thereof . Research has applied an alternative evolutionary method , evolution strategies , to the generation of simple Braitenberg vehicles . This application accelerates the development of such controllers by more than an order of magnitude -LRB- a few hours compared to more than two days -RRB- . Motivated by this useful speedup , the paper investigates the evolution of more complex architectures , receptive-field controllers , that can employ nonlinear interactions and , therefore , can yield more complex behavior . It is interesting to note that the evolution strategy yields the same efficacy in terms of function evaluations , even though the second class of controllers requires up to 10 times more parameters than the simple Braitenberg architecture . In addition to the speedup , there is an important theoretical reason for preferring an evolution strategy over a genetic algorithm for this problem , namely the presence of epistasis', 'A scalable model of cerebellar adaptive timing and sequencing : the recurrent slide and latch -LRB- RSL -RRB- model From the dawn of modern neural network theory , the mammalian cerebellum has been a favored object of mathematical modeling studies . Early studies focused on the fanout , convergence , thresholding , and learned weighting of perceptual-motor signals within the cerebellar cortex . This led to the still viable idea that the granule cell stage in the cerebellar cortex performs a sparse expansive recoding of the time-varying input vector . This recoding reveals and emphasizes combinations in a distributed representation that serves as a basis for the learned , state-dependent control actions engendered by cerebellar outputs to movement related centers . To make optimal use of available signals , the cerebellum must be able to sift the evolving state representation for the most reliable predictors of the need for control actions , and to use those predictors even if they appear only transiently and well in advance of the optimal time for initiating the control action . The paper proposes a modification to prior , population , models for cerebellar adaptive timing and sequencing . Since it replaces a population with a single element , the proposed RSL model is in one sense maximally efficient , and therefore optimal from the perspective of scalability', 'A suggestion of fractional-order controller for flexible spacecraft attitude control A controller design method for flexible spacecraft attitude control is proposed . The system is first described by a partial differential equation with internal damping . Then the frequency response is analyzed , and the three basic characteristics of the flexible system , namely , average function , lower bound and upper bound are defined . On this basis , a fractional-order controller is proposed , which functions as phase stabilization control for lower frequency and smoothly enters to amplitude stabilization at higher frequency by proper amplitude attenuation . It is shown that the equivalent damping ratio increases in proportion to the square of frequency', 'Extracting straight road structure in urban environments using IKONOS satellite imagery We discuss a fully automatic technique for extracting roads in urban environments . The method has its bases in a vegetation mask derived from multispectral IKONOS data and in texture derived from panchromatic IKONOS data . These two techniques together are used to distinguish road pixels . We then move from individual pixels to an object-based representation that allows reasoning on a higher level . Recognition of individual segments and intersections and the relationships among them are used to determine underlying road structure and to then logically hypothesize the existence of additional road network components . We show results on an image of San Diego , California . The object-based processing component may be adapted to utilize other basis techniques as well , and could be used to build a road network in any scene having a straight-line structured topology', 'Nonlinear control of a shape memory alloy actuated manipulator This paper presents a nonlinear , robust control algorithm for accurate positioning of a single degree of freedom rotary manipulator actuated by Shape Memory Alloy -LRB- SMA -RRB- . A model for an SMA actuated manipulator is presented . The model includes nonlinear dynamics of the manipulator , a constitutive model of Shape Memory Alloy , and electrical and heat transfer behavior of SMA wire . This model is used for open and closed loop motion simulations of the manipulator . Experiments are presented that show results similar to both closed and open loop simulation results . Due to modeling uncertainty and nonlinear behavior of the system , classic control methods such as Proportional-Integral-Derivative control are not able to present fast and accurate performance . Hence a nonlinear , robust control algorithm is presented based on Variable Structure Control . This algorithm is a control gain switching technique based on the weighted average of position and velocity feedbacks . This method has been designed through simulation and tested experimentally . Results show fast , accurate , and robust performance of the control system . Computer simulation and experimental results for different stabilization and tracking situations are also presented', \"Lob 's theorem as a limitation on mechanism We argue that Lob 's Theorem implies a limitation on mechanism . Specifically , we argue , via an application of a generalized version of Lob 's Theorem , that any particular device known by an observer to be mechanical can not be used as an epistemic authority -LRB- of a particular type -RRB- by that observer : either the belief-set of such an authority is not mechanizable or , if it is , there is no identifiable formal system of which the observer can know -LRB- or truly believe -RRB- it to be the theorem-set . This gives , we believe , an important and hitherto unnoticed connection between mechanism and the use of authorities by human-like epistemic agents\"]\n","[['a', 'conflict', 'between', 'language', 'and', 'atom', '##istic', 'information', 'fred', 'dr', '##ets', '##ke', 'and', 'jerry', 'f', '##od', '##or', 'are', 'responsible', 'for', 'popular', '##izing', 'three', 'well', '-', 'known', 'these', '##s', 'in', 'contemporary', 'philosophy', 'of', 'mind', ':', 'the', 'thesis', 'of', 'information', '-', 'based', 'semantics', '-', 'l', '##rb', '-', 'ib', '##s', '-', 'rr', '##b', '-', ',', 'the', 'thesis', 'of', 'content', 'atom', '##ism', '-', 'l', '##rb', '-', 'atom', '##ism', '-', 'rr', '##b', '-', 'and', 'the', 'thesis', 'of', 'the', 'language', 'of', 'thought', '-', 'l', '##rb', '-', 'lot', '-', 'rr', '##b', '-', '.', 'lot', 'concerns', 'the', 'semantic', '##ally', 'relevant', 'structure', 'of', 'representations', 'involved', 'in', 'cognitive', 'states', 'such', 'as', 'beliefs', 'and', 'desires', '.', 'it', 'maintains', 'that', 'all', 'such', 'representations', 'must', 'have', 'syn', '##ta', '##ctic', 'structures', 'mirror', '##ing', 'the', 'structure', 'of', 'their', 'contents', '.', 'ib', '##s', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'relations', 'that', 'connect', 'cognitive', 'representations', 'and', 'their', 'parts', 'to', 'their', 'contents', '-', 'l', '##rb', '-', 'semantic', 'relations', '-', 'rr', '##b', '-', '.', 'it', 'holds', 'that', 'these', 'relations', 'super', '##ven', '##e', 'solely', 'on', 'relations', 'of', 'the', 'kind', 'that', 'support', 'information', 'content', ',', 'perhaps', 'with', 'some', 'help', 'from', 'logical', 'principles', 'of', 'combination', '.', 'atom', '##ism', 'is', 'a', 'thesis', 'about', 'the', 'nature', 'of', 'the', 'content', 'of', 'simple', 'symbols', '.', 'it', 'holds', 'that', 'each', 'substantive', 'simple', 'symbol', 'possesses', 'its', 'content', 'independently', 'of', 'all', 'other', 'symbols', 'in', 'the', 'representation', '##al', 'system', '.', 'i', 'argue', 'that', 'dr', '##ets', '##ke', \"'\", 's', 'and', 'f', '##od', '##or', \"'\", 's', 'theories', 'are', 'false', 'and', 'that', 'their', 'false', '##hood', 'results', 'from', 'a', 'conflict', 'ib', '##s', 'and', 'atom', '##ism', ',', 'on', 'the', 'one', 'hand', ',', 'and', 'lot', ',', 'on', 'the', 'other'], ['selective', 'representing', 'and', 'world', '-', 'making', 'we', 'discuss', 'the', 'thesis', 'of', 'selective', 'representing', '-', 'the', 'idea', 'that', 'the', 'contents', 'of', 'the', 'mental', 'representations', 'had', 'by', 'organisms', 'are', 'highly', 'constrained', 'by', 'the', 'biological', 'niche', '##s', 'within', 'which', 'the', 'organisms', 'evolved', '.', 'while', 'such', 'a', 'thesis', 'has', 'been', 'defended', 'by', 'several', 'authors', 'elsewhere', ',', 'our', 'primary', 'concern', 'here', 'is', 'to', 'take', 'up', 'the', 'issue', 'of', 'the', 'compatibility', 'of', 'selective', 'representing', 'and', 'realism', '.', 'we', 'hope', 'to', 'show', 'three', 'things', '.', 'first', ',', 'that', 'the', 'notion', 'of', 'selective', 'representing', 'is', 'fully', 'consistent', 'with', 'the', 'real', '##ist', 'idea', 'of', 'a', 'mind', '-', 'independent', 'world', '.', 'second', ',', 'that', 'not', 'only', 'are', 'these', 'two', 'consistent', ',', 'but', 'that', 'the', 'latter', '-', 'l', '##rb', '-', 'the', 'real', '##ist', 'conception', 'of', 'a', 'mind', '-', 'independent', 'world', '-', 'rr', '##b', '-', 'provides', 'the', 'most', 'powerful', 'perspective', 'from', 'which', 'to', 'mo', '##tiv', '##ate', 'and', 'understand', 'the', 'differing', 'per', '##ce', '##pt', '##ual', 'and', 'cognitive', 'profiles', 'themselves', '.', 'third', ',', 'that', 'the', '-', 'l', '##rb', '-', 'genuine', 'and', 'important', '-', 'rr', '##b', '-', 'sense', 'in', 'which', 'organism', 'and', 'environment', 'may', 'together', 'constitute', 'an', 'integrated', 'system', 'of', 'scientific', 'interest', 'poses', 'no', 'additional', 'threat', 'to', 'the', 'real', '##ist', 'conception'], ['does', 'classic', '##ism', 'explain', 'universal', '##ity', '?', 'arguments', 'against', 'a', 'pure', 'classical', 'component', 'of', 'mind', 'one', 'of', 'the', 'hallmark', '##s', 'of', 'human', 'cognition', 'is', 'the', 'capacity', 'to', 'general', '##ize', 'over', 'arbitrary', 'constituents', '.', 'marcus', '-', 'l', '##rb', '-', 'cognition', '66', ',', 'p', '.', '153', ';', 'cognitive', 'psychology', '37', ',', 'p', '.', '243', ',', '1998', '-', 'rr', '##b', '-', 'argued', 'that', 'this', 'capacity', ',', 'called', '`', '`', 'universal', 'general', '##ization', \"'\", \"'\", '-', 'l', '##rb', '-', 'universal', '##ity', '-', 'rr', '##b', '-', ',', 'is', 'not', 'supported', 'by', 'connection', '##ist', 'models', '.', 'instead', ',', 'universal', '##ity', 'is', 'best', 'explained', 'by', 'classical', 'symbol', 'systems', ',', 'with', 'connection', '##ism', 'as', 'its', 'implementation', '.', 'here', 'it', 'is', 'argued', 'that', 'universal', '##ity', 'is', 'also', 'a', 'problem', 'for', 'classic', '##ism', 'in', 'that', 'the', 'syntax', '-', 'sensitive', 'rules', 'that', 'are', 'supposed', 'to', 'provide', 'causal', 'explanations', 'of', 'mental', 'processes', 'are', 'either', 'too', 'strict', ',', 'pre', '##cl', '##uding', 'possible', 'general', '##izations', ';', 'or', 'too', 'lax', ',', 'providing', 'no', 'information', 'as', 'to', 'the', 'appropriate', 'alternative', '.', 'consequently', ',', 'universal', '##ity', 'is', 'not', 'explained', 'by', 'a', 'classical', 'theory'], ['separate', 'accounts', 'go', 'mainstream', '-', 'l', '##sb', '-', 'investment', '-', 'rs', '##b', '-', 'new', 'en', '##tra', '##nts', 'are', 'shaking', 'up', 'the', 'separate', '-', 'account', 'industry', 'by', 'supplying', 'web', '-', 'based', 'platforms', 'that', 'give', 'advisers', 'the', 'tools', 'to', 'pick', 'independent', 'money', 'managers'], ['evolving', 'rec', '##eptive', '-', 'field', 'controllers', 'for', 'mobile', 'robots', 'the', 'use', 'of', 'evolutionary', 'methods', 'to', 'generate', 'controllers', 'for', 'real', '-', 'world', 'autonomous', 'agents', 'has', 'attracted', 'attention', '.', 'most', 'of', 'the', 'per', '##tine', '##nt', 'research', 'has', 'employed', 'genetic', 'algorithms', 'or', 'variations', 'thereof', '.', 'research', 'has', 'applied', 'an', 'alternative', 'evolutionary', 'method', ',', 'evolution', 'strategies', ',', 'to', 'the', 'generation', 'of', 'simple', 'bra', '##ite', '##nberg', 'vehicles', '.', 'this', 'application', 'accelerate', '##s', 'the', 'development', 'of', 'such', 'controllers', 'by', 'more', 'than', 'an', 'order', 'of', 'magnitude', '-', 'l', '##rb', '-', 'a', 'few', 'hours', 'compared', 'to', 'more', 'than', 'two', 'days', '-', 'rr', '##b', '-', '.', 'motivated', 'by', 'this', 'useful', 'speed', '##up', ',', 'the', 'paper', 'investigates', 'the', 'evolution', 'of', 'more', 'complex', 'architecture', '##s', ',', 'rec', '##eptive', '-', 'field', 'controllers', ',', 'that', 'can', 'employ', 'nonlinear', 'interactions', 'and', ',', 'therefore', ',', 'can', 'yield', 'more', 'complex', 'behavior', '.', 'it', 'is', 'interesting', 'to', 'note', 'that', 'the', 'evolution', 'strategy', 'yields', 'the', 'same', 'efficacy', 'in', 'terms', 'of', 'function', 'evaluation', '##s', ',', 'even', 'though', 'the', 'second', 'class', 'of', 'controllers', 'requires', 'up', 'to', '10', 'times', 'more', 'parameters', 'than', 'the', 'simple', 'bra', '##ite', '##nberg', 'architecture', '.', 'in', 'addition', 'to', 'the', 'speed', '##up', ',', 'there', 'is', 'an', 'important', 'theoretical', 'reason', 'for', 'preferring', 'an', 'evolution', 'strategy', 'over', 'a', 'genetic', 'algorithm', 'for', 'this', 'problem', ',', 'namely', 'the', 'presence', 'of', 'ep', '##ista', '##sis'], ['a', 'scala', '##ble', 'model', 'of', 'ce', '##re', '##bella', '##r', 'adaptive', 'timing', 'and', 'sequencing', ':', 'the', 'rec', '##urrent', 'slide', 'and', 'latch', '-', 'l', '##rb', '-', 'rs', '##l', '-', 'rr', '##b', '-', 'model', 'from', 'the', 'dawn', 'of', 'modern', 'neural', 'network', 'theory', ',', 'the', 'mammalian', 'ce', '##re', '##bell', '##um', 'has', 'been', 'a', 'favored', 'object', 'of', 'mathematical', 'modeling', 'studies', '.', 'early', 'studies', 'focused', 'on', 'the', 'fan', '##out', ',', 'convergence', ',', 'threshold', '##ing', ',', 'and', 'learned', 'weight', '##ing', 'of', 'per', '##ce', '##pt', '##ual', '-', 'motor', 'signals', 'within', 'the', 'ce', '##re', '##bella', '##r', 'cortex', '.', 'this', 'led', 'to', 'the', 'still', 'viable', 'idea', 'that', 'the', 'gran', '##ule', 'cell', 'stage', 'in', 'the', 'ce', '##re', '##bella', '##r', 'cortex', 'performs', 'a', 'sparse', 'expansive', 'rec', '##od', '##ing', 'of', 'the', 'time', '-', 'varying', 'input', 'vector', '.', 'this', 'rec', '##od', '##ing', 'reveals', 'and', 'emphasizes', 'combinations', 'in', 'a', 'distributed', 'representation', 'that', 'serves', 'as', 'a', 'basis', 'for', 'the', 'learned', ',', 'state', '-', 'dependent', 'control', 'actions', 'eng', '##end', '##ered', 'by', 'ce', '##re', '##bella', '##r', 'outputs', 'to', 'movement', 'related', 'centers', '.', 'to', 'make', 'optimal', 'use', 'of', 'available', 'signals', ',', 'the', 'ce', '##re', '##bell', '##um', 'must', 'be', 'able', 'to', 'si', '##ft', 'the', 'evolving', 'state', 'representation', 'for', 'the', 'most', 'reliable', 'predict', '##ors', 'of', 'the', 'need', 'for', 'control', 'actions', ',', 'and', 'to', 'use', 'those', 'predict', '##ors', 'even', 'if', 'they', 'appear', 'only', 'transient', '##ly', 'and', 'well', 'in', 'advance', 'of', 'the', 'optimal', 'time', 'for', 'initiating', 'the', 'control', 'action', '.', 'the', 'paper', 'proposes', 'a', 'modification', 'to', 'prior', ',', 'population', ',', 'models', 'for', 'ce', '##re', '##bella', '##r', 'adaptive', 'timing', 'and', 'sequencing', '.', 'since', 'it', 'replaces', 'a', 'population', 'with', 'a', 'single', 'element', ',', 'the', 'proposed', 'rs', '##l', 'model', 'is', 'in', 'one', 'sense', 'maximal', '##ly', 'efficient', ',', 'and', 'therefore', 'optimal', 'from', 'the', 'perspective', 'of', 'scala', '##bility'], ['a', 'suggestion', 'of', 'fraction', '##al', '-', 'order', 'controller', 'for', 'flexible', 'spacecraft', 'attitude', 'control', 'a', 'controller', 'design', 'method', 'for', 'flexible', 'spacecraft', 'attitude', 'control', 'is', 'proposed', '.', 'the', 'system', 'is', 'first', 'described', 'by', 'a', 'partial', 'differential', 'equation', 'with', 'internal', 'damp', '##ing', '.', 'then', 'the', 'frequency', 'response', 'is', 'analyzed', ',', 'and', 'the', 'three', 'basic', 'characteristics', 'of', 'the', 'flexible', 'system', ',', 'namely', ',', 'average', 'function', ',', 'lower', 'bound', 'and', 'upper', 'bound', 'are', 'defined', '.', 'on', 'this', 'basis', ',', 'a', 'fraction', '##al', '-', 'order', 'controller', 'is', 'proposed', ',', 'which', 'functions', 'as', 'phase', 'stabilization', 'control', 'for', 'lower', 'frequency', 'and', 'smoothly', 'enters', 'to', 'amplitude', 'stabilization', 'at', 'higher', 'frequency', 'by', 'proper', 'amplitude', 'at', '##ten', '##uation', '.', 'it', 'is', 'shown', 'that', 'the', 'equivalent', 'damp', '##ing', 'ratio', 'increases', 'in', 'proportion', 'to', 'the', 'square', 'of', 'frequency'], ['extract', '##ing', 'straight', 'road', 'structure', 'in', 'urban', 'environments', 'using', 'ik', '##ono', '##s', 'satellite', 'imagery', 'we', 'discuss', 'a', 'fully', 'automatic', 'technique', 'for', 'extract', '##ing', 'roads', 'in', 'urban', 'environments', '.', 'the', 'method', 'has', 'its', 'bases', 'in', 'a', 'vegetation', 'mask', 'derived', 'from', 'multi', '##sp', '##ect', '##ral', 'ik', '##ono', '##s', 'data', 'and', 'in', 'texture', 'derived', 'from', 'pan', '##ch', '##romatic', 'ik', '##ono', '##s', 'data', '.', 'these', 'two', 'techniques', 'together', 'are', 'used', 'to', 'distinguish', 'road', 'pixels', '.', 'we', 'then', 'move', 'from', 'individual', 'pixels', 'to', 'an', 'object', '-', 'based', 'representation', 'that', 'allows', 'reasoning', 'on', 'a', 'higher', 'level', '.', 'recognition', 'of', 'individual', 'segments', 'and', 'intersections', 'and', 'the', 'relationships', 'among', 'them', 'are', 'used', 'to', 'determine', 'underlying', 'road', 'structure', 'and', 'to', 'then', 'logical', '##ly', 'h', '##yp', '##oth', '##es', '##ize', 'the', 'existence', 'of', 'additional', 'road', 'network', 'components', '.', 'we', 'show', 'results', 'on', 'an', 'image', 'of', 'san', 'diego', ',', 'california', '.', 'the', 'object', '-', 'based', 'processing', 'component', 'may', 'be', 'adapted', 'to', 'utilize', 'other', 'basis', 'techniques', 'as', 'well', ',', 'and', 'could', 'be', 'used', 'to', 'build', 'a', 'road', 'network', 'in', 'any', 'scene', 'having', 'a', 'straight', '-', 'line', 'structured', 'topology'], ['nonlinear', 'control', 'of', 'a', 'shape', 'memory', 'alloy', 'act', '##uated', 'mani', '##pu', '##lat', '##or', 'this', 'paper', 'presents', 'a', 'nonlinear', ',', 'robust', 'control', 'algorithm', 'for', 'accurate', 'positioning', 'of', 'a', 'single', 'degree', 'of', 'freedom', 'rotary', 'mani', '##pu', '##lat', '##or', 'act', '##uated', 'by', 'shape', 'memory', 'alloy', '-', 'l', '##rb', '-', 'sm', '##a', '-', 'rr', '##b', '-', '.', 'a', 'model', 'for', 'an', 'sm', '##a', 'act', '##uated', 'mani', '##pu', '##lat', '##or', 'is', 'presented', '.', 'the', 'model', 'includes', 'nonlinear', 'dynamics', 'of', 'the', 'mani', '##pu', '##lat', '##or', ',', 'a', 'con', '##sti', '##tu', '##tive', 'model', 'of', 'shape', 'memory', 'alloy', ',', 'and', 'electrical', 'and', 'heat', 'transfer', 'behavior', 'of', 'sm', '##a', 'wire', '.', 'this', 'model', 'is', 'used', 'for', 'open', 'and', 'closed', 'loop', 'motion', 'simulations', 'of', 'the', 'mani', '##pu', '##lat', '##or', '.', 'experiments', 'are', 'presented', 'that', 'show', 'results', 'similar', 'to', 'both', 'closed', 'and', 'open', 'loop', 'simulation', 'results', '.', 'due', 'to', 'modeling', 'uncertainty', 'and', 'nonlinear', 'behavior', 'of', 'the', 'system', ',', 'classic', 'control', 'methods', 'such', 'as', 'proportional', '-', 'integral', '-', 'derivative', 'control', 'are', 'not', 'able', 'to', 'present', 'fast', 'and', 'accurate', 'performance', '.', 'hence', 'a', 'nonlinear', ',', 'robust', 'control', 'algorithm', 'is', 'presented', 'based', 'on', 'variable', 'structure', 'control', '.', 'this', 'algorithm', 'is', 'a', 'control', 'gain', 'switching', 'technique', 'based', 'on', 'the', 'weighted', 'average', 'of', 'position', 'and', 'velocity', 'feedback', '##s', '.', 'this', 'method', 'has', 'been', 'designed', 'through', 'simulation', 'and', 'tested', 'experimental', '##ly', '.', 'results', 'show', 'fast', ',', 'accurate', ',', 'and', 'robust', 'performance', 'of', 'the', 'control', 'system', '.', 'computer', 'simulation', 'and', 'experimental', 'results', 'for', 'different', 'stabilization', 'and', 'tracking', 'situations', 'are', 'also', 'presented'], ['lo', '##b', \"'\", 's', 'theorem', 'as', 'a', 'limitation', 'on', 'mechanism', 'we', 'argue', 'that', 'lo', '##b', \"'\", 's', 'theorem', 'implies', 'a', 'limitation', 'on', 'mechanism', '.', 'specifically', ',', 'we', 'argue', ',', 'via', 'an', 'application', 'of', 'a', 'generalized', 'version', 'of', 'lo', '##b', \"'\", 's', 'theorem', ',', 'that', 'any', 'particular', 'device', 'known', 'by', 'an', 'observer', 'to', 'be', 'mechanical', 'can', 'not', 'be', 'used', 'as', 'an', 'ep', '##iste', '##mic', 'authority', '-', 'l', '##rb', '-', 'of', 'a', 'particular', 'type', '-', 'rr', '##b', '-', 'by', 'that', 'observer', ':', 'either', 'the', 'belief', '-', 'set', 'of', 'such', 'an', 'authority', 'is', 'not', 'me', '##chan', '##iza', '##ble', 'or', ',', 'if', 'it', 'is', ',', 'there', 'is', 'no', 'identifiable', 'formal', 'system', 'of', 'which', 'the', 'observer', 'can', 'know', '-', 'l', '##rb', '-', 'or', 'truly', 'believe', '-', 'rr', '##b', '-', 'it', 'to', 'be', 'the', 'theorem', '-', 'set', '.', 'this', 'gives', ',', 'we', 'believe', ',', 'an', 'important', 'and', 'hit', '##her', '##to', 'unnoticed', 'connection', 'between', 'mechanism', 'and', 'the', 'use', 'of', 'authorities', 'by', 'human', '-', 'like', 'ep', '##iste', '##mic', 'agents']]\n","[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O'], ['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B'], ['O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"]}]},{"cell_type":"code","source":["print(len(abstracts[0]))\n","print(len(texts[0]))\n","print(len(labels[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NT6Pn7wO3Ik","executionInfo":{"status":"ok","timestamp":1714623129254,"user_tz":-420,"elapsed":488,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"6bfa99a1-b2a2-486f-f438-3d93c92a5a24"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["1289\n","265\n","265\n"]}]},{"cell_type":"code","source":["import numpy as np\n","label_to_index = {'O': 0, 'B': 1, 'I': 2, 'P': 3}\n","def one_hot_labels(labels):\n","  one_hot_labels = np.zeros((len(labels), len(label_to_index)), dtype=int)\n","  for i, label in enumerate(labels):\n","      one_hot_labels[i, label_to_index[label]] = 1\n","  return one_hot_labels\n","print(one_hot_labels(['O', 'O', 'O', 'B', 'I', 'I', 'O']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahSpHI2rGqff","executionInfo":{"status":"ok","timestamp":1714623130870,"user_tz":-420,"elapsed":4,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"721f3a5b-7e43-47d8-ef3a-c7eb83b906db"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 0 0 0]\n"," [1 0 0 0]\n"," [1 0 0 0]\n"," [0 1 0 0]\n"," [0 0 1 0]\n"," [0 0 1 0]\n"," [1 0 0 0]]\n"]}]},{"cell_type":"code","source":["labels = [one_hot_labels(label) for label in labels]"],"metadata":{"id":"kvl57NymHWKF","executionInfo":{"status":"ok","timestamp":1714623133593,"user_tz":-420,"elapsed":1,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["print(len(texts[0]))\n","print(len(labels[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MQIvcG4zFM0","executionInfo":{"status":"ok","timestamp":1714623135578,"user_tz":-420,"elapsed":3,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}},"outputId":"8bba87e4-236c-423e-a787-101d45c99976"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["265\n","265\n"]}]},{"cell_type":"code","source":["for i in range(len(texts)):\n","  if len(texts[i])!=len(labels[i]):\n","    print(i)\n","    break"],"metadata":{"id":"qcnGWG88873p","executionInfo":{"status":"ok","timestamp":1714623135578,"user_tz":-420,"elapsed":2,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["# encoding = tokenizer(' '.join(texts[0]), return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)"],"metadata":{"id":"j2v73QrkAiRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(encoding)"],"metadata":{"id":"MBtuVNTkA5nB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextClassificationDataset(Dataset):\n","    def __init__(self, abstracts, texts, labels, tokenizer, max_length):\n","        self.abstracts = abstracts\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","    def __len__(self):\n","        return len(self.texts)\n","    def __getitem__(self, idx):\n","        abstract = self.abstracts[idx]\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        # print('label before', label)\n","        # label = np.concatenate((label, np.array(['D'] * (512-len(label)))))\n","        # print('label after', label)\n","        encoding = self.tokenizer(abstract, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n","        # decoding = [self.tokenizer.decode(idx) for idx in encoding['input_ids'][0]]\n","        return {'abstract': abstract, 'abstract_tokens': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'BIO_labels': torch.tensor(label)}\n","        # return {'abstract_tokens': encoding['input_ids'][0], 'BIO_labels': label}"],"metadata":{"id":"ik-3S3-48JzV","executionInfo":{"status":"ok","timestamp":1714623139168,"user_tz":-420,"elapsed":3,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["## Model"],"metadata":{"id":"2hBEOlIivgiC"}},{"cell_type":"markdown","source":["### BERT Model"],"metadata":{"id":"JWrWwn28vlNu"}},{"cell_type":"code","source":["# input: abstract dạng mảng các phần tử là các token\n","# output: semantic embedding của từng phần tử trong abstract\n","# yêu cầu: độ dài của mảng các embedding bằng với độ dài của mảng các token."],"metadata":{"id":"Ejx0Bhvsvkwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTEncoder(nn.Module):\n","  def __init__(self, bert_model_name):\n","    super(BERTEncoder, self).__init__()\n","    self.tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n","    self.bert = BertModel.from_pretrained(bert_model_name)\n","    for param in self.bert.parameters():\n","      param.requires_grad = True\n","\n","  def forward(self, input_ids, attention_mask):\n","    # abstract_tokens = [tokenizer.decode(idx) for idx in abstract_tokens]\n","    # abstracts = \" \".join(abstract_tokens)\n","    # inputs = self.tokenizer(abstracts, return_tensors=\"pt\")\n","    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","    # inputs = self.tokenizer(\"Hello world!\", return_tensors=\"pt\").to('cuda')\n","    # output_test = self.bert(**inputs)\n","    # print(output_test.last_hidden_state)\n","    return outputs.last_hidden_state[0]\n","\n","  def get_output_shape(self):\n","    return self.bert.config.hidden_size"],"metadata":{"id":"Zq1VB-BywG5S","executionInfo":{"status":"ok","timestamp":1714623140735,"user_tz":-420,"elapsed":3,"user":{"displayName":"Văn Hoàng Phúc Nguyễn","userId":"11486740343166561778"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["### Model Feed Forward"],"metadata":{"id":"OkZK4ztUbYdI"}},{"cell_type":"code","source":["# class BERTClassifier(nn.Module):\n","#     def __init__(self, bert_model_name, num_classes):\n","#         super(BERTClassifier, self).__init__()\n","#         self.bert = BertModel.from_pretrained(bert_model_name)\n","#         self.dropout = nn.Dropout(0.1)\n","#         self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n","\n","#     def forward(self, input_ids, attention_mask):\n","#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","#         pooled_output = outputs.pooler_output\n","#         x = self.dropout(pooled_output)\n","#         logits = self.fc(x)\n","#         return logits"],"metadata":{"id":"uY_4TNaYuaWX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FFClassifier(nn.Module):\n","    # input_shape: bert output shape, num_classes = 3 (BIO)\n","    def __init__(self, input_shape, hidden_size, num_classes):\n","        super(FFClassifier, self).__init__()\n","        self.fc1 = nn.Linear(input_shape, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, num_classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, token_embeds):\n","        x = torch.relu(self.fc1(token_embeds))\n","        x = self.fc2(x)\n","        x = self.softmax(x)\n","        return x"],"metadata":{"id":"Nw9nPkhf2S0m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Phraseformer Model"],"metadata":{"id":"6VlIvrVCuwVs"}},{"cell_type":"code","source":["# input: abstract_tokens là mảng các token của abstract\n","# output: label của từng token là mảng BIO_labels có độ dài bằng với mảng abstract_tokens"],"metadata":{"id":"-BVlizO01jII"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Phraseformer(nn.Module):\n","  def __init__(self, bert_model_name, is_train_bert, is_graph_embedding):\n","    super(Phraseformer, self).__init__()\n","    # self.bert_model_name = bert_model_name\n","    self.bertEmbed = BERTEncoder(bert_model_name)\n","    if is_train_bert:\n","      print(\"Có transfer learning bert\")\n","    if is_graph_embedding:\n","      print(\"Có kết hợp graph embedding\")\n","    self.ffclassifier = FFClassifier(self.bertEmbed.get_output_shape(), 100, 3)\n","\n","  def forward(self, abstract_tokens, input_ids, attention_mask):\n","    bertEmbedding = self.bertEmbed(input_ids, attention_mask)\n","    labels = self.ffclassifier(bertEmbedding)\n","    return labels"],"metadata":{"id":"k0Pfjah_uvzI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"GXpkaKqX7nLS"}},{"cell_type":"markdown","source":["### Initialize"],"metadata":{"id":"zdVgcWpRnoqR"}},{"cell_type":"code","source":["# init hyperparameter of model\n","is_train_bert = False\n","is_graph_embedding = False\n","bert_model_name = \"google-bert/bert-base-uncased\"\n","max_length = 512\n","hidden_size = 100\n","num_classes = 3\n","batch_size = 1\n","num_epochs = 4\n","learning_rate = 2e-5"],"metadata":{"id":"6rpmb3GbvFaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Cur"],"metadata":{"id":"krwkwQa_A1hz"}},{"cell_type":"code","source":["def train(model, data_loader, optimizer, scheduler, device):\n","    model.train()\n","    # print(0)\n","    for batch in data_loader:\n","        # print(1.1)\n","        optimizer.zero_grad()\n","        abstract_tokens = batch['abstract_tokens']\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['BIO_labels'].to(device)\n","        # print(len(abstract_tokens))\n","        # print(len(input_ids[0]))\n","        # print(input_ids[0])\n","        # print(len(attention_mask[0]))\n","        # print(attention_mask[0])\n","        # print(len(labels[0]))\n","        # print(2)\n","        outputs = model(abstract_tokens=abstract_tokens, input_ids=input_ids, attention_mask=attention_mask)\n","        # print(3)\n","        # print('outputs', outputs)\n","        # print(len(outputs))\n","        labels = labels.float()\n","        # print('labels', labels)\n","        # print(len(labels[0]))\n","        labels = labels.view(-1, labels.size(-1)).argmax(dim=1)\n","        # print('after labels', labels)\n","        outputs_split = outputs[1:len(labels)+1]\n","\n","        # old loss\n","        # loss = nn.CrossEntropyLoss()(outputs_split, labels)\n","        class_weights = torch.tensor([1, 1, 1], device=device, dtype=torch.float)\n","        loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = loss_fn(outputs_split, labels)\n","        print(loss)\n","\n","        # new loss\n","        # Tạo mask cho các vị trí trong labels bằng 0 và không phải 0\n","        # mask_zero = (labels == 0).float()\n","        # mask_non_zero = (labels != 0).float()\n","\n","        # Tính toán loss cho các vị trí labels bằng 0\n","        # loss_fn = nn.CrossEntropyLoss(reduction='mean')\n","        # loss1 = loss_fn(outputs_split * mask_zero.unsqueeze(1), torch.zeros_like(labels, dtype=torch.long))\n","        # print(\"loss1:\", outputs_split * mask_zero.unsqueeze(1))\n","        # print(\"loss1:\", torch.zeros_like(labels, dtype=torch.long))\n","        # # Tính toán loss cho các vị trí labels khác 0\n","        # loss2 = loss_fn(outputs_split * mask_non_zero.unsqueeze(1), labels)\n","        # print(\"loss2:\", outputs_split * mask_non_zero.unsqueeze(1))\n","        # print(\"loss2:\", labels)\n","\n","        # print(\"Loss1:\", loss1)\n","        # print(\"Loss2:\", loss2)\n","\n","        # loss = loss1*0.01 + loss2*0.99\n","\n","        # loss = nn.CrossEntropyLoss()(outputs[:len(labels[0])], labels[0].float())\n","        # print(4)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()"],"metadata":{"id":"GWv4Ri5d7pE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","\n","# # Đầu ra dự đoán của mô hình\n","# outputs = torch.tensor([[0.9924, 0.0044, 0.0032],\n","#                         [0.9934, 0.0038, 0.0028],\n","#                         [0.9936, 0.0036, 0.0028]])\n","# #### eval\n","# _, preds = torch.max(outputs, dim=1)\n","# print(preds)\n","# # # Nhãn được biểu diễn dưới dạng one-hot encoding\n","# # labels = torch.tensor([[[1, 0, 0],\n","# #                         [1, 0, 0],\n","# #                         [0, 1, 0]]], dtype=torch.float32)\n","\n","# # # Định dạng lại nhãn để phù hợp với đầu ra\n","# # labels = labels.view(-1, labels.size(-1)).argmax(dim=1)\n","# # print(labels)\n","# # # Tính toán hàm mất mát\n","# # loss_fn = nn.CrossEntropyLoss()\n","# # loss = loss_fn(outputs, labels)\n","\n","# # print(loss)\n"],"metadata":{"id":"7bttlH3mc4DU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data_loader, device):\n","    model.eval()\n","    predictions = []\n","    actual_labels = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            abstract_tokens = batch['abstract_tokens']\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['BIO_labels'].to(device)\n","            outputs = model(abstract_tokens=abstract_tokens, input_ids=input_ids, attention_mask=attention_mask)\n","            labels = labels.float()\n","            labels = labels.view(-1, labels.size(-1)).argmax(dim=1)\n","            outputs = outputs[1:len(labels)+1]\n","            _, preds = torch.max(outputs, dim=1)\n","            predictions.extend(preds.cpu().tolist())\n","            actual_labels.extend(labels.cpu().tolist())\n","    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"],"metadata":{"id":"WTOetG9iAO_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# max_length_abs = 512\n","# Loại bỏ các mảng dữ liệu có len(texts) >= 510\n","abstracts_filtered = []\n","texts_filtered = []\n","labels_filtered = []\n","\n","for abstract, text, label in zip(abstracts, texts, labels):\n","    if len(text) < 510:  # Chỉ giữ lại các mảng có độ dài nhỏ hơn 510\n","        abstracts_filtered.append(abstract)\n","        texts_filtered.append(text)\n","        labels_filtered.append(label)"],"metadata":{"id":"SAtCV2H_mrj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(texts_filtered))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j2ZPOyd8nkWX","executionInfo":{"status":"ok","timestamp":1714296396098,"user_tz":-420,"elapsed":6,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"5a332c4e-9380-448d-e4e3-44eb2b2d351f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2739\n"]}]},{"cell_type":"code","source":["# train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","combined_data = list(zip(abstracts_filtered, texts_filtered))\n","# Chia dữ liệu kết hợp thành tập huấn luyện và tập kiểm tra\n","train_data, val_data, train_labels, val_labels = train_test_split(combined_data, labels_filtered, test_size=0.2, random_state=42)\n","# Tách dữ liệu của \"abstracts\" và \"texts\" sau khi chia\n","train_abstracts, train_texts = zip(*train_data)\n","val_abstracts, val_texts = zip(*val_data)"],"metadata":{"id":"EdVV08iI8H2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n","train_dataset = TextClassificationDataset(train_abstracts, train_texts, train_labels, tokenizer, max_length)\n","val_dataset = TextClassificationDataset(val_abstracts, val_texts, val_labels, tokenizer, max_length)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"],"metadata":{"id":"tMnS9iGZ8P0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = Phraseformer(bert_model_name, is_train_bert, is_graph_embedding).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGH3Z6IK-fY9","executionInfo":{"status":"ok","timestamp":1714296397548,"user_tz":-420,"elapsed":1125,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"afbf6c5e-b705-4472-ebe0-c7ccec683874"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(), lr=learning_rate)\n","total_steps = len(train_dataloader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"],"metadata":{"id":"Ky91T8le_23K","executionInfo":{"status":"ok","timestamp":1714296397549,"user_tz":-420,"elapsed":9,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2511150-96cb-422e-d1f4-2e5e11cb7604"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#### Cur"],"metadata":{"id":"ItPbDHbhDsHc"}},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train(model, train_dataloader, optimizer, scheduler, device)\n","    accuracy, report = evaluate(model, val_dataloader, device)\n","    print(f\"Validation Accuracy: {accuracy:.4f}\")\n","    print(report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0UG0W05AEOF","executionInfo":{"status":"ok","timestamp":1714296828049,"user_tz":-420,"elapsed":427511,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"062d5370-2145-4a6f-ce49-6a06dade3553"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","tensor(1.0803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0779, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0629, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0295, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0337, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9656, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8845, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9345, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7840, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8197, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8840, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7247, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9254, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8295, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8901, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6890, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6785, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7443, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5833, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7942, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8987, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9247, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7625, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7197, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8229, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7942, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8942, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7547, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5970, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9933, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8356, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7229, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7472, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8088, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7837, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6255, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8630\n","              precision    recall  f1-score   support\n","\n","           0       0.86      1.00      0.93     85102\n","           1       0.00      0.00      0.00      4599\n","           2       0.00      0.00      0.00      8907\n","\n","    accuracy                           0.86     98608\n","   macro avg       0.29      0.33      0.31     98608\n","weighted avg       0.74      0.86      0.80     98608\n","\n","Epoch 2/4\n","tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5816, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5607, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5973, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8397, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9157, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7423, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8442, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7779, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7847, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7229, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7255, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5525, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8635, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7249, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7933, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7310, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6294, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7932, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5537, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5975, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7167, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6466, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8240, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7092, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8352, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7355, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8630\n","              precision    recall  f1-score   support\n","\n","           0       0.86      1.00      0.93     85102\n","           1       0.00      0.00      0.00      4599\n","           2       0.00      0.00      0.00      8907\n","\n","    accuracy                           0.86     98608\n","   macro avg       0.29      0.33      0.31     98608\n","weighted avg       0.74      0.86      0.80     98608\n","\n","Epoch 3/4\n","tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8265, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8734, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8242, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5748, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8483, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7961, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6229, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8921, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5889, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0656, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6310, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8396, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7656, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7514, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5714, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8110, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7919, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7991, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6338, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6356, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9431, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6397, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8422, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7647, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7465, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6841, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8546, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7504, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8209, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7191, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8402, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8189, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8195, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8287, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7921, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8793, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6270, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8303, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6713, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5896, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7340, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9245, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7410, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6423, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 0.8630\n","              precision    recall  f1-score   support\n","\n","           0       0.86      1.00      0.93     85102\n","           1       0.00      0.00      0.00      4599\n","           2       0.00      0.00      0.00      8907\n","\n","    accuracy                           0.86     98608\n","   macro avg       0.29      0.33      0.31     98608\n","weighted avg       0.74      0.86      0.80     98608\n","\n","Epoch 4/4\n","tensor(0.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7419, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5784, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6169, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8194, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5673, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9265, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7361, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9249, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8613, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6178, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5603, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8029, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5760, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7679, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8811, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5659, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7296, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6337, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6157, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7503, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7830, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7381, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6607, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8399, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9499, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7805, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7447, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6650, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7331, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6189, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8294, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7176, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7831, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9689, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7969, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9854, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7259, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7716, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7555, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9345, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8686, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7157, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7993, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5602, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7594, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7407, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5901, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6958, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8513, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7842, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7845, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7785, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9071, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8591, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8283, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6607, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0239, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8540, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8345, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7100, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9278, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7899, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7254, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7157, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8135, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7505, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8223, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7567, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7835, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8145, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7693, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7332, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8533, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6198, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7107, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6841, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5959, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7732, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8494, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0649, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6321, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7224, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7851, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5900, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8257, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7970, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7864, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8159, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8440, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5926, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5868, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8018, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7487, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7313, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7654, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8375, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6020, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7182, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8986, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7240, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7678, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7918, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9757, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7461, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5843, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8298, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6312, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8804, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5767, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6034, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9433, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7922, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8251, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8343, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5863, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6202, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7234, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5745, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8634, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9810, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6530, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6260, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5747, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6210, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5753, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6324, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7759, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5579, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7134, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7807, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7309, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9190, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6007, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7856, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8297, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6955, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7339, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8142, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7541, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7194, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8939, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8062, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7194, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7743, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8033, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7940, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9548, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8908, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7474, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8164, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6390, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6316, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7812, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7129, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7685, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8317, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8914, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7269, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5819, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7003, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8380, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5523, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7789, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5667, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8046, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5657, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5553, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9878, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7920, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9263, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7455, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8684, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7479, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5815, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5671, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8575, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7895, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8726, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8168, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6116, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8948, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7067, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7883, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8741, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5934, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8943, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8545, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8438, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8372, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7655, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8570, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9173, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9746, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7823, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7319, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9151, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7822, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6471, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7954, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6265, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7522, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8715, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5764, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8036, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7960, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5619, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7903, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8484, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5516, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.1070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5578, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5999, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7115, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9963, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7006, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7581, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8721, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8720, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6539, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8015, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6216, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8250, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7225, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Validation Accuracy: 0.8630\n","              precision    recall  f1-score   support\n","\n","           0       0.86      1.00      0.93     85102\n","           1       0.00      0.00      0.00      4599\n","           2       0.00      0.00      0.00      8907\n","\n","    accuracy                           0.86     98608\n","   macro avg       0.29      0.33      0.31     98608\n","weighted avg       0.74      0.86      0.80     98608\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"markdown","source":["### Save Model"],"metadata":{"id":"ulO13PHNco4C"}},{"cell_type":"code","source":["# model BERT + 199\n","# model_path = \"./SaveModel/NO_BERT_199.pth\"\n","# model BERT + 155\n","model_path = \"./SaveModel/NO_BERT_111.pth\""],"metadata":{"id":"YLVQ9dT3dSUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), model_path)"],"metadata":{"id":"rf29omPUcoNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Model"],"metadata":{"id":"XmeXp3PXdK6z"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17-LebnNdMsZ","executionInfo":{"status":"ok","timestamp":1714296830117,"user_tz":-420,"elapsed":490,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"ea7088df-b85c-477b-b93d-3a8e9d5fffcd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":147}]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"v0skI1KSdsE4"}},{"cell_type":"markdown","source":["Validation"],"metadata":{"id":"nwG-EhKIfY7-"}},{"cell_type":"code","source":["def extract_keywords_id(abstract_tokens, preds):\n","    keywords = []\n","    current_keyword = []\n","    for token, pred in zip(abstract_tokens, preds):\n","        if pred == 1:  # Nhãn thể hiện token bắt đầu một keyword\n","            if current_keyword != []:\n","                keywords.append(current_keyword)\n","            current_keyword = [token]  # Token đầu tiên của keyword\n","        elif pred == 2:  # Nhãn thể hiện token bên trong keyword\n","            current_keyword.append(token)  # Thêm token vào keyword\n","    if current_keyword != []:\n","        keywords.append(current_keyword)\n","    list_keyword = []\n","    for one_keyword in keywords:\n","        decoded_sequence = tokenizer.decode(one_keyword)\n","        list_keyword.append(decoded_sequence)\n","    return list_keyword"],"metadata":{"id":"Ur5QAizBqdYY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_keywords(abstract_tokens, preds):\n","    keywords = []\n","    current_keyword = \"\"\n","    for token, pred in zip(abstract_tokens, preds):\n","        if pred == 1:  # Nhãn thể hiện token bắt đầu một keyword\n","            if current_keyword != \"\":\n","                keywords.append(current_keyword)\n","            current_keyword = token[0]  # Token đầu tiên của keyword\n","        elif pred == 2:  # Nhãn thể hiện token bên trong keyword\n","            current_keyword += \" \" + token[0]  # Thêm token vào keyword\n","    if current_keyword != \"\":\n","        keywords.append(current_keyword)\n","    return keywords\n","\n","# Sử dụng hàm extract_keywords\n","abstract_tokens = [('learning',), ('non',), ('##re',), ('##gul',), ('##ar',), ('languages',), (':',), ('a',), ('comparison',), ('of',), ('simple',), ('rec',), ('##urrent',), ('networks',), ('and',), ('l',), ('##st',), ('##m',), ('rodriguez',), ('-',), ('l',), ('##rb',), ('-',), ('2001',), ('-',), ('rr',), ('##b',), ('-',), ('examined',), ('the',), ('learning',), ('ability',), ('of',), ('simple',), ('rec',), ('##urrent',), ('nets',), ('-',), ('l',), ('##rb',), ('-',), ('sr',), ('##ns',), ('-',), ('rr',), ('##b',), ('-',), ('-',), ('l',), ('##rb',), ('-',), ('elm',), ('##an',), (',',), ('1990',), ('-',), ('rr',), ('##b',), ('-',), ('on',), ('simple',), ('context',), ('-',), ('sensitive',), ('and',), ('context',), ('-',), ('free',), ('languages',), ('.',), ('in',), ('response',), ('to',), ('rodriguez',), (\"'\",), ('s',), ('-',), ('l',), ('##rb',), ('-',), ('2001',), ('-',), ('rr',), ('##b',), ('-',), ('article',), (',',), ('we',), ('compare',), ('the',), ('performance',), ('of',), ('simple',), ('rec',), ('##urrent',), ('nets',), ('and',), ('long',), ('short',), ('-',), ('term',), ('memory',), ('rec',), ('##urrent',), ('nets',), ('on',), ('context',), ('-',), ('free',), ('and',), ('context',), ('-',), ('sensitive',), ('languages',)]\n","preds = torch.tensor([1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n","        0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 2, 2, 0, 0, 0,\n","        0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2,\n","        0, 1, 1, 0, 2, 1, 2, 2, 2, 0, 1, 0, 2, 0, 1, 0, 2, 2]).tolist()\n","\n","keywords = extract_keywords(abstract_tokens, preds)\n","print(keywords)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JztrnJGrjq7P","executionInfo":{"status":"ok","timestamp":1714291261202,"user_tz":-420,"elapsed":4,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"2b6e20ae-8c00-422b-e7d2-aeed23f7a175"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['learning', 'non ##re ##gul ##ar languages', 'simple rec ##urrent networks', 'l ##st ##m', 'learning ability', 'simple rec ##urrent nets ##rb ##ns', 'elm ##an', 'simple', 'context sensitive', 'context free languages', 'rodriguez ##rb', 'simple rec ##urrent nets', 'long', 'short term', 'memory rec ##urrent nets', 'context free', 'context sensitive languages']\n"]}]},{"cell_type":"code","source":["def evaluate_batch(model, batch, device):\n","    with torch.no_grad():\n","        abstract_tokens = batch['abstract_tokens']\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['BIO_labels'].to(device)\n","        outputs = model(abstract_tokens=abstract_tokens, input_ids=input_ids, attention_mask=attention_mask)\n","        labels = labels.float()\n","        labels = labels.view(-1, labels.size(-1)).argmax(dim=1)\n","        outputs = outputs[1:len(labels)+1]\n","        _, preds = torch.max(outputs, dim=1)\n","        # print(abstract_tokens)\n","        # print(input_ids)\n","        # print(input_ids[0][1:len(labels)+1])\n","        # print(preds)\n","        # print(labels)\n","        predict_keyword = extract_keywords(abstract_tokens, preds)\n","        labels_keyword = extract_keywords(abstract_tokens, labels)\n","        # print(predict_keyword)\n","        # print(labels_keyword)\n","        preds_Id = extract_keywords_id(input_ids[0][1:len(labels)+1], preds)\n","        labels_Id = extract_keywords_id(input_ids[0][1:len(labels)+1], labels)\n","        # print(preds_Id)\n","        # print(labels_Id)\n","    return preds_Id, labels_Id"],"metadata":{"id":"sS5FFBmYfa3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"rJ6Ch5SsB_hS"}},{"cell_type":"code","source":["val_iterator = iter(val_dataloader)\n","batch = next(val_iterator)\n","preds_Id, labels_Id = evaluate_batch(model, batch, device)\n","print(preds_Id)\n","print(labels_Id)\n","batch = next(val_iterator)\n","preds_Id, labels_Id = evaluate_batch(model, batch, device)\n","print(preds_Id)\n","print(labels_Id)\n","batch = next(val_iterator)\n","preds_Id, labels_Id = evaluate_batch(model, batch, device)\n","print(preds_Id)\n","print(labels_Id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4P4wAIkahKUz","executionInfo":{"status":"ok","timestamp":1714296830117,"user_tz":-420,"elapsed":4,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"29fae1fd-93c5-4f9b-ba02-7f88c7f0fc92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","['lstm', 'performance']\n","[]\n","['variable fractional delay allpass filters', 'variable fractional delay allpass filters', 'fractional delay parameter', 'weighted equation error', 'cost function', 'weighted equation error', 'optimal polynomial coefficients', 'linear simultaneous equations']\n","[]\n","['even unimodular integral lattices', 'gaussian integers', 'automorphisms', 'niemeier lattices']\n"]}]},{"cell_type":"code","source":["# tokenizer = AutoTokenizer.from_pretrained(bert_model_name)"],"metadata":{"id":"TWi4_L0xfW8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# abs = final_df.iloc[1]['abstract']"],"metadata":{"id":"yDwePB8FezET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluate"],"metadata":{"id":"3Y88s27KsLDm"}},{"cell_type":"code","source":["def F1_score(preds_Id, labels_Id):\n","    TP = len(set(preds_Id) & set(labels_Id))\n","    FP = len(set(preds_Id) - set(labels_Id))\n","    FN = len(set(labels_Id) - set(preds_Id))\n","\n","    try:\n","        P = TP / (TP + FP)\n","        R = TP / (TP + FN)\n","    except:\n","        # F1 = None\n","        return None\n","\n","    if (P!=0 and R!=0):\n","        F1 = 2 * (P * R) / (P + R)\n","    else:\n","        F1 = None\n","    return F1"],"metadata":{"id":"02myn9oOurDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preds_Id = ['learning', 'nonregular languages', 'simple recurrent networks', 'lstm', 'learning ability', 'simple recurrent netsrbns', 'elman', 'simple', 'context sensitive', 'context free languages', 'rodriguezrb', 'simple recurrent nets', 'long', 'short term', 'memory recurrent nets', 'context free', 'context sensitive languages']\n","# labels_Id = ['lstm', 'performance']\n","# preds_Id = ['design', 'variable', 'fractional delay allpass filters', 'weighted least squares method', 'weighted least method', 'variable fractional delay allpass filters coefficient', 'variable allpass filter polynomial', 'fractional delay parameter', 'nonlinear phase error', 'weighted equation error', 'cost functionratic form', 'weighted equation error', 'optimal', 'polynomial coefficients', 'linear simultaneous equations']\n","# labels_Id = ['variable fractional delay allpass filters', 'variable fractional delay allpass filters', 'fractional delay parameter', 'weighted equation error', 'cost function', 'weighted equation error', 'optimal polynomial coefficients', 'linear simultaneous equations']\n","preds_Id = ['even', 'unimodular', 'gaussian lattices', 'unimodular', 'gaussian lattices', 'unimodular integral lattices', 'gaussian integers classification', 'automorphisms', 'tau automorphism groups', 'niemeier lattices', 'unimodular', 'real', 'integral lattices', 'even', 'unimodular gaussian lattices', 'equivalence']\n","labels_Id = ['even unimodular integral lattices', 'gaussian integers', 'automorphisms', 'niemeier lattices']\n","\n","TP = len(set(preds_Id) & set(labels_Id))\n","FP = len(set(preds_Id) - set(labels_Id))\n","FN = len(set(labels_Id) - set(preds_Id))\n","try:\n","    P = TP / (TP + FP)\n","    R = TP / (TP + FN)\n","except:\n","    F1 = None\n","\n","if (P!=0 and R!=0):\n","    F1 = 2 * (P * R) / (P + R)\n","else:\n","    F1 = None\n","\n","print(\"Precision:\", P)\n","print(\"Recall:\", R)\n","print(\"F1 score:\", F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oavYQPY_sNQA","executionInfo":{"status":"ok","timestamp":1714297898414,"user_tz":-420,"elapsed":385,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"f2f17058-4904-448f-86ce-d02111cb2b1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.16666666666666666\n","Recall: 0.5\n","F1 score: 0.25\n"]}]},{"cell_type":"code","source":["print(\"F1: \", F1_score(preds_Id, labels_Id))"],"metadata":{"id":"R6lIKv5CuD8N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714297901574,"user_tz":-420,"elapsed":488,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"21db396d-c3d5-47d2-cb8c-801057a1a87a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1:  0.25\n"]}]},{"cell_type":"code","source":["def F1_batch(model, batch, device):\n","    with torch.no_grad():\n","        abstract_tokens = batch['abstract_tokens']\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['BIO_labels'].to(device)\n","        outputs = model(abstract_tokens=abstract_tokens, input_ids=input_ids, attention_mask=attention_mask)\n","        labels = labels.float()\n","        labels = labels.view(-1, labels.size(-1)).argmax(dim=1)\n","        outputs = outputs[1:len(labels)+1]\n","        _, preds = torch.max(outputs, dim=1)\n","        # print(abstract_tokens)\n","        # print(input_ids)\n","        # print(input_ids[0][1:len(labels)+1])\n","        # print(preds)\n","        # print(labels)\n","        predict_keyword = extract_keywords(abstract_tokens, preds)\n","        labels_keyword = extract_keywords(abstract_tokens, labels)\n","        # print(predict_keyword)\n","        # print(labels_keyword)\n","        preds_Id = extract_keywords_id(input_ids[0][1:len(labels)+1], preds)\n","        labels_Id = extract_keywords_id(input_ids[0][1:len(labels)+1], labels)\n","        # print(preds_Id)\n","        # print(labels_Id)\n","    return F1_score(preds_Id, labels_Id)"],"metadata":{"id":"AIul3dgEv1h2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run"],"metadata":{"id":"K0orG6mEB74X"}},{"cell_type":"code","source":["print(len(val_dataloader))\n","F1_list = []\n","for batch in val_dataloader:\n","    F1 = F1_batch(model, batch, device)\n","    if (F1!=None):\n","        F1_list.append(F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rErrYYIIvKba","executionInfo":{"status":"ok","timestamp":1714297938935,"user_tz":-420,"elapsed":34605,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"822e6c39-483b-4c8e-ea4a-2e4d3df67ba8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["548\n"]}]},{"cell_type":"code","source":["print(F1_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZwpptyaxtGs","executionInfo":{"status":"ok","timestamp":1714297938936,"user_tz":-420,"elapsed":16,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"7e12d1cd-34b4-483d-b10b-851fd68446be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n"]}]},{"cell_type":"code","source":["sum = 0\n","for F1 in F1_list:\n","  sum += F1\n","print(sum)\n","print(sum/len(F1_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"gZ6WmUtBx_K-","executionInfo":{"status":"error","timestamp":1714297938937,"user_tz":-420,"elapsed":14,"user":{"displayName":"Thành Trấn","userId":"03753152682592362464"}},"outputId":"ce5b42b1-3e2f-47db-d654-02edf7023e2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-162-82b4862f001b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]}]}